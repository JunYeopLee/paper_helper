{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract valid sentences and vocabulary set from pdf format file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1. Read PDF file using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"b'Unsupervised Learning of Sentence Embeddings\",\n",
      " 'using Compositional n-Gram Features',\n",
      " '',\n",
      " 'Matteo Pagliardini * 1 Prakhar Gupta * 2 Martin Jaggi 2',\n",
      " '',\n",
      " 'arXiv:1703.02507v2 [cs.CL] 10 Jul 2017',\n",
      " '',\n",
      " 'Abstract',\n",
      " 'The recent tremendous success of unsupervised',\n",
      " 'word embeddings in a multitude of applications',\n",
      " 'raises the obvious question if similar methods',\n",
      " 'could be derived to improve embeddings (i.e.',\n",
      " 'semantic representations) of word sequences as',\n",
      " 'well. We present a simple but efficient unsupervised objective to train '\n",
      " 'distributed representations of sentences. Our method outperforms',\n",
      " 'the state-of-the-art unsupervised models on most',\n",
      " 'benchmark tasks, highlighting the robustness of',\n",
      " 'the produced general-purpose sentence embeddings.',\n",
      " '',\n",
      " '1. Introduction',\n",
      " 'Improving unsupervised learning is of key importance for',\n",
      " 'advancing machine learning methods, as to unlock access',\n",
      " 'to almost unlimited amounts of data to be used as training',\n",
      " 'resources. The majority of recent success stories of deep',\n",
      " 'learning does not fall into this category but instead relied',\n",
      " 'on supervised training (in particular in the vision domain).',\n",
      " 'A very notable exception comes from the text and natural',\n",
      " 'language processing domain, in the form of semantic word',\n",
      " 'embeddings trained unsupervised (Mikolov et al., 2013b;a;',\n",
      " 'Pennington et al., 2014). Within only a few years from their',\n",
      " 'invention, such word representations \\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93 which '\n",
      " 'are based on',\n",
      " 'a simple matrix factorization model as we formalize below',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93 are now routinely trained on very large '\n",
      " 'amounts of raw',\n",
      " 'text data, and have become ubiquitous building blocks of a',\n",
      " 'majority of current state-of-the-art NLP applications.',\n",
      " '',\n",
      " 'learning for NLP leads towards increasingly powerful',\n",
      " 'and complex models, such as recurrent neural networks',\n",
      " '(RNNs), LSTMs, attention models and even Neural Turing',\n",
      " 'Machine architectures. While extremely strong in expressiveness, the '\n",
      " 'increased model complexity makes such models much slower to train on larger '\n",
      " 'datasets. On the other end',\n",
      " 'of the spectrum, simpler '\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9cshallow\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9d models '\n",
      " 'such as matrix',\n",
      " 'factorizations (or bilinear models) can benefit from training',\n",
      " 'on much larger sets of data, which can be a key advantage,',\n",
      " 'especially in the unsupervised setting.',\n",
      " 'Surprisingly, for constructing sentence embeddings,',\n",
      " 'naively using averaged word vectors was recently shown',\n",
      " 'to outperform LSTMs (see (Wieting et al., 2016a) for plain',\n",
      " 'averaging, and (Arora et al., 2017) for weighted averaging). This example '\n",
      " 'shows potential in exploiting the tradeoff between model complexity and '\n",
      " 'ability to process huge',\n",
      " 'amounts of text using scalable algorithms, towards the simpler side. In view '\n",
      " 'of this trade-off, our work here further',\n",
      " 'advances unsupervised learning of sentence embeddings.',\n",
      " 'Our proposed model can be seen as an extension of the CBOW (Mikolov et al., '\n",
      " '2013b;a) training objective to train',\n",
      " 'sentence instead of word embeddings. We demonstrate that',\n",
      " 'the empirical performance of our resulting general-purpose',\n",
      " 'sentence embeddings very significantly exceeds the state of',\n",
      " 'the art, while keeping the model simplicity as well as training and '\n",
      " 'inference complexity exactly as low as in averaging',\n",
      " 'methods (Wieting et al., 2016a; Arora et al., 2017), thereby',\n",
      " 'also putting the title of (Arora et al., 2017) in perspective.',\n",
      " 'Contributions. The main contributions in this work can',\n",
      " 'be summarized as follows:1',\n",
      " '',\n",
      " 'While very useful semantic representations are available',\n",
      " 'for words, it remains challenging to produce and learn such',\n",
      " 'semantic embeddings for longer pieces of text, such as sentences, paragraphs '\n",
      " 'or entire documents. Even more so, it',\n",
      " 'remains a key goal to learn such general-purpose representations in an '\n",
      " 'unsupervised way.',\n",
      " '',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xcb\\\\x98 Model. We propose Sent2Vec, a simple '\n",
      " 'unsupervised',\n",
      " 'model allowing to compose sentence embeddings using the word vectors along '\n",
      " 'with n-gram embeddings,',\n",
      " 'simultaneously training composition and the embedding vectors themselves.',\n",
      " '',\n",
      " 'Currently, two contrary research trends have emerged in',\n",
      " 'text understanding: On one hand, a strong trend in deep-',\n",
      " '',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xcb\\\\x98 Scalability. The computational complexity of '\n",
      " 'our embeddings is only O(1) vector operations per word processed, both '\n",
      " 'during training and inference of the sen-',\n",
      " '',\n",
      " '*',\n",
      " '',\n",
      " 'Equal contribution 1 Iprova SA, Switzerland 2 Computer and',\n",
      " 'Communication Sciences, EPFL, Switzerland. Correspondence',\n",
      " 'to: Martin Jaggi <martin.jaggi@epfl.ch>.',\n",
      " '',\n",
      " '1',\n",
      " 'All our code and pre-trained models are publicly available',\n",
      " 'on http://github.com/epfml/sent2vec.',\n",
      " '',\n",
      " '\\\\x0cUnsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " '',\n",
      " 'tence embeddings. This strongly contrasts all neural',\n",
      " 'network based approaches, and allows our model to',\n",
      " 'learn from extremely large datasets, which is a crucial',\n",
      " 'advantage in the unsupervised setting.',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xcb\\\\x98 Performance. Our method shows significant '\n",
      " 'performance improvements compared to the current stateof-the-art '\n",
      " 'unsupervised and even semi-supervised',\n",
      " 'models. The resulting general-purpose embeddings',\n",
      " 'show strong robustness when transferred to a wide',\n",
      " 'range of prediction benchmarks.',\n",
      " '',\n",
      " 'Formally, we learn source v w and target uw embeddings',\n",
      " 'for each word w in the vocabulary, with embedding dimension h and k = |V| as '\n",
      " 'in (1). The sentence embedding',\n",
      " 'is defined as the average of the source word embeddings of',\n",
      " 'its constituent words, as in (2). We augment this model furthermore by also '\n",
      " 'learning source embeddings for not only',\n",
      " 'unigrams but also n-grams present in each sentence, and',\n",
      " 'averaging the n-gram embeddings along with the words,',\n",
      " 'i.e., the sentence embedding v S for S is modeled as',\n",
      " 'v S :=',\n",
      " '',\n",
      " '1',\n",
      " '|R(S)| V \\\\xc3\\\\x8e\\\\xc5\\\\xa1R(S)',\n",
      " '',\n",
      " '=',\n",
      " '',\n",
      " '1',\n",
      " '|R(S)|',\n",
      " '',\n",
      " 'X',\n",
      " '',\n",
      " 'vw',\n",
      " '',\n",
      " '(2)',\n",
      " '',\n",
      " 'w\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88R(S)',\n",
      " '',\n",
      " '2. Model',\n",
      " 'Our model is inspired by simple matrix factor models (bilinear models) such '\n",
      " 'as recently very successfully used in',\n",
      " 'unsupervised learning of word embeddings (Mikolov et al.,',\n",
      " '2013b;a; Pennington et al., 2014; Bojanowski et al., 2017)',\n",
      " 'as well as supervised of sentence classification (Joulin',\n",
      " 'et al., 2017). More precisely, these models are formalized',\n",
      " 'as an optimization problem of the form',\n",
      " 'min',\n",
      " '',\n",
      " 'U ,V',\n",
      " '',\n",
      " 'X',\n",
      " '',\n",
      " 'fS (U V \\\\xc3\\\\x8e\\\\xc5\\\\xa1S )',\n",
      " '',\n",
      " '(1)',\n",
      " '',\n",
      " 'where R(S) is the list of n-grams (including unigrams)',\n",
      " 'present in sentence S. In order to predict a missing word',\n",
      " 'from the context, our objective models the softmax output',\n",
      " 'approximated by negative sampling following (Mikolov',\n",
      " 'et al., 2013b). For the large number of output classes',\n",
      " '|V| to be predicted, negative sampling is known to significantly improve '\n",
      " 'training efficiency, see also (Goldberg',\n",
      " '& Levy, 2014). Given the binary logistic loss function',\n",
      " '` : x 7\\\\xc3\\\\xa2\\\\xc2\\\\x86\\\\xc2\\\\x92 log (1 + '\n",
      " 'e\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92x ) coupled with negative sampling, our',\n",
      " 'unsupervised training objective is formulated as follows:',\n",
      " '',\n",
      " 'S\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88C',\n",
      " '',\n",
      " 'for two parameter matrices U \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88 '\n",
      " 'Rk\\\\xc4\\\\x82\\\\xc2\\\\x97h and V \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88 '\n",
      " 'Rh\\\\xc4\\\\x82\\\\xc2\\\\x97|V| ,',\n",
      " 'where V denotes the vocabulary. In all models studied,',\n",
      " 'the columns of the matrix V will collect the learned word',\n",
      " 'vectors, having h dimensions. For a given sentence S,',\n",
      " 'which can be of arbitrary length, the indicator vector \\\\xc3\\\\x8e\\\\xc5\\\\xa1S '\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88',\n",
      " '{0, 1}|V| is a binary vector encoding S (bag of words encoding).',\n",
      " 'Fixed-length context windows S running over the corpus are used in word '\n",
      " 'embedding methods as in C-BOW',\n",
      " '(Mikolov et al., 2013b;a) and GloVe (Pennington et al.,',\n",
      " '2014). Here we have k = |V| and each cost function',\n",
      " 'fS : Rk \\\\xc3\\\\xa2\\\\xc2\\\\x86\\\\xc2\\\\x92 R only depends on a single row of its '\n",
      " 'input, describing the observed target word for the given fixed-length',\n",
      " 'context S. In contrast, for sentence embeddings which',\n",
      " 'are the focus of our paper here, S will be entire sentences',\n",
      " 'or documents (therefore variable length). This property is',\n",
      " 'shared with the supervised FastText classifier (Joulin et al.,',\n",
      " '2017), which however uses soft-max with k \\\\x1c |V| being',\n",
      " 'the number of class labels.',\n",
      " '2.1. Proposed Unsupervised Model',\n",
      " 'We propose a new unsupervised model, Sent2Vec, for',\n",
      " 'learning universal sentence embeddings. Conceptually, the',\n",
      " 'model can be interpreted as a natural extension of the wordcontexts from '\n",
      " 'C-BOW (Mikolov et al., 2013b;a) to a larger',\n",
      " 'sentence context, with the sentence words being specifically optimized '\n",
      " 'towards additive combination over the sentence, by means of the unsupervised '\n",
      " 'objective function.',\n",
      " '',\n",
      " 'min',\n",
      " '',\n",
      " 'U ,V',\n",
      " '',\n",
      " '\\\\x13',\n",
      " 'X X \\\\x12',\n",
      " '\\\\x01',\n",
      " '\\\\x01 X',\n",
      " '>',\n",
      " '` u>',\n",
      " 'v',\n",
      " '+',\n",
      " '`',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92',\n",
      " 'u',\n",
      " 'v',\n",
      " '0',\n",
      " 'wt S\\\\\\\\{wt }',\n",
      " 'w S\\\\\\\\{wt }',\n",
      " 'w0 \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88Nwt',\n",
      " '',\n",
      " 'S\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88C wt \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88S',\n",
      " '',\n",
      " 'where S corresponds to the current sentence and Nwt is',\n",
      " 'the set of words sampled negatively for the word wt '\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88 S.',\n",
      " 'The negatives are sampled2 following a multinomial distribution where',\n",
      " 'with a probability',\n",
      " '\\\\x0e word',\n",
      " '\\\\x01',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x9a each',\n",
      " 'P w ispassociated',\n",
      " 'fwi , where fw is the norqn (w) := fw',\n",
      " 'wi \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88V',\n",
      " 'malized frequency of w in the corpus.',\n",
      " 'To select the possible target unigrams (positives), we use',\n",
      " 'subsampling as in (Joulin et al., 2017; Bojanowski et al.,',\n",
      " '2017), each word w being discarded',\n",
      " '1\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92',\n",
      " '\\\\x08 p with probability',\n",
      " '\\\\t',\n",
      " 'qp (w) where qp (w) := min 1, t/fw + t/fw . Where t',\n",
      " 'is the subsampling hyper-parameter. Subsampling prevents',\n",
      " 'very frequent words of having too much influence in the',\n",
      " 'learning as they would introduce strong biases in the prediction task. With '\n",
      " 'positives subsampling and respecting the',\n",
      " 'negative sampling distribution, the precise training objective function '\n",
      " 'becomes',\n",
      " 'X X\\\\x12',\n",
      " '\\\\x01',\n",
      " 'min',\n",
      " 'qp (wt )` u>',\n",
      " '(3)',\n",
      " 'wt v S\\\\\\\\{wt }',\n",
      " 'U ,V',\n",
      " '',\n",
      " 'S\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88C wt \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88S',\n",
      " '',\n",
      " '+ |Nwt |',\n",
      " '',\n",
      " 'X',\n",
      " '',\n",
      " '0',\n",
      " '',\n",
      " 'qn (w )` \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92',\n",
      " '',\n",
      " 'u>',\n",
      " 'w0 v S\\\\\\\\{wt }',\n",
      " '',\n",
      " '\\\\x01',\n",
      " '',\n",
      " '\\\\x13',\n",
      " '',\n",
      " 'w0 \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88V',\n",
      " '2',\n",
      " '',\n",
      " 'To efficiently sample negatives, a pre-processing table is constructed, '\n",
      " 'containing the words corresponding to the square root of',\n",
      " 'their corpora frequency. Then, the negatives Nwt are sampled',\n",
      " 'uniformly at random from the negatives table except the target wt',\n",
      " 'itself, following (Joulin et al., 2017; Bojanowski et al., 2017).',\n",
      " '',\n",
      " '\\\\x0cUnsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " '',\n",
      " '2.2. Computational Efficiency',\n",
      " 'In contrast to more complex neural network based models, one of the core '\n",
      " 'advantages of the proposed technique',\n",
      " 'is the low computational cost for both inference and training. Given a '\n",
      " 'sentence S and a trained model, computing',\n",
      " 'the sentence representation v S only requires |S| \\\\xc3\\\\x82\\\\xcb\\\\x87 h '\n",
      " 'floating',\n",
      " 'point operations (or |R(S)| \\\\xc3\\\\x82\\\\xcb\\\\x87 h to be precise for the '\n",
      " 'n-gram',\n",
      " 'case, see (2)), where h is the embedding dimension. The',\n",
      " 'same holds for the cost of training with SGD on the objective (3), per '\n",
      " 'sentence seen in the training corpus. Due to the',\n",
      " 'simplicity of the model, parallel training is straight-forward',\n",
      " 'using parallelized or distributed SGD.',\n",
      " '2.3. Comparison to C-BOW',\n",
      " 'C-BOW (Mikolov et al., 2013b;a) tries to predict a chosen',\n",
      " 'target word given its fixed-size context window, the context',\n",
      " 'being defined by the average of the vectors associated with',\n",
      " 'the words at a distance less than the window size hyperparameter ws. If our '\n",
      " 'system, when restricted to unigram',\n",
      " 'features, can be seen as an extension of C-BOW where',\n",
      " 'the context window includes the entire sentence, in practice there are few '\n",
      " 'important differences as C-BOW uses',\n",
      " 'important tricks to facilitate the learning of word embeddings. C-BOW first '\n",
      " 'uses frequent word subsampling on the',\n",
      " 'sentences, deciding to discard each token w with probability qp (w) or alike '\n",
      " '(small variations exist across implementations). Subsampling prevents the '\n",
      " 'generation of n-grams',\n",
      " 'features, and deprives the sentence of an important part of',\n",
      " 'its syntactical features. It also shortens the distance between subsampled '\n",
      " 'words, implicitly increasing the span of',\n",
      " 'the context window. A second trick consists of using dynamic context '\n",
      " 'windows: for each subsampled word w, the',\n",
      " 'size of its associated context window is sampled uniformly',\n",
      " 'between 1 and ws. Using dynamic context windows is',\n",
      " 'equivalent to weighing by the distance from the focus word',\n",
      " 'w divided by the window size (Levy et al., 2015). This',\n",
      " 'makes the prediction task local, and go against our objective of creating '\n",
      " 'sentence embeddings as we want to learn',\n",
      " 'how to compose all n-gram features present in a sentence.',\n",
      " 'In the results section, we report a significant improvement',\n",
      " 'of our method over C-BOW.',\n",
      " '2.4. Model Training',\n",
      " 'Three different datasets have been used to train our models:',\n",
      " 'the Toronto book corpus3 , Wikipedia sentences and tweets.',\n",
      " 'The Wikipedia and Toronto books sentences have been tokenized using the '\n",
      " 'Stanford NLP library (Manning et al.,',\n",
      " '2014), while for tweets we used the NLTK tweets tokenizer',\n",
      " '(Bird et al., 2009). For training, we select a sentence randomly from the '\n",
      " 'dataset and then proceed to select all the',\n",
      " '3',\n",
      " '',\n",
      " 'http://www.cs.toronto.edu/\\\\xc3\\\\x8b\\\\xc2\\\\x9cmbweb/',\n",
      " '',\n",
      " 'possible target unigrams using subsampling. We update the',\n",
      " 'weights using SGD with a linearly decaying learning rate.',\n",
      " 'Also, to prevent overfitting, for each sentence we use',\n",
      " 'dropout on its list of n-grams R(S) \\\\\\\\ {U (S)}, where U (S)',\n",
      " 'is the set of all unigrams contained in sentence S. After',\n",
      " 'empirically trying multiple dropout schemes, we find that',\n",
      " 'dropping K n-grams (n > 1) for each sentence is giving superior results '\n",
      " 'compared to dropping each token with some',\n",
      " 'fixed probability. This dropout mechanism would negatively impact shorter '\n",
      " 'sentences. The regularization can be',\n",
      " 'pushed further by applying L1 regularization to the word',\n",
      " 'vectors. Encouraging sparsity in the embedding vectors is',\n",
      " 'particularly beneficial for high dimension h. The additional',\n",
      " 'soft thresholding in every SGD step adds negligible computational cost. See '\n",
      " 'also Appendix B.',\n",
      " 'We train two models on each dataset, one with unigrams',\n",
      " 'only and one with unigrams and bigrams. All training',\n",
      " 'parameters for the models are provided in Table 5 in the',\n",
      " 'supplementary material. Our C++ implementation builds',\n",
      " 'upon the FastText library (Joulin et al., 2017; Bojanowski',\n",
      " 'et al., 2017). We will make our code and pre-trained models available '\n",
      " 'open-source.',\n",
      " '',\n",
      " '3. Related Work',\n",
      " 'We discuss existing models which have been proposed to',\n",
      " 'construct sentence embeddings. While there is a large body',\n",
      " 'of works in this direction \\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93 several among '\n",
      " 'these using e.g.',\n",
      " 'labelled datasets of paraphrase pairs to obtain sentence embeddings in a '\n",
      " 'supervised manner (Wieting et al., 2016b;a)',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93 we here focus on unsupervised, '\n",
      " 'task-independent models. While some methods require ordered raw text i.e., a',\n",
      " 'coherent corpus where the next sentence is a logical continuation of the '\n",
      " 'previous sentence, others rely only on raw',\n",
      " 'text i.e., an unordered collection of sentences. Finally we',\n",
      " 'also discuss alternative models built from structured data',\n",
      " 'sources.',\n",
      " '3.1. Unsupervised Models Independent of Sentence',\n",
      " 'Ordering',\n",
      " 'The ParagraphVector DBOW model (Le & Mikolov,',\n",
      " '2014) is a log-linear model which is trained to learn sentence as well as '\n",
      " 'word embeddings and then use a softmax distribution to predict words '\n",
      " 'contained in the sentence',\n",
      " 'given the sentence vector representation. They also propose a different '\n",
      " 'model ParagraphVector DM where they',\n",
      " 'use n-grams of consecutive words along with the sentence',\n",
      " 'vector representation to predict the next word.',\n",
      " '(Hill et al., 2016a) propose a Sequential (Denoising) Autoencoder, S(D)AE. '\n",
      " 'This model first introduces noise in',\n",
      " 'the input data: Firstly each word is deleted with probability p0 , then for '\n",
      " 'each non-overlapping bigram, words',\n",
      " '',\n",
      " '\\\\x0cUnsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " '',\n",
      " 'are swapped with probability px . The model then uses an',\n",
      " 'LSTM-based architecture to retrieve the original sentence',\n",
      " 'from the corrupted version. The model can then be used',\n",
      " 'to encode new sentences into vector representations. In the',\n",
      " 'case of p0 = px = 0, the model simply becomes a Sequential Autoencoder. '\n",
      " '(Hill et al., 2016a) also propose a variant',\n",
      " '(S(D)AE + embs.) in which the words are represented by',\n",
      " 'fixed pre-trained word vector embeddings.',\n",
      " '(Arora et al., 2017) propose a model in which sentences',\n",
      " 'are represented as a weighted average of fixed (pre-trained)',\n",
      " 'word vectors, followed by post-processing step of subtracting the principal '\n",
      " 'component. Using the generative model',\n",
      " 'of (Arora et al., 2016), words are generated conditioned on',\n",
      " 'a sentence '\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9cdiscourse\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9d vector '\n",
      " 'cs :',\n",
      " 'P r[w | cs ] = \\\\xc3\\\\x8e\\\\xc4\\\\x85fw + (1 \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92 '\n",
      " '\\\\xc3\\\\x8e\\\\xc4\\\\x85)',\n",
      " '',\n",
      " 'exp(c\\\\xc4\\\\x9a\\\\xc2\\\\x83>',\n",
      " 's vw )',\n",
      " ',',\n",
      " 'Zc\\\\xc4\\\\x9a\\\\xc2\\\\x83s',\n",
      " '',\n",
      " '>',\n",
      " 'w\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88V exp(c\\\\xc4\\\\x9a\\\\xc2\\\\x83s v w )',\n",
      " '',\n",
      " 'P',\n",
      " '',\n",
      " 'where Zc\\\\xc4\\\\x9a\\\\xc2\\\\x83s :=',\n",
      " 'and c\\\\xc4\\\\x9a\\\\xc2\\\\x83s := \\\\xc3\\\\x8e\\\\xcb\\\\x9bc0 + (1 '\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92',\n",
      " '\\\\xc3\\\\x8e\\\\xcb\\\\x9b)cs and \\\\xc3\\\\x8e\\\\xc4\\\\x85, \\\\xc3\\\\x8e\\\\xcb\\\\x9b are '\n",
      " 'scalars. c0 is the common discourse vector, representing a shared component '\n",
      " 'among all discourses,',\n",
      " 'mainly related to syntax. It allows the model to better generate syntactical '\n",
      " 'features. The \\\\xc3\\\\x8e\\\\xc4\\\\x85fw term is here to enable',\n",
      " 'the model to generate some frequent words even if their',\n",
      " 'matching with the discourse vector c\\\\xc4\\\\x9a\\\\xc2\\\\x83s is low.',\n",
      " 'Therefore, this model tries to generate sentences as a mixture of three type '\n",
      " 'of words: words matching the sentence',\n",
      " 'discourse vector cs , syntactical words matching c0 , and',\n",
      " 'words with high fw . (Arora et al., 2017) demonstrated',\n",
      " 'thatPfor this model, the MLE of c\\\\xc4\\\\x9a\\\\xc2\\\\x83s can be approximated',\n",
      " 'by w\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88S fwa+a v w , where a is a scalar. The '\n",
      " 'sentence discourse vector can hence be obtained by subtracting c0 estimated '\n",
      " 'by the first principal component of c\\\\xc4\\\\x9a\\\\xc2\\\\x83s '\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x99s on a set',\n",
      " 'of sentences. In other words, the sentence embeddings are',\n",
      " 'obtained by a weighted average of the word vectors stripping away the syntax '\n",
      " 'by subtracting the common discourse',\n",
      " 'vector and down-weighting frequent tokens. They generate sentence embeddings '\n",
      " 'from diverse pre-trained word',\n",
      " 'embeddings among which are unsupervised word embeddings such as GloVe '\n",
      " '(Pennington et al., 2014) as well as supervised word embeddings such as '\n",
      " 'paragram-SL999 (PSL)',\n",
      " '(Wieting et al., 2015) trained on the Paraphrase Database',\n",
      " '(Ganitkevitch et al., 2013).',\n",
      " 'In a very different line of work, C-PHRASE (Pham et al.,',\n",
      " '2015) relies on additional information from the syntactic',\n",
      " 'parse tree of each sentence, which is incorporated into the',\n",
      " 'C-BOW training objective.',\n",
      " '(Huang & Anandkumar, 2016) show that single layer',\n",
      " 'CNNs can be modeled using a tensor decomposition approach. While building on '\n",
      " 'an unsupervised objective, the',\n",
      " 'employed dictionary learning step for obtaining phrase',\n",
      " 'templates is task-specific (for each use-case), not resulting',\n",
      " 'in general-purpose embeddings.',\n",
      " '',\n",
      " '3.2. Unsupervised Models Depending on Sentence',\n",
      " 'Ordering',\n",
      " 'The SkipThought model (Kiros et al., 2015) combines',\n",
      " 'sentence level models with recurrent neural networks.',\n",
      " 'Given a sentence Si from an ordered corpus, the model is',\n",
      " 'trained to predict Si\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x921 and Si+1 .',\n",
      " 'FastSent (Hill et al., 2016a) is a sentence-level log-linear',\n",
      " 'bag-of-words model. Like SkipThought, it uses adjacent',\n",
      " 'sentences as the prediction target and is trained in an unsupervised '\n",
      " 'fashion. Using word sequences allows the model',\n",
      " 'to improve over the earlier work of paragraph2vec (Le &',\n",
      " 'Mikolov, 2014). (Hill et al., 2016a) augment FastSent further by training it '\n",
      " 'to predict the constituent words of the',\n",
      " 'sentence as well. This model is named FastSent + AE in',\n",
      " 'our comparisons.',\n",
      " 'Compared to our approach, Siamese C-BOW (Kenter',\n",
      " 'et al., 2016) shares the idea of learning to average word embeddings over a '\n",
      " 'sentence. However, it relies on a Siamese',\n",
      " 'neural network architecture to predict surrounding sentences, contrasting '\n",
      " 'our simpler unsupervised objective.',\n",
      " 'Note that on the character sequence level instead of word',\n",
      " 'sequences, FastText (Bojanowski et al., 2017) uses the',\n",
      " 'same conceptual model to obtain better word embeddings.',\n",
      " 'This is most similar to our proposed model, with two',\n",
      " 'key differences: Firstly, we predict from source word sequences to target '\n",
      " 'words, as opposed to character sequences',\n",
      " 'to target words, and secondly, our model is averaging the',\n",
      " 'source embeddings instead of summing them.',\n",
      " '3.3. Models requiring structured data',\n",
      " 'DictRep (Hill et al., 2016b) is trained to map dictionary',\n",
      " 'definitions of the words to the pre-trained word embeddings of these words. '\n",
      " 'They use two different architectures,',\n",
      " 'namely BOW and RNN (LSTM) with the choice of learning the input word '\n",
      " 'embeddings or using them pre-trained.',\n",
      " 'A similar architecture is used by the CaptionRep variant,',\n",
      " 'but here the task is the mapping of given image captions to',\n",
      " 'a pre-trained vector representation of these images.',\n",
      " '',\n",
      " '4. Evaluation Tasks',\n",
      " 'We use a standard set of supervised as well as unsupervised benchmark tasks '\n",
      " 'from the literature to evaluate our',\n",
      " 'trained models, following (Hill et al., 2016a). The breadth',\n",
      " 'of tasks allows to fairly measure generalization to a wide',\n",
      " 'area of different domains, testing the general-purpose quality '\n",
      " '(universality) of all competing sentence embeddings.',\n",
      " 'For downstream supervised evaluations, sentence embeddings are combined with '\n",
      " 'logistic regression to predict target labels. In the unsupervised evaluation '\n",
      " 'for sentence similarity, correlation of the cosine similarity between two '\n",
      " 'em-',\n",
      " '',\n",
      " '\\\\x0cUnsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " '',\n",
      " 'beddings is compared to human annotators.',\n",
      " 'Downstream Supervised Evaluation. Sentence embeddings are evaluated for '\n",
      " 'various supervised classification',\n",
      " 'tasks as follows. We evaluate paraphrase identification',\n",
      " '(MSRP) (Dolan et al., 2004), classification of movie review',\n",
      " 'sentiment (MR) (Pang & Lee, 2005), product reviews (CR)',\n",
      " '(Hu & Liu, 2004), subjectivity classification (SUBJ)(Pang',\n",
      " '& Lee, 2004), opinion polarity (MPQA) (Wiebe et al.,',\n",
      " '2005) and question type classification (TREC) (Voorhees,',\n",
      " '2002). To classify, we use the code provided by (Kiros',\n",
      " 'et al., 2015) in the same manner as in (Hill et al.,',\n",
      " '2016a). For the MSRP dataset, containing pairs of sentences (S1 , S2 ) with '\n",
      " 'associated paraphrase label, we generate feature vectors by concatenating '\n",
      " 'their Sent2Vec representations |v S1 \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92 v S2 | '\n",
      " 'with the component-wise product v S1 \\\\x0c v S2 . The predefined training '\n",
      " 'split is used to',\n",
      " 'tune the L2 penalty parameter using cross-validation and',\n",
      " 'the accuracy and F1 scores are computed on the test set.',\n",
      " 'For the remaining 5 datasets, Sent2Vec embeddings are inferred from input '\n",
      " 'sentences and directly fed to a logistic',\n",
      " 'regression classifier. Accuracy scores are obtained using',\n",
      " '10-fold cross-validation for the MR, CR, SUBJ and MPQA',\n",
      " 'datasets. For those datasets nested cross-validation is used',\n",
      " 'to tune the L2 penalty. For the TREC dataset, as for the',\n",
      " 'MRSP dataset, the L2 penalty is tuned on the predefined',\n",
      " 'train split using 10-fold cross-validation, and the accuracy',\n",
      " 'is computed on the test set.',\n",
      " 'Unsupervised Similarity Evaluation. We perform unsupervised evaluation of '\n",
      " 'the the learnt sentence embeddings using the sentence cosine similarity, on '\n",
      " 'the STS',\n",
      " '2014 (Agirre et al., 2014) and SICK 2014 (Marelli et al.,',\n",
      " '2014) datasets. These similarity scores are compared to the',\n",
      " 'gold-standard human judgements using Pearson\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x99s '\n",
      " 'r (Pearson, 1895) and Spearman\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x99s '\n",
      " '\\\\xc4\\\\x8e\\\\xc2\\\\x81 (Spearman, 1904) correlation',\n",
      " 'scores. The SICK dataset consists of about 10,000 sentence',\n",
      " 'pairs along with relatedness scores of the pairs. The STS',\n",
      " '2014 dataset contains 3,770 pairs, divided into six different categories on '\n",
      " 'the basis of origin of sentences/phrases',\n",
      " 'namely Twitter, headlines, news, forum, WordNet and images. See (Agirre et '\n",
      " 'al., 2014) for more precise information',\n",
      " 'on how the pairs have been created.',\n",
      " '',\n",
      " '5. Results and Discussion',\n",
      " 'In Tables 1 and 2, we compare our results with those obtained by (Hill et '\n",
      " 'al., 2016a) on different models. Along',\n",
      " 'with the models discussed in Section 3, this also includes',\n",
      " 'the sentence embedding baselines obtained by simple averaging of word '\n",
      " 'embeddings over the sentence, in both the',\n",
      " 'C-BOW and skip-gram variants. TF-IDF BOW is a representation consisting of '\n",
      " 'the counts of the 200,000 most common feature-words, weighed by their TF-IDF '\n",
      " 'frequencies.',\n",
      " 'To ensure coherence, we only include unsupervised mod-',\n",
      " '',\n",
      " 'els in the main paper. Performance of supervised and semisupervised models '\n",
      " 'on these evaluations can be observed in',\n",
      " 'Tables 6 and 7 in the supplementary material.',\n",
      " 'Downstream Supervised Evaluation Results. On running supervised evaluations '\n",
      " 'and observing the results in',\n",
      " 'Table 1, we find that on an average our models are second only to '\n",
      " 'SkipThought vectors. Also, both our models',\n",
      " 'achieve state of the art results on the CR task. We also observe that on '\n",
      " 'half of the supervised tasks, our unigrams +',\n",
      " 'bigram model is the the best model after SkipThought. Our',\n",
      " 'models are weaker on the MSRP task (which consists of the',\n",
      " 'identification of labelled paraphrases) compared to stateof-the-art methods. '\n",
      " 'However, we observe that the models',\n",
      " 'which perform extremely well on this task end up faring',\n",
      " 'very poorly on the other tasks, indicating a lack of generalizability.',\n",
      " 'On rest of the tasks, our models perform extremely well.',\n",
      " 'The SkipThought model is able to outperform our models',\n",
      " 'on most of the tasks as it is trained to predict the previous',\n",
      " 'and next sentences and a lot of tasks are able to make use of',\n",
      " 'this contextual information missing in our Sent2Vec models. For example, the '\n",
      " 'TREC task is a poor measure of how',\n",
      " 'one predicts the content of the sentence (the question) but',\n",
      " 'a good measure of how the next sentence in the sequence',\n",
      " '(the answer) is predicted.',\n",
      " 'Unsupervised Similarity Evaluation Results. In Table 2,',\n",
      " 'we see that our Sent2Vec models are state-of-the-art on the',\n",
      " 'majority of tasks when comparing to all the unsupervised',\n",
      " 'models trained on the Toronto corpus, and clearly achieve',\n",
      " 'the best averaged performance. Our Sent2Vec models also',\n",
      " 'on average outperform or are at par with the C-PHRASE',\n",
      " 'model, despite significantly lagging behind on the STS',\n",
      " '2014 WordNet and News subtasks. This observation can',\n",
      " 'be attributed to the fact that a big chunk of the data that',\n",
      " 'the C-PHRASE model is trained on comes from English',\n",
      " 'Wikipedia, helping it to perform well on datasets involving definition and '\n",
      " 'news items. Also, C-PHRASE uses data',\n",
      " 'three times the size of the Toronto book corpus. Interestingly, our model '\n",
      " 'outperforms C-PHRASE when trained on',\n",
      " 'Wikipedia, as shown in Table 3, despite the fact that we use',\n",
      " 'no parse tree information.',\n",
      " 'In the official results of the more recent edition of the STS',\n",
      " '2017 benchmark (Cer et al., 2017), our model also significantly outperforms '\n",
      " 'C-PHRASE, and delivers the best unsupervised baseline method.',\n",
      " 'Macro Average. To summarize our contributions on both',\n",
      " 'supervised and unsupervised tasks, in Table 3 we present',\n",
      " 'the results in terms of the macro average over the averages',\n",
      " '4',\n",
      " 'For the Siamese C-BOW model trained on the Toronto corpus, supervised '\n",
      " 'evaluation as well as similarity evaluation results',\n",
      " 'on the SICK 2014 dataset are unavailable.',\n",
      " '',\n",
      " '\\\\x0cUnsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " 'Data',\n",
      " '',\n",
      " 'Unordered Sentences:',\n",
      " '(Toronto Books;',\n",
      " '70 million sentences,',\n",
      " '0.9 Billion Words)',\n",
      " '',\n",
      " 'Ordered Sentences:',\n",
      " 'Toronto Books',\n",
      " '2.8 Billion words',\n",
      " '',\n",
      " 'Model',\n",
      " 'SAE',\n",
      " 'SAE + embs.',\n",
      " 'SDAE',\n",
      " 'SDAE + embs.',\n",
      " 'ParagraphVec DBOW',\n",
      " 'ParagraphVec DM',\n",
      " 'Skipgram',\n",
      " 'C-BOW',\n",
      " 'Unigram TFIDF',\n",
      " 'Sent2Vec uni.',\n",
      " 'Sent2Vec uni. + bi.',\n",
      " 'SkipThought',\n",
      " 'FastSent',\n",
      " 'FastSent+AE',\n",
      " 'C-PHRASE',\n",
      " '',\n",
      " 'MSRP (Acc / F1)',\n",
      " '74.3 / 81.7',\n",
      " '70.6 / 77.9',\n",
      " '76.4 / 83.4',\n",
      " '73.7 / 80.7',\n",
      " '72.9 / 81.1',\n",
      " '73.6 / 81.9',\n",
      " '69.3 / 77.2',\n",
      " '67.6 / 76.1',\n",
      " '73.6 / 81.7',\n",
      " '72.2 / 80.3',\n",
      " '72.5 / 80.8',\n",
      " '73.0 / 82.0',\n",
      " '72.2 / 80.3',\n",
      " '71.2 / 79.1',\n",
      " '72.2 / 79.6',\n",
      " '',\n",
      " 'MR',\n",
      " '62.6',\n",
      " '73.2',\n",
      " '67.6',\n",
      " '74.6',\n",
      " '60.2',\n",
      " '61.5',\n",
      " '73.6',\n",
      " '73.6',\n",
      " '73.7',\n",
      " '75.1',\n",
      " '75.8',\n",
      " '76.5',\n",
      " '70.8',\n",
      " '71.8',\n",
      " '75.7',\n",
      " '',\n",
      " 'CR',\n",
      " '68.0',\n",
      " '75.3',\n",
      " '74.0',\n",
      " '78.0',\n",
      " '66.9',\n",
      " '68.6',\n",
      " '77.3',\n",
      " '77.3',\n",
      " '79.2',\n",
      " '80.2',\n",
      " '80.3',\n",
      " '80.1',\n",
      " '78.4',\n",
      " '76.7',\n",
      " '78.8',\n",
      " '',\n",
      " 'SUBJ',\n",
      " '86.1',\n",
      " '89.8',\n",
      " '89.3',\n",
      " '90.8',\n",
      " '76.3',\n",
      " '76.4',\n",
      " '89.2',\n",
      " '89.1',\n",
      " '90.3',\n",
      " '90.6',\n",
      " '91.2',\n",
      " '93.6',\n",
      " '88.7',\n",
      " '88.8',\n",
      " '91.1',\n",
      " '',\n",
      " 'MPQA',\n",
      " '76.8',\n",
      " '86.2',\n",
      " '81.3',\n",
      " '86.9',\n",
      " '70.7',\n",
      " '78.1',\n",
      " '85.0',\n",
      " '85.0',\n",
      " '82.4',\n",
      " '86.3',\n",
      " '85.9',\n",
      " '87.1',\n",
      " '80.6',\n",
      " '81.5',\n",
      " '86.2',\n",
      " '',\n",
      " 'TREC',\n",
      " '80.2',\n",
      " '80.4',\n",
      " '77.7',\n",
      " '78.4',\n",
      " '59.4',\n",
      " '55.8',\n",
      " '82.2',\n",
      " '82.2',\n",
      " '85.0',\n",
      " '83.8',\n",
      " '86.4',\n",
      " '92.2',\n",
      " '76.8',\n",
      " '80.4',\n",
      " '78.8',\n",
      " '',\n",
      " 'Average',\n",
      " '74.7',\n",
      " '79.3',\n",
      " '78.3',\n",
      " '80.4',\n",
      " '67.7',\n",
      " '69.0',\n",
      " '78.5',\n",
      " '79.1',\n",
      " '80.7',\n",
      " '81.4',\n",
      " '82.0',\n",
      " '83.8',\n",
      " '77.9',\n",
      " '78.4',\n",
      " '80.5',\n",
      " '',\n",
      " 'Table 1: Comparison of the performance of different models on different '\n",
      " 'supervised evaluation tasks. An underline',\n",
      " 'indicates the best performance for the dataset. Top 3 performances in each '\n",
      " 'data category are shown in bold. The average',\n",
      " 'is calculated as the average of accuracy for each category (For MSRP, we '\n",
      " 'take the average of two entries.)',\n",
      " 'Model',\n",
      " 'SAE',\n",
      " 'SAE + embs.',\n",
      " 'SDAE',\n",
      " 'SDAE + embs.',\n",
      " 'ParagraphVec DBOW',\n",
      " 'ParagraphVec DM',\n",
      " 'Skipgram',\n",
      " 'C-BOW',\n",
      " 'Unigram TF-IDF',\n",
      " 'Sent2Vec uni.',\n",
      " 'Sent2Vec uni. + bi.',\n",
      " 'SkipThought',\n",
      " 'FastSent',\n",
      " 'FastSent+AE',\n",
      " 'Siamese C-BOW4',\n",
      " 'C-PHRASE',\n",
      " '',\n",
      " 'News',\n",
      " '.17/.16',\n",
      " '.52/.54',\n",
      " '.07/.04',\n",
      " '.51/.54',\n",
      " '.31/.34',\n",
      " '.42/.46',\n",
      " '.56/.59',\n",
      " '.57/.61',\n",
      " '.48/.48',\n",
      " '.62/.67',\n",
      " '.62/.67',\n",
      " '.44/.45',\n",
      " '.58/.59',\n",
      " '.56/.59',\n",
      " '.58/.59',\n",
      " '.69/.71',\n",
      " '',\n",
      " 'Forum',\n",
      " '.12/.12',\n",
      " '.22/.23',\n",
      " '.11/.13',\n",
      " '.29/.29',\n",
      " '.32/.32',\n",
      " '.33/.34',\n",
      " '.42/.42',\n",
      " '.43/.44',\n",
      " '.40/.38',\n",
      " '.49/.49',\n",
      " '.51/.51',\n",
      " '.14/.15',\n",
      " '.41/.36',\n",
      " '.41/.40',\n",
      " '.42/.41',\n",
      " '.43/.41',\n",
      " '',\n",
      " 'STS 2014',\n",
      " 'WordNet',\n",
      " 'Twitter',\n",
      " '.30/.23',\n",
      " '.28/.22',\n",
      " '.60/.55',\n",
      " '.60/.60',\n",
      " '.33/.24',\n",
      " '.44/.42',\n",
      " '.56/.50',\n",
      " '.57/.58',\n",
      " '.53/.50',\n",
      " '.43/.46',\n",
      " '.51/.48',\n",
      " '.54/.57',\n",
      " '.73/.70',\n",
      " '.71/.74',\n",
      " '.72/.69',\n",
      " '.71/.75',\n",
      " '.60/.59',\n",
      " '.63/.65',\n",
      " '.75/.72',\n",
      " '.70/.75',\n",
      " '.71/.68',\n",
      " '.70/.75',\n",
      " '.39/.34',\n",
      " '.42/.43',\n",
      " '.74/.70',\n",
      " '.63/.66',\n",
      " '.69/.64',\n",
      " '.70/.74',\n",
      " '.66/.61',\n",
      " '.71/.73',\n",
      " '.76/.73',\n",
      " '.60/.65',\n",
      " '',\n",
      " 'Images',\n",
      " '.49/.46',\n",
      " '.64/.64',\n",
      " '.44/.38',\n",
      " '.59/.59',\n",
      " '.46/.44',\n",
      " '.32/.30',\n",
      " '.65/.67',\n",
      " '.71/.73',\n",
      " '.72/.74',\n",
      " '.78/.82',\n",
      " '.75/.79',\n",
      " '.55/.60',\n",
      " '.74/.78',\n",
      " '.63/.65',\n",
      " '.65/.65',\n",
      " '.75/.79',\n",
      " '',\n",
      " 'Headlines',\n",
      " '.13/.11',\n",
      " '.41/.41',\n",
      " '.36/.36',\n",
      " '.43/.44',\n",
      " '.39/.41',\n",
      " '.46/.47',\n",
      " '.55/.58',\n",
      " '.55/.59',\n",
      " '.49/.49',\n",
      " '.61/.63',\n",
      " '.59/.62',\n",
      " '.43/.44',\n",
      " '.57/.59',\n",
      " '.58/.60',\n",
      " '.63/.64',\n",
      " '.60/.65',\n",
      " '',\n",
      " 'SICK 2014',\n",
      " 'Test + Train',\n",
      " '.32/.31',\n",
      " '.47/.49',\n",
      " '.46/.46',\n",
      " '.46/.46',\n",
      " '.42/.46',\n",
      " '.44/.40',\n",
      " '.60/.69',\n",
      " '.60/.69',\n",
      " '.52/.58',\n",
      " '.61/.70',\n",
      " '.62/.70',\n",
      " '.57/.60',\n",
      " '.61/.72',\n",
      " '.60/.65',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92',\n",
      " '.60/.72',\n",
      " '',\n",
      " 'Average',\n",
      " '.26/.23',\n",
      " '.50/.49',\n",
      " '.31/.29',\n",
      " '.49/.49',\n",
      " '.41/.42',\n",
      " '.43/.43',\n",
      " '.60/.63',\n",
      " '.60/.65',\n",
      " '.55/.56',\n",
      " '.65/.68',\n",
      " '.65/.67',\n",
      " '.42/.43',\n",
      " '.61/.63',\n",
      " '.60/.61',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92',\n",
      " '.63/.67',\n",
      " '',\n",
      " 'Table 2: Unsupervised Evaluation Tasks: Comparison of the performance of '\n",
      " 'different models on Spearman/Pearson',\n",
      " 'correlation measures. An underline indicates the best performance for the '\n",
      " 'dataset. Top 3 performances in each data',\n",
      " 'category are shown in bold. The average is calculated as the average of '\n",
      " 'entries for each correlation measure.',\n",
      " 'of both supervised and unsupervised tasks along with the',\n",
      " 'training times of the models5 . For unsupervised tasks, averages are taken '\n",
      " 'over both Spearman and Pearson scores.',\n",
      " 'The comparison includes the best performing unsupervised',\n",
      " 'and semi-supervised methods described in Section 3. For',\n",
      " 'models trained on the Toronto books dataset, we report a',\n",
      " '3.8 % points improvement over the state of the art. Considering all '\n",
      " 'supervised, semi-supervised methods and all',\n",
      " 'datasets compared in (Hill et al., 2016a), we report a 2.2 %',\n",
      " 'points improvement.',\n",
      " 'We also see a noticeable improvement in accuracy as we',\n",
      " 'use larger datasets like twitter and Wikipedia dump. We',\n",
      " 'can also see that the Sent2Vec models are also faster to train',\n",
      " 'when compared to methods like SkipThought and DictRep',\n",
      " 'owing to the SGD step allowing a high degree of parallelizability.',\n",
      " 'We can clearly see Sent2Vec outperforming other unsupervised and even '\n",
      " 'semi-supervised methods. This can be at5',\n",
      " '',\n",
      " 'time taken to train C-PHRASE models is unavailable',\n",
      " '',\n",
      " 'tributed to the superior generalizability of our model across',\n",
      " 'supervised and unsupervised tasks.',\n",
      " 'Comparison with Arora et al. (2017). In Table 4, we',\n",
      " 'report an experimental comparison to the model of Arora',\n",
      " 'et al. (2017), which is particularly tailored to sentence similarity tasks. '\n",
      " 'In the table, the suffix W indicates that their',\n",
      " 'down-weighting scheme has been used, while the suffix',\n",
      " 'R indicates the removal of the first principal component.',\n",
      " 'They report values of a \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88 '\n",
      " '[10\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x924 , 10\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x923 ] as '\n",
      " 'giving the best',\n",
      " 'results and used a = 10\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x923 for all their '\n",
      " 'experiments. Their',\n",
      " 'down-weighting scheme hints us to reduce the importance',\n",
      " 'of syntactical features. To do so, we use a simple blacklist containing the '\n",
      " '25 most frequent tokens in the Twitter',\n",
      " 'corpus and discard them before averaging. Results are also',\n",
      " 'reported in Table 4.',\n",
      " 'We observe that our results are competitive with the embeddings of Arora et '\n",
      " 'al. (2017) for purely unsupervised methods. We confirm their empirical '\n",
      " 'finding that reducing the',\n",
      " 'influence of the syntax helps performance on semantic sim-',\n",
      " '',\n",
      " '\\\\x0cUnsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " 'Type',\n",
      " '',\n",
      " 'Training corpus',\n",
      " '',\n",
      " 'Method',\n",
      " '',\n",
      " 'unsupervised',\n",
      " 'unsupervised',\n",
      " 'unsupervised',\n",
      " 'unsupervised',\n",
      " 'unsupervised',\n",
      " 'unsupervised',\n",
      " 'semi-supervised',\n",
      " 'unsupervised',\n",
      " 'unsupervised',\n",
      " 'unsupervised',\n",
      " 'unsupervised',\n",
      " '',\n",
      " 'twitter (19.7B words)',\n",
      " 'twitter (19.7B words)',\n",
      " 'Wikipedia (1.7B words)',\n",
      " 'Wikipedia (1.7B words)',\n",
      " 'Toronto books (0.9B words)',\n",
      " 'Toronto books (0.9B words)',\n",
      " 'structured dictionary dataset',\n",
      " '2.8B words + parse info.',\n",
      " 'Toronto books (0.9B words)',\n",
      " 'Toronto books (0.9B words)',\n",
      " 'Toronto books (0.9B words)',\n",
      " '',\n",
      " 'Sent2Vec uni. + bi.',\n",
      " 'Sent2Vec uni.',\n",
      " 'Sent2Vec uni. + bi.',\n",
      " 'Sent2Vec uni.',\n",
      " 'Sent2Vec books uni.',\n",
      " 'Sent2Vec books uni. + bi.',\n",
      " 'DictRep BOW + emb',\n",
      " 'C-PHRASE',\n",
      " 'C-BOW',\n",
      " 'FastSent',\n",
      " 'SkipThought',\n",
      " '',\n",
      " 'Supervised',\n",
      " 'average',\n",
      " '83.5',\n",
      " '82.2',\n",
      " '83.3',\n",
      " '82.4',\n",
      " '81.4',\n",
      " '82.0',\n",
      " '80.5',\n",
      " '80.5',\n",
      " '79.1',\n",
      " '77.9',\n",
      " '83.8',\n",
      " '',\n",
      " 'Unsupervised',\n",
      " 'average',\n",
      " '68.3',\n",
      " '69.0',\n",
      " '66.2',\n",
      " '66.3',\n",
      " '66.7',\n",
      " '65.9',\n",
      " '66.9',\n",
      " '64.9',\n",
      " '62.8',\n",
      " '62.0',\n",
      " '42.5',\n",
      " '',\n",
      " 'Macro',\n",
      " 'average',\n",
      " '75.9',\n",
      " '75.6',\n",
      " '74.8',\n",
      " '74.3',\n",
      " '74.0',\n",
      " '74.0',\n",
      " '73.7',\n",
      " '72.7',\n",
      " '70.2',\n",
      " '70.0',\n",
      " '63.1',\n",
      " '',\n",
      " 'Training time',\n",
      " '(in hours)',\n",
      " '6.5*',\n",
      " '3*',\n",
      " '2*',\n",
      " '3.5*',\n",
      " '1*',\n",
      " '1.2*',\n",
      " '24**',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92',\n",
      " '2',\n",
      " '2',\n",
      " '336**',\n",
      " '',\n",
      " 'Table 3: Best unsupervised and semi-supervised methods ranked by macro '\n",
      " 'average along with their training times. ** indicates trained on GPU. * '\n",
      " 'indicates trained on a single node using 30 cores. Training times for '\n",
      " 'non-Sent2Vec models are',\n",
      " 'due to (Hill et al., 2016a)',\n",
      " 'Dataset',\n",
      " 'STS 2014',\n",
      " 'SICK 2014',\n",
      " '',\n",
      " 'Unsupervised',\n",
      " 'GloVe + W',\n",
      " '0.594',\n",
      " '0.705',\n",
      " '',\n",
      " 'Unsupervised',\n",
      " 'GloVe + WR',\n",
      " '0.685',\n",
      " '0.722',\n",
      " '',\n",
      " 'Semi-supervised',\n",
      " 'PSL + WR',\n",
      " '0.735',\n",
      " '0.729',\n",
      " '',\n",
      " 'Sent2Vec Unigrams',\n",
      " 'Tweets Model',\n",
      " '0.710',\n",
      " '0.710',\n",
      " '',\n",
      " 'Sent2Vec Unigrams',\n",
      " 'Tweets Model With Blacklist',\n",
      " '0.718',\n",
      " '0.719',\n",
      " '',\n",
      " 'Table 4: Comparison of the performance of the unsupervised and '\n",
      " 'semi-supervised sentence embeddings by (Arora et al.,',\n",
      " '2017) with our models, in terms of Pearson\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x99s '\n",
      " 'correlation.',\n",
      " 'to unsupervised evaluations but gives a significant boost-up',\n",
      " 'in accuracy on supervised tasks.',\n",
      " '',\n",
      " 'Figure 1: Left figure: the profile of the word vector L2 norms as a function '\n",
      " 'of log(fw ) for each vocabulary word',\n",
      " 'w, as learnt by our unigram model trained on Toronto',\n",
      " 'books. Right figure: down-weighting scheme proposed by',\n",
      " 'a',\n",
      " '.',\n",
      " 'Arora et al. (2017): weight(w) = a+f',\n",
      " 'w',\n",
      " '',\n",
      " 'ilarity tasks, and we show that applying a simple blacklist',\n",
      " 'already yields a noticeable amelioration. It is important to',\n",
      " 'note that the scores obtained from supervised task-specific',\n",
      " 'PSL embeddings trained for the purpose of semantic similarity outperform our '\n",
      " 'method on both SICK and average',\n",
      " 'STS 2014, which is expected as our model is trained purely',\n",
      " 'unsupervised.',\n",
      " 'The effect of datasets and n-grams. Despite being trained',\n",
      " 'on three very different datasets, all of our models generalize well to '\n",
      " 'sometimes very specific domains. Models',\n",
      " 'trained on Toronto Corpus are the state-of-the art on the',\n",
      " 'STS 2014 images dataset even beating the supervised CaptionRep model trained '\n",
      " 'on images. We also see that addition',\n",
      " 'of bigrams to our models doesn\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x99t help much when '\n",
      " 'it comes',\n",
      " '',\n",
      " 'On learning the importance and the direction of the',\n",
      " 'word vectors. Our model \\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93 by learning how to '\n",
      " 'generate',\n",
      " 'and compose word vectors \\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93 has to learn both '\n",
      " 'the direction',\n",
      " 'of the word embeddings as well as their norm. Considering the norms of the '\n",
      " 'used word vectors as by our averaging over the sentence, we observe an '\n",
      " 'interesting distribution of the '\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9cimportance\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9d of '\n",
      " 'each word. In Figure 1 we',\n",
      " 'show the profile of the L2 -norm as a function of log(fw ) for',\n",
      " 'each w \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88 V, and compare it to the static '\n",
      " 'down-weighting',\n",
      " 'mechanism of Arora et al. (2017). We can observe that our',\n",
      " 'model is learning to down-weight frequent tokens by itself.',\n",
      " 'It is also down-weighting rare tokens and the norm profile',\n",
      " 'seems to roughly follow Luhn\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x99s hypothesis '\n",
      " '(Luhn, 1958),',\n",
      " 'a well known information retrieval paradigm, stating that',\n",
      " 'mid-rank terms are the most significant to discriminate content. Modifying '\n",
      " 'the objective function would change the',\n",
      " 'weighting scheme learnt. From a more semantic oriented',\n",
      " 'objective, it should be possible to learn to attribute lower',\n",
      " 'norms for very frequent terms, to more specifically fit sentence similarity '\n",
      " 'tasks.',\n",
      " '',\n",
      " '6. Conclusion',\n",
      " 'In this paper, we introduced a novel unsupervised and computationally '\n",
      " 'efficient method to train and infer sentence',\n",
      " 'embeddings. On supervised evaluations, our method, on',\n",
      " 'an average, achieves better performance than all other unsupervised '\n",
      " 'competitors except the SkipThought vectors.',\n",
      " 'However, SkipThought vectors show an extremely poor',\n",
      " 'performance on sentence similarity tasks while our model',\n",
      " '',\n",
      " '\\\\x0cUnsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " '',\n",
      " 'is state-of-the-art for these evaluations on average. Future',\n",
      " 'work could focus on augmenting the model to exploit data',\n",
      " 'with ordered sentences. Furthermore, we would like to further investigate '\n",
      " 'the models ability as giving pre-trained embeddings to enable downstream '\n",
      " 'transfer learning tasks.',\n",
      " 'Acknowledgments. We are indebted to Piotr Bojanowski',\n",
      " 'and Armand Joulin for helpful discussions.',\n",
      " '',\n",
      " 'References',\n",
      " 'Agirre, Eneko, Banea, Carmen, Cardie, Claire, Cer, Daniel, Diab,',\n",
      " 'Mona, Gonzalez-Agirre, Aitor, Guo, Weiwei, Mihalcea, Rada,',\n",
      " 'Rigau, German, and Wiebe, Janyce. Semeval-2014 task 10:',\n",
      " 'Multilingual semantic textual similarity. In Proceedings of the',\n",
      " '8th international workshop on semantic evaluation (SemEval',\n",
      " '2014), pp. 81\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9391. Association for Computational '\n",
      " 'Linguistics',\n",
      " 'Dublin, Ireland, 2014.',\n",
      " 'Arora, Sanjeev, Li, Yuanzhi, Liang, Yingyu, Ma, Tengyu, and',\n",
      " 'Risteski, Andrej. A Latent Variable Model Approach to PMIbased Word '\n",
      " 'Embeddings. In Transactions of the Association',\n",
      " 'for Computational Linguistics, pp. 385\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93399, '\n",
      " 'July 2016.',\n",
      " 'Arora, Sanjeev, Liang, Yingyu, and Ma, Tengyu. A simple but',\n",
      " 'tough-to-beat baseline for sentence embeddings. In International Conference '\n",
      " 'on Learning Representations (ICLR), 2017.',\n",
      " 'Bird, Steven, Klein, Ewan, and Loper, Edward. Natural language',\n",
      " 'processing with Python: analyzing text with the natural language toolkit. '\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9d O\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x99Reilly Media, '\n",
      " 'Inc.\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9d, 2009.',\n",
      " 'Bojanowski, Piotr, Grave, Edouard, Joulin, Armand, and',\n",
      " 'Mikolov, Tomas. Enriching Word Vectors with Subword Information. '\n",
      " 'Transactions of the Association for Computational',\n",
      " 'Linguistics, 5:135\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93146, 2017.',\n",
      " 'Cer, Daniel, Diab, Mona, Agirre, Eneko, Lopez-Gazpio, Inigo,',\n",
      " 'and Specia, Lucia. SemEval-2017 Task 1: Semantic Textual',\n",
      " 'Similarity Multilingual and Cross-lingual Focused Evaluation.',\n",
      " 'In SemEval-2017 - Proceedings of the 11th International Workshop on Semantic '\n",
      " 'Evaluations, pp. 1\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9314, Vancouver, Canada,',\n",
      " 'August 2017. Association for Computational Linguistics.',\n",
      " 'Dolan, Bill, Quirk, Chris, and Brockett, Chris. Unsupervised',\n",
      " 'construction of large paraphrase corpora: Exploiting massively',\n",
      " 'parallel news sources. In Proceedings of the 20th international',\n",
      " 'conference on Computational Linguistics, pp. 350. Association',\n",
      " 'for Computational Linguistics, 2004.',\n",
      " 'Ganitkevitch, Juri, Van Durme, Benjamin, and Callison-Burch,',\n",
      " 'Chris. Ppdb: The paraphrase database. In HLT-NAACL, pp.',\n",
      " '758\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93764, 2013.',\n",
      " 'Goldberg, Yoav and Levy, Omer. word2vec Explained: deriving',\n",
      " 'Mikolov et al.\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x99s negative-sampling '\n",
      " 'word-embedding method.',\n",
      " 'arXiv, February 2014.',\n",
      " '',\n",
      " 'Hu, Minqing and Liu, Bing. Mining and summarizing customer',\n",
      " 'reviews. In Proceedings of the tenth ACM SIGKDD international conference on '\n",
      " 'Knowledge discovery and data mining,',\n",
      " 'pp. 168\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93177. ACM, 2004.',\n",
      " 'Huang, Furong and Anandkumar, Animashree. Unsupervised',\n",
      " 'Learning of Word-Sequence Representations from Scratch via',\n",
      " 'Convolutional Tensor Decomposition. arXiv, 2016.',\n",
      " 'Joulin, Armand, Grave, Edouard, Bojanowski, Piotr, and',\n",
      " 'Mikolov, Tomas. Bag of Tricks for Efficient Text Classification. In '\n",
      " 'Proceedings of the 15th Conference of the European Chapter of the '\n",
      " 'Association for Computational Linguistics, Short Papers, pp. '\n",
      " '427\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93431, Valencia, Spain, 2017.',\n",
      " 'Kenter, Tom, Borisov, Alexey, and de Rijke, Maarten. Siamese',\n",
      " 'CBOW: Optimizing Word Embeddings for Sentence Representations. In ACL - '\n",
      " 'Proceedings of the 54th Annual Meeting of',\n",
      " 'the Association for Computational Linguistics, pp. '\n",
      " '941\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93951,',\n",
      " 'Berlin, Germany, 2016.',\n",
      " 'Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan R, Zemel,',\n",
      " 'Richard, Urtasun, Raquel, Torralba, Antonio, and Fidler, Sanja.',\n",
      " 'Skip-Thought Vectors. In NIPS 2015 - Advances in Neural Information '\n",
      " 'Processing Systems 28, pp. 3294\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x933302, 2015.',\n",
      " 'Le, Quoc V and Mikolov, Tomas. Distributed Representations',\n",
      " 'of Sentences and Documents. In ICML 2014 - Proceedings of',\n",
      " 'the 31st International Conference on Machine Learning, volume 14, pp. '\n",
      " '1188\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x931196, 2014.',\n",
      " 'Levy, Omer, Goldberg, Yoav, and Dagan, Ido. Improving distributional '\n",
      " 'similarity with lessons learned from word embeddings.',\n",
      " 'Transactions of the Association for Computational Linguistics,',\n",
      " '3:211\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93225, 2015.',\n",
      " 'Luhn, Hans Peter. The automatic creation of literature abstracts. IBM '\n",
      " 'Journal of research and development, 2(2):159\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93',\n",
      " '165, 1958.',\n",
      " 'Manning, Christopher D, Surdeanu, Mihai, Bauer, John, Finkel,',\n",
      " 'Jenny Rose, Bethard, Steven, and McClosky, David. The stanford corenlp '\n",
      " 'natural language processing toolkit. In ACL (System Demonstrations), pp. '\n",
      " '55\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9360, 2014.',\n",
      " 'Marelli, Marco, Menini, Stefano, Baroni, Marco, Bentivogli,',\n",
      " 'Luisa, Bernardi, Raffaella, and Zamparelli, Roberto. A sick',\n",
      " 'cure for the evaluation of compositional distributional semantic models. In '\n",
      " 'LREC, pp. 216\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93223, 2014.',\n",
      " 'Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey.',\n",
      " 'Efficient estimation of word representations in vector space.',\n",
      " 'arXiv preprint arXiv:1301.3781, 2013a.',\n",
      " 'Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and',\n",
      " 'Dean, Jeff. Distributed Representations of Words and Phrases',\n",
      " 'and their Compositionality. In NIPS - Advances in Neural Information '\n",
      " 'Processing Systems 26, pp. 3111\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x933119, 2013b.',\n",
      " '',\n",
      " 'Hill, Felix, Cho, Kyunghyun, and Korhonen, Anna. Learning Distributed '\n",
      " 'Representations of Sentences from Unlabelled Data. In',\n",
      " 'Proceedings of NAACL-HLT, February 2016a.',\n",
      " '',\n",
      " 'Pang, Bo and Lee, Lillian. A sentimental education: Sentiment',\n",
      " 'analysis using subjectivity summarization based on minimum',\n",
      " 'cuts. In Proceedings of the 42nd annual meeting on Association for '\n",
      " 'Computational Linguistics, pp. 271. Association for',\n",
      " 'Computational Linguistics, 2004.',\n",
      " '',\n",
      " 'Hill, Felix, Cho, KyungHyun, Korhonen, Anna, and Bengio, Yoshua.',\n",
      " 'Learning to understand phrases by embedding the dictionary. TACL, '\n",
      " '4:17\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9330, 2016b. URL',\n",
      " 'https://tacl2013.cs.columbia.edu/ojs/',\n",
      " 'index.php/tacl/article/view/711.',\n",
      " '',\n",
      " 'Pang, Bo and Lee, Lillian. Seeing stars: Exploiting class relationships for '\n",
      " 'sentiment categorization with respect to rating scales.',\n",
      " 'In Proceedings of the 43rd annual meeting on association for',\n",
      " 'computational linguistics, pp. 115\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93124. '\n",
      " 'Association for Computational Linguistics, 2005.',\n",
      " '',\n",
      " '\\\\x0cUnsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " 'Pearson, Karl. Note on regression and inheritance in the case of',\n",
      " 'two parents. Proceedings of the Royal Society of London, 58:',\n",
      " '240\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93242, 1895.',\n",
      " 'Pennington, Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: '\n",
      " 'Global vectors for word representation. In',\n",
      " 'EMNLP, volume 14, pp. 1532\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x931543, 2014.',\n",
      " 'Pham, NT, Kruszewski, G, Lazaridou, A, and Baroni, M. Jointly',\n",
      " 'optimizing word representations for lexical and sentential tasks',\n",
      " 'with the c-phrase model. ACL/IJCNLP, 2015.',\n",
      " 'Rockafellar, R Tyrrell. Monotone operators and the proximal',\n",
      " 'point algorithm. SIAM journal on control and optimization,',\n",
      " '14(5):877\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93898, 1976.',\n",
      " 'Spearman, Charles. The proof and measurement of association',\n",
      " 'between two things. The American journal of psychology, 15',\n",
      " '(1):72\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93101, 1904.',\n",
      " 'Voorhees, Ellen M. Overview of the trec 2001 question answering',\n",
      " 'track. In NIST special publication, pp. 42\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x9351, '\n",
      " '2002.',\n",
      " 'Wiebe, Janyce, Wilson, Theresa, and Cardie, Claire. Annotating',\n",
      " 'expressions of opinions and emotions in language. Language',\n",
      " 'resources and evaluation, 39(2):165\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x93210, 2005.',\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, Livescu, Karen,',\n",
      " 'and Roth, Dan. From paraphrase database to compositional',\n",
      " 'paraphrase model and back. In TACL - Transactions of the',\n",
      " 'Association for Computational Linguistics, 2015.',\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, and Livescu,',\n",
      " 'Karen.',\n",
      " 'Towards universal paraphrastic sentence embeddings. In International '\n",
      " 'Conference on Learning Representations (ICLR), 2016a.',\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, and Livescu,',\n",
      " 'Karen. Charagram: Embedding Words and Sentences via',\n",
      " 'Character n-grams. In EMNLP - Proceedings of the 2016 Conference on '\n",
      " 'Empirical Methods in Natural Language Processing, pp. '\n",
      " '1504\\\\xc3\\\\xa2\\\\xc2\\\\x80\\\\xc2\\\\x931515, Stroudsburg, PA, USA, 2016b. '\n",
      " 'Association for Computational Linguistics.',\n",
      " '',\n",
      " '\\\\x0cUnsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " '',\n",
      " 'Supplementary Material',\n",
      " 'A. Parameters for training models',\n",
      " 'Model',\n",
      " 'Book corpus',\n",
      " 'Sent2Vec',\n",
      " 'unigrams',\n",
      " 'Book corpus',\n",
      " 'Sent2Vec',\n",
      " 'unigrams + bigrams',\n",
      " 'Wiki Sent2Vec',\n",
      " 'unigrams',\n",
      " 'Wiki Sent2Vec',\n",
      " 'unigrams + bigrams',\n",
      " 'Twitter Sent2Vec',\n",
      " 'unigrams',\n",
      " 'Twitter Sent2Vec',\n",
      " 'unigrams + bigrams',\n",
      " '',\n",
      " 'Embedding',\n",
      " 'Dimensions',\n",
      " '',\n",
      " 'Minimum',\n",
      " 'word count',\n",
      " '',\n",
      " 'Minimum',\n",
      " 'Target word',\n",
      " 'Count',\n",
      " '',\n",
      " 'Initial',\n",
      " 'Lear ning',\n",
      " 'Rate',\n",
      " '',\n",
      " 'Epochs',\n",
      " '',\n",
      " 'Subsampling',\n",
      " 'hyper-parameter',\n",
      " '',\n",
      " 'Bigrams',\n",
      " 'Dropped',\n",
      " 'per sentence',\n",
      " '',\n",
      " 'Number of',\n",
      " 'negatives',\n",
      " 'sampled',\n",
      " '',\n",
      " '700',\n",
      " '',\n",
      " '5',\n",
      " '',\n",
      " '8',\n",
      " '',\n",
      " '0.2',\n",
      " '',\n",
      " '13',\n",
      " '',\n",
      " '1 \\\\xc4\\\\x82\\\\xc2\\\\x97 10\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x925',\n",
      " '',\n",
      " '-',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '700',\n",
      " '',\n",
      " '5',\n",
      " '',\n",
      " '5',\n",
      " '',\n",
      " '0.2',\n",
      " '',\n",
      " '12',\n",
      " '',\n",
      " '5 \\\\xc4\\\\x82\\\\xc2\\\\x97 10\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x926',\n",
      " '',\n",
      " '7',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '600',\n",
      " '',\n",
      " '8',\n",
      " '',\n",
      " '20',\n",
      " '',\n",
      " '0.2',\n",
      " '',\n",
      " '9',\n",
      " '',\n",
      " '1 \\\\xc4\\\\x82\\\\xc2\\\\x97 10\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x925',\n",
      " '',\n",
      " '-',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '9',\n",
      " '',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x926',\n",
      " '',\n",
      " '4',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x926',\n",
      " '',\n",
      " '700',\n",
      " '',\n",
      " '8',\n",
      " '',\n",
      " '20',\n",
      " '',\n",
      " '0.2',\n",
      " '',\n",
      " '5 \\\\xc4\\\\x82\\\\xc2\\\\x97 10',\n",
      " '',\n",
      " '700',\n",
      " '',\n",
      " '20',\n",
      " '',\n",
      " '20',\n",
      " '',\n",
      " '0.2',\n",
      " '',\n",
      " '3',\n",
      " '',\n",
      " '1 \\\\xc4\\\\x82\\\\xc2\\\\x97 10',\n",
      " '',\n",
      " '-',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " '700',\n",
      " '',\n",
      " '20',\n",
      " '',\n",
      " '20',\n",
      " '',\n",
      " '0.2',\n",
      " '',\n",
      " '3',\n",
      " '',\n",
      " '1 \\\\xc4\\\\x82\\\\xc2\\\\x97 10\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x926',\n",
      " '',\n",
      " '3',\n",
      " '',\n",
      " '10',\n",
      " '',\n",
      " 'Table 5: Training parameters for the Sent2Vec models',\n",
      " '',\n",
      " 'B. L1 regularization of models',\n",
      " 'Optionally, our model can be additionally improved by adding an L1 '\n",
      " 'regularizer term in the objective function, leading to',\n",
      " 'slightly better generalization performance. Additionally, encouraging '\n",
      " 'sparsity in the embedding vectors is beneficial for',\n",
      " 'memory reasons, allowing higher embedding dimensions h.',\n",
      " 'We propose to apply L1 regularization individually to each word (and n-gram) '\n",
      " 'vector (both source and target vectors).',\n",
      " 'Formally, the training objective function (3) then becomes',\n",
      " '\\\\x12\\\\x10',\n",
      " '\\\\x11',\n",
      " 'X X',\n",
      " '\\\\x01',\n",
      " '(4)',\n",
      " 'min',\n",
      " 'qp (wt ) ` u>',\n",
      " 'wt v S\\\\\\\\{wt } + \\\\xc4\\\\x8e\\\\xc2\\\\x84 (kuwt k1 + kv S\\\\\\\\{wt } k1 ) +',\n",
      " 'U ,V',\n",
      " '',\n",
      " 'S\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88C wt \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88S',\n",
      " '',\n",
      " '|Nwt |',\n",
      " '',\n",
      " 'X',\n",
      " '',\n",
      " '\\\\x11\\\\x13',\n",
      " '\\\\x10',\n",
      " '\\\\x01',\n",
      " '>',\n",
      " 'qn (w ) ` \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92 uw0 v S\\\\\\\\{wt } + '\n",
      " '\\\\xc4\\\\x8e\\\\xc2\\\\x84 (kuw0 k1 )',\n",
      " '0',\n",
      " '',\n",
      " 'w0 \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x88V',\n",
      " '',\n",
      " 'where \\\\xc4\\\\x8e\\\\xc2\\\\x84 is the regularization parameter.',\n",
      " 'Now, in order to minimize a function of the form f (z) + g(z) where g(z) is '\n",
      " 'not differentiable over the domain, we can use',\n",
      " 'the basic proximal-gradient scheme. In this iterative method, after doing a '\n",
      " 'gradient descent step on f (z) with learning rate',\n",
      " '\\\\xc3\\\\x8e\\\\xc4\\\\x85, we update z as',\n",
      " 'zn+1 = prox\\\\xc3\\\\x8e\\\\xc4\\\\x85,g (zn+ 12 )',\n",
      " '',\n",
      " '(5)',\n",
      " '',\n",
      " '1',\n",
      " 'where prox\\\\xc3\\\\x8e\\\\xc4\\\\x85,g (x) = arg miny {g(y) + '\n",
      " '2\\\\xc3\\\\x8e\\\\xc4\\\\x85',\n",
      " 'ky \\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92 xk22 } is called the proximal '\n",
      " 'function(Rockafellar, 1976) of g with \\\\xc3\\\\x8e\\\\xc4\\\\x85 being',\n",
      " 'the proximal parameter and zn+ 21 is the value of z after a gradient (or '\n",
      " 'SGD) step on zn .',\n",
      " '',\n",
      " 'In our case, g(z) = kzk1 and the corresponding proximal operator is given by',\n",
      " 'prox\\\\xc3\\\\x8e\\\\xc4\\\\x85,g (x) = sign(x) \\\\x0c max(|xn | '\n",
      " '\\\\xc3\\\\xa2\\\\xc2\\\\x88\\\\xc2\\\\x92 \\\\xc3\\\\x8e\\\\xc4\\\\x85, 0)',\n",
      " '',\n",
      " '(6)',\n",
      " '',\n",
      " 'where \\\\x0c corresponds to element-wise product.',\n",
      " 'Similar to the proximal-gradient scheme, in our case we can optionally use '\n",
      " 'the thresholding operator on the updated word',\n",
      " '\\\\xc4\\\\x8e\\\\xc2\\\\x84 \\\\xc3\\\\x82\\\\xcb\\\\x87lr 0',\n",
      " 'and n-gram vectors after an SGD step. The soft thresholding parameter used '\n",
      " 'for this update is |R(S\\\\\\\\{w',\n",
      " 'and \\\\xc4\\\\x8e\\\\xc2\\\\x84 \\\\xc3\\\\x82\\\\xcb\\\\x87 lr0 for',\n",
      " 't })|',\n",
      " '0',\n",
      " 'the source and target vectors respectively where lr is the current learning '\n",
      " 'rate, \\\\xc4\\\\x8e\\\\xc2\\\\x84 is the L1 regularization parameter and S',\n",
      " 'is the sentence on which SGD is being run.',\n",
      " '',\n",
      " '\\\\x0cUnsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " '',\n",
      " 'We observe that L1 regularization using the proximal step gives our models a '\n",
      " 'small boost in performance. Also, applying',\n",
      " 'the thresholding operator takes only |R(S\\\\\\\\{wt })|\\\\xc3\\\\x82\\\\xcb\\\\x87h '\n",
      " 'floating point operations for the updating the word vectors corresponding to '\n",
      " 'the sentence and (|N | + 1) \\\\xc3\\\\x82\\\\xcb\\\\x87 h for updating the target '\n",
      " 'as well as the negative word vectors, where |N | is the number of',\n",
      " 'negatives sampled and h is the embedding dimension. Thus, performing L1 '\n",
      " 'regularization using soft-thresholding operator',\n",
      " 'comes with a small computational overhead.',\n",
      " 'We set \\\\xc4\\\\x8e\\\\xc2\\\\x84 to be 0.0005 for both the Wikipedia and the '\n",
      " 'Toronto Book Corpus unigrams + bigrams models.',\n",
      " '',\n",
      " 'C. Performance comparison with Sent2Vec models trained on different corpora',\n",
      " 'Data',\n",
      " 'Unordered Sentences:',\n",
      " '(Toronto Books)',\n",
      " 'Unordered sentences: Wikipedia',\n",
      " '(69 million sentences; 1.7 B words)',\n",
      " 'Unordered sentences: Twitter',\n",
      " '(1.2 billion sentences; 19.7 B words)',\n",
      " '',\n",
      " 'Other structured',\n",
      " 'Data Sources',\n",
      " '',\n",
      " 'Model',\n",
      " 'Sent2Vec uni.',\n",
      " 'Sent2Vec uni. + bi.',\n",
      " 'Sent2Vec uni. + bi.L1reg',\n",
      " 'Sent2Vec uni.',\n",
      " 'Sent2Vec uni. + bi.',\n",
      " 'Sent2Vec uni. + bi.L1reg',\n",
      " 'Sent2Vec uni.',\n",
      " 'Sent2Vec uni. + bi.',\n",
      " 'CaptionRep BOW',\n",
      " 'CaptionRep RNN',\n",
      " 'DictRep BOW',\n",
      " 'DictRep BOW+embs',\n",
      " 'DictRep RNN',\n",
      " 'DictRep RNN+embs.',\n",
      " '',\n",
      " 'MSRP (Acc / F1)',\n",
      " '72.2 / 80.3',\n",
      " '72.5 / 80.8',\n",
      " '71.6 / 80.1',\n",
      " '71.8 / 80.2',\n",
      " '72.4 / 80.8',\n",
      " '73.6 / 81.5',\n",
      " '71.5 / 80.0',\n",
      " '72.4 / 80.6',\n",
      " '73.6 / 81.9',\n",
      " '72.6 / 81.1',\n",
      " '73.7 / 81.6',\n",
      " '68.4 / 76.8',\n",
      " '73.2 / 81.6',\n",
      " '66.8 / 76.0',\n",
      " '',\n",
      " 'MR',\n",
      " '75.1',\n",
      " '75.8',\n",
      " '76.1',\n",
      " '77.3',\n",
      " '77.9',\n",
      " '78.1',\n",
      " '77.1',\n",
      " '78.0',\n",
      " '61.9',\n",
      " '55.0',\n",
      " '71.3',\n",
      " '76.7',\n",
      " '67.8',\n",
      " '72.5',\n",
      " '',\n",
      " 'CR',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " '80.2',\n",
      " '80.3',\n",
      " '80.9',\n",
      " '80.3',\n",
      " '80.9',\n",
      " '81.5',\n",
      " '81.3',\n",
      " '82.1',\n",
      " '69.3',\n",
      " '64.9',\n",
      " '75.6',\n",
      " '78.7',\n",
      " '72.7',\n",
      " '73.5',\n",
      " '',\n",
      " 'SUBJ',\n",
      " '90.6',\n",
      " '91.2',\n",
      " '91.1',\n",
      " '92.0',\n",
      " '92.6',\n",
      " '92.8',\n",
      " '90.8',\n",
      " '91.8',\n",
      " '77.4',\n",
      " '64.9',\n",
      " '86.6',\n",
      " '90.7',\n",
      " '81.4',\n",
      " '85.6',\n",
      " '',\n",
      " 'MPQA',\n",
      " '86.3',\n",
      " '85.9',\n",
      " '86.1',\n",
      " '87.4',\n",
      " '86.9',\n",
      " '87.2',\n",
      " '87.3',\n",
      " '86.7',\n",
      " '70.8',\n",
      " '71.0',\n",
      " '82.5',\n",
      " '87.2',\n",
      " '82.5',\n",
      " '85.7',\n",
      " '',\n",
      " 'TREC',\n",
      " '83.8',\n",
      " '86.4',\n",
      " '86.8',\n",
      " '85.4',\n",
      " '89.2',\n",
      " '87.4',\n",
      " '85.4',\n",
      " '89.8',\n",
      " '72.2',\n",
      " '62.4',\n",
      " '73.8',\n",
      " '81.0',\n",
      " '75.8',\n",
      " '72.0',\n",
      " '',\n",
      " 'Average',\n",
      " '81.4',\n",
      " '82.0',\n",
      " '82.1',\n",
      " '82.4',\n",
      " '83.3',\n",
      " '83.4',\n",
      " '82.2',\n",
      " '83.5',\n",
      " '70.9',\n",
      " '65.1',\n",
      " '77.3',\n",
      " '80.5',\n",
      " '75.6',\n",
      " '76.0',\n",
      " '',\n",
      " 'Table 6: Comparison of the performance of different Sent2Vec models with '\n",
      " 'different semi-supervised/supervised models',\n",
      " 'on different downstream supervised evaluation tasks. An underline indicates '\n",
      " 'the best performance for the dataset and',\n",
      " 'Sent2Vec model performances are bold if they perform as well or better than '\n",
      " 'all other non-Sent2Vec models, including',\n",
      " 'those presented in Table 1.',\n",
      " '',\n",
      " 'Model',\n",
      " 'Sent2Vec book corpus uni.',\n",
      " 'Sent2Vec book corpus uni. + bi.',\n",
      " 'Sent2Vec book corpus uni. + bi. L1 reg',\n",
      " 'Sent2Vec wiki uni.',\n",
      " 'Sent2Vec wiki uni. + bi.',\n",
      " 'Sent2Vec wiki uni. + bi. L1 reg',\n",
      " 'Sent2Vec twitter uni.',\n",
      " 'Sent2Vec twitter uni. + bi.',\n",
      " 'CaptionRep BOW',\n",
      " 'CaptionRep RNN',\n",
      " 'DictRep BOW',\n",
      " 'DictRep BOW + embs.',\n",
      " 'DictRep RNN',\n",
      " 'DictRep RNN + embs.',\n",
      " '',\n",
      " 'News',\n",
      " '.62/.67',\n",
      " '.62/.67',\n",
      " '.62/.68',\n",
      " '.66/.71',\n",
      " '.68/.74',\n",
      " '.69/.75',\n",
      " '.67/.74',\n",
      " '.68/.74',\n",
      " '.26/.26',\n",
      " '.05/.05',\n",
      " '.62/.67',\n",
      " '.65/.72',\n",
      " '.40/.46',\n",
      " '.51/.60',\n",
      " '',\n",
      " 'Forum',\n",
      " '.49/.49',\n",
      " '.51/.51',\n",
      " '.51/.52',\n",
      " '.47/.47',\n",
      " '.50/.50',\n",
      " '.52/.52',\n",
      " '.52/.53',\n",
      " '.54/.54',\n",
      " '.29/.22',\n",
      " '.13/.09',\n",
      " '.42/.40',\n",
      " '.49/.47',\n",
      " '.26/.23',\n",
      " '.29/.27',\n",
      " '',\n",
      " 'STS 2014',\n",
      " 'WordNet',\n",
      " 'Twitter',\n",
      " '.75/.72.',\n",
      " '.70/.75',\n",
      " '.71/.68',\n",
      " '.70/.75',\n",
      " '.72/.70',\n",
      " '.69/.75',\n",
      " '.70/.68',\n",
      " '.68/.72',\n",
      " '.66/.64',\n",
      " '.67/.72',\n",
      " '.72/.69',\n",
      " '.67/.72',\n",
      " '.75/.72',\n",
      " '.72/.78',\n",
      " '.72/.69',\n",
      " '.70/.77',\n",
      " '.50/.35',\n",
      " '.37/.31',\n",
      " '.40/.33',\n",
      " '.36/.30',\n",
      " '.81/.81',\n",
      " '.62/.66',\n",
      " '.85/.86',\n",
      " '.67/.72',\n",
      " '.78/.78',\n",
      " '.42/.42',\n",
      " '.80/.81',\n",
      " '.44/.47',\n",
      " '',\n",
      " 'Images',\n",
      " '.78/.82',\n",
      " '.75/.79',\n",
      " '.76/.81',\n",
      " '.76/.79',\n",
      " '.75/.79',\n",
      " '.76/.80',\n",
      " '.77/.81',\n",
      " '.76/.79',\n",
      " '.78/.81',\n",
      " '.76/.82',\n",
      " '.66/.68',\n",
      " '.71/.74',\n",
      " '.56/.56',\n",
      " '.65/.70',\n",
      " '',\n",
      " 'Headlines',\n",
      " '.61/.63',\n",
      " '.59/.62',\n",
      " '.60/.63',\n",
      " '.63/.67',\n",
      " '.62/.67',\n",
      " '.61/.66',\n",
      " '.64/.68',\n",
      " '.62/.67',\n",
      " '.39/.36',\n",
      " '.30/.28',\n",
      " '.53/.58',\n",
      " '.57/.61',\n",
      " '.38/.40',\n",
      " '.42/.46',\n",
      " '',\n",
      " 'SICK 2014',\n",
      " 'Test + Train',\n",
      " '.61/.70',\n",
      " '.62/.70',\n",
      " '.62/.71',\n",
      " '.64/.71',\n",
      " '.63/.71',\n",
      " '.63/.72',\n",
      " '.62/.71',\n",
      " '.63/.72',\n",
      " '.45/.44',\n",
      " '.36/.35',\n",
      " '.61/.63',\n",
      " '.61/.70',\n",
      " '.47/.49',\n",
      " '.52/.56',\n",
      " '',\n",
      " 'Average',\n",
      " '.65/.68',\n",
      " '.65/.67',\n",
      " '.66/.68',\n",
      " '.65/.68',\n",
      " '.65/.68',\n",
      " '.66/.69',\n",
      " '.67/.71',\n",
      " '.66/.70',\n",
      " '.54/.62',\n",
      " '.51/.59',\n",
      " '.58/.66',\n",
      " '.62/.70',\n",
      " '.49/.55',\n",
      " '.49/.59',\n",
      " '',\n",
      " 'Table 7: Unsupervised Evaluation: Comparison of the performance of different '\n",
      " 'Sent2Vec models with semisupervised/supervised models on Spearman/Pearson '\n",
      " 'correlation measures. An underline indicates the best performance',\n",
      " 'for the dataset and Sent2Vec model performances are bold if they perform as '\n",
      " 'well or better than all other non-Sent2Vec',\n",
      " 'models, including those presented in Table 2.',\n",
      " '',\n",
      " 'D. Dataset Description',\n",
      " 'Sentence Length',\n",
      " 'Average',\n",
      " 'Standard Deviation',\n",
      " '',\n",
      " 'News',\n",
      " '17.23',\n",
      " '8.66',\n",
      " '',\n",
      " 'Forum',\n",
      " '10.12',\n",
      " '3.30',\n",
      " '',\n",
      " 'STS 2014',\n",
      " 'WordNet',\n",
      " 'Twitter',\n",
      " '8.85',\n",
      " '11.64',\n",
      " '3.10',\n",
      " '5.28',\n",
      " '',\n",
      " 'Images',\n",
      " '10.17',\n",
      " '2.77',\n",
      " '',\n",
      " 'Headlines',\n",
      " '7.82',\n",
      " '2.21',\n",
      " '',\n",
      " 'SICK 2014',\n",
      " 'Test + Train',\n",
      " '9.67',\n",
      " '3.75',\n",
      " '',\n",
      " 'Wikipedia',\n",
      " 'Dataset',\n",
      " '25.25',\n",
      " '12.56',\n",
      " '',\n",
      " 'Twitter',\n",
      " 'Dataset',\n",
      " '16.31',\n",
      " '7.22',\n",
      " '',\n",
      " 'Table 8: Average sentence lengths for the datasets used in the comparison.',\n",
      " '',\n",
      " 'Book Corpus',\n",
      " 'Dataset',\n",
      " '13.32',\n",
      " '8.94',\n",
      " '',\n",
      " \"\\\\x0c'\"]\n"
     ]
    }
   ],
   "source": [
    "# Read text from pdf file and split by lines.\n",
    "import textract\n",
    "from pprint import pprint\n",
    "raw_text = textract.process('example2.pdf')\n",
    "pprint(str(raw_text).split('\\\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2. Text cleaning & merging lines into paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Unsupervised Learning of Sentence Embeddings\n",
      "using Compositional n-Gram Features\n",
      "\n",
      "Matteo Pagliardini * 1 Prakhar Gupta * 2 Martin Jaggi 2\n",
      "\n",
      "arXiv:1703.02507v2 [cs.CL] 10 Jul 2017\n",
      "\n",
      "Abstract\n",
      "The recent tremendous success of unsupervised\n",
      "word embeddings in a multitude of applications\n",
      "raises the obvious question if similar methods\n",
      "could be derived to improve embeddings (i.e.\n",
      "semantic representations) of word sequences as\n",
      "well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms\n",
      "the state-of-the-art unsupervised models on most\n",
      "benchmark tasks, highlighting the robustness of\n",
      "the produced general-purpose sentence embeddings.\n",
      "\n",
      "1. Introduction\n",
      "Improving unsupervised learning is of key importance for\n",
      "advancing machine learning methods, as to unlock access\n",
      "to almost unlimited amounts of data to be used as training\n",
      "resources. The majority of recent success stories of deep\n",
      "learning does not fall into this category but instead relied\n",
      "on supervised training (in particular in the vision domain).\n",
      "A very notable exception comes from the text and natural\n",
      "language processing domain, in the form of semantic word\n",
      "embeddings trained unsupervised (Mikolov et al., 2013b;a;\n",
      "Pennington et al., 2014). Within only a few years from their\n",
      "invention, such word representations  which are based on\n",
      "a simple matrix factorization model as we formalize below\n",
      " are now routinely trained on very large amounts of raw\n",
      "text data, and have become ubiquitous building blocks of a\n",
      "majority of current state-of-the-art NLP applications.\n",
      "\n",
      "learning for NLP leads towards increasingly powerful\n",
      "and complex models, such as recurrent neural networks\n",
      "(RNNs), LSTMs, attention models and even Neural Turing\n",
      "Machine architectures. While extremely strong in expressiveness, the increased model complexity makes such models much slower to train on larger datasets. On the other end\n",
      "of the spectrum, simpler shallow models such as matrix\n",
      "factorizations (or bilinear models) can benefit from training\n",
      "on much larger sets of data, which can be a key advantage,\n",
      "especially in the unsupervised setting.\n",
      "Surprisingly, for constructing sentence embeddings,\n",
      "naively using averaged word vectors was recently shown\n",
      "to outperform LSTMs (see (Wieting et al., 2016a) for plain\n",
      "averaging, and (Arora et al., 2017) for weighted averaging). This example shows potential in exploiting the tradeoff between model complexity and ability to process huge\n",
      "amounts of text using scalable algorithms, towards the simpler side. In view of this trade-off, our work here further\n",
      "advances unsupervised learning of sentence embeddings.\n",
      "Our proposed model can be seen as an extension of the CBOW (Mikolov et al., 2013b;a) training objective to train\n",
      "sentence instead of word embeddings. We demonstrate that\n",
      "the empirical performance of our resulting general-purpose\n",
      "sentence embeddings very significantly exceeds the state of\n",
      "the art, while keeping the model simplicity as well as training and inference complexity exactly as low as in averaging\n",
      "methods (Wieting et al., 2016a; Arora et al., 2017), thereby\n",
      "also putting the title of (Arora et al., 2017) in perspective.\n",
      "Contributions. The main contributions in this work can\n",
      "be summarized as follows:1\n",
      "\n",
      "While very useful semantic representations are available\n",
      "for words, it remains challenging to produce and learn such\n",
      "semantic embeddings for longer pieces of text, such as sentences, paragraphs or entire documents. Even more so, it\n",
      "remains a key goal to learn such general-purpose representations in an unsupervised way.\n",
      "\n",
      " Model. We propose Sent2Vec, a simple unsupervised\n",
      "model allowing to compose sentence embeddings using the word vectors along with n-gram embeddings,\n",
      "simultaneously training composition and the embedding vectors themselves.\n",
      "\n",
      "Currently, two contrary research trends have emerged in\n",
      "text understanding: On one hand, a strong trend in deep-\n",
      "\n",
      " Scalability. The computational complexity of our embeddings is only O(1) vector operations per word processed, both during training and inference of the sen-\n",
      "\n",
      "*\n",
      "\n",
      "Equal contribution 1 Iprova SA, Switzerland 2 Computer and\n",
      "Communication Sciences, EPFL, Switzerland. Correspondence\n",
      "to: Martin Jaggi <martin.jaggi@epfl.ch>.\n",
      "\n",
      "1\n",
      "All our code and pre-trained models are publicly available\n",
      "on http://github.com/epfml/sent2vec.\n",
      "\n",
      "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features\n",
      "\n",
      "tence embeddings. This strongly contrasts all neural\n",
      "network based approaches, and allows our model to\n",
      "learn from extremely large datasets, which is a crucial\n",
      "advantage in the unsupervised setting.\n",
      " Performance. Our method shows significant performance improvements compared to the current stateof-the-art unsupervised and even semi-supervised\n",
      "models. The resulting general-purpose embeddings\n",
      "show strong robustness when transferred to a wide\n",
      "range of prediction benchmarks.\n",
      "\n",
      "Formally, we learn source v w and target uw embeddings\n",
      "for each word w in the vocabulary, with embedding dimension h and k = |V| as in (1). The sentence embedding\n",
      "is defined as the average of the source word embeddings of\n",
      "its constituent words, as in (2). We augment this model furthermore by also learning source embeddings for not only\n",
      "unigrams but also n-grams present in each sentence, and\n",
      "averaging the n-gram embeddings along with the words,\n",
      "i.e., the sentence embedding v S for S is modeled as\n",
      "v S :=\n",
      "\n",
      "1\n",
      "|R(S)| V R(S)\n",
      "\n",
      "=\n",
      "\n",
      "1\n",
      "|R(S)|\n",
      "\n",
      "X\n",
      "\n",
      "vw\n",
      "\n",
      "(2)\n",
      "\n",
      "wR(S)\n",
      "\n",
      "2. Model\n",
      "Our model is inspired by simple matrix factor models (bilinear models) such as recently very successfully used in\n",
      "unsupervised learning of word embeddings (Mikolov et al.,\n",
      "2013b;a; Pennington et al., 2014; Bojanowski et al., 2017)\n",
      "as well as supervised of sentence classification (Joulin\n",
      "et al., 2017). More precisely, these models are formalized\n",
      "as an optimization problem of the form\n",
      "min\n",
      "\n",
      "U ,V\n",
      "\n",
      "X\n",
      "\n",
      "fS (U V S )\n",
      "\n",
      "(1)\n",
      "\n",
      "where R(S) is the list of n-grams (including unigrams)\n",
      "present in sentence S. In order to predict a missing word\n",
      "from the context, our objective models the softmax output\n",
      "approximated by negative sampling following (Mikolov\n",
      "et al., 2013b). For the large number of output classes\n",
      "|V| to be predicted, negative sampling is known to significantly improve training efficiency, see also (Goldberg\n",
      "& Levy, 2014). Given the binary logistic loss function\n",
      "` : x 7 log (1 + ex ) coupled with negative sampling, our\n",
      "unsupervised training objective is formulated as follows:\n",
      "\n",
      "SC\n",
      "\n",
      "for two parameter matrices U  Rkh and V  Rh|V| ,\n",
      "where V denotes the vocabulary. In all models studied,\n",
      "the columns of the matrix V will collect the learned word\n",
      "vectors, having h dimensions. For a given sentence S,\n",
      "which can be of arbitrary length, the indicator vector S \n",
      "{0, 1}|V| is a binary vector encoding S (bag of words encoding).\n",
      "Fixed-length context windows S running over the corpus are used in word embedding methods as in C-BOW\n",
      "(Mikolov et al., 2013b;a) and GloVe (Pennington et al.,\n",
      "2014). Here we have k = |V| and each cost function\n",
      "fS : Rk  R only depends on a single row of its input, describing the observed target word for the given fixed-length\n",
      "context S. In contrast, for sentence embeddings which\n",
      "are the focus of our paper here, S will be entire sentences\n",
      "or documents (therefore variable length). This property is\n",
      "shared with the supervised FastText classifier (Joulin et al.,\n",
      "2017), which however uses soft-max with k  |V| being\n",
      "the number of class labels.\n",
      "2.1. Proposed Unsupervised Model\n",
      "We propose a new unsupervised model, Sent2Vec, for\n",
      "learning universal sentence embeddings. Conceptually, the\n",
      "model can be interpreted as a natural extension of the wordcontexts from C-BOW (Mikolov et al., 2013b;a) to a larger\n",
      "sentence context, with the sentence words being specifically optimized towards additive combination over the sentence, by means of the unsupervised objective function.\n",
      "\n",
      "min\n",
      "\n",
      "U ,V\n",
      "\n",
      "\n",
      "X X \n",
      "\n",
      " X\n",
      ">\n",
      "` u>\n",
      "v\n",
      "+\n",
      "`\n",
      "\n",
      "u\n",
      "v\n",
      "0\n",
      "wt S\\\\{wt }\n",
      "w S\\\\{wt }\n",
      "w0 Nwt\n",
      "\n",
      "SC wt S\n",
      "\n",
      "where S corresponds to the current sentence and Nwt is\n",
      "the set of words sampled negatively for the word wt  S.\n",
      "The negatives are sampled2 following a multinomial distribution where\n",
      "with a probability\n",
      " word\n",
      "\n",
      " each\n",
      "P w ispassociated\n",
      "fwi , where fw is the norqn (w) := fw\n",
      "wi V\n",
      "malized frequency of w in the corpus.\n",
      "To select the possible target unigrams (positives), we use\n",
      "subsampling as in (Joulin et al., 2017; Bojanowski et al.,\n",
      "2017), each word w being discarded\n",
      "1\n",
      " p with probability\n",
      "\\t\n",
      "qp (w) where qp (w) := min 1, t/fw + t/fw . Where t\n",
      "is the subsampling hyper-parameter. Subsampling prevents\n",
      "very frequent words of having too much influence in the\n",
      "learning as they would introduce strong biases in the prediction task. With positives subsampling and respecting the\n",
      "negative sampling distribution, the precise training objective function becomes\n",
      "X X\n",
      "\n",
      "min\n",
      "qp (wt )` u>\n",
      "(3)\n",
      "wt v S\\\\{wt }\n",
      "U ,V\n",
      "\n",
      "SC wt S\n",
      "\n",
      "+ |Nwt |\n",
      "\n",
      "X\n",
      "\n",
      "0\n",
      "\n",
      "qn (w )` \n",
      "\n",
      "u>\n",
      "w0 v S\\\\{wt }\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "w0 V\n",
      "2\n",
      "\n",
      "To efficiently sample negatives, a pre-processing table is constructed, containing the words corresponding to the square root of\n",
      "their corpora frequency. Then, the negatives Nwt are sampled\n",
      "uniformly at random from the negatives table except the target wt\n",
      "itself, following (Joulin et al., 2017; Bojanowski et al., 2017).\n",
      "\n",
      "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features\n",
      "\n",
      "2.2. Computational Efficiency\n",
      "In contrast to more complex neural network based models, one of the core advantages of the proposed technique\n",
      "is the low computational cost for both inference and training. Given a sentence S and a trained model, computing\n",
      "the sentence representation v S only requires |S|  h floating\n",
      "point operations (or |R(S)|  h to be precise for the n-gram\n",
      "case, see (2)), where h is the embedding dimension. The\n",
      "same holds for the cost of training with SGD on the objective (3), per sentence seen in the training corpus. Due to the\n",
      "simplicity of the model, parallel training is straight-forward\n",
      "using parallelized or distributed SGD.\n",
      "2.3. Comparison to C-BOW\n",
      "C-BOW (Mikolov et al., 2013b;a) tries to predict a chosen\n",
      "target word given its fixed-size context window, the context\n",
      "being defined by the average of the vectors associated with\n",
      "the words at a distance less than the window size hyperparameter ws. If our system, when restricted to unigram\n",
      "features, can be seen as an extension of C-BOW where\n",
      "the context window includes the entire sentence, in practice there are few important differences as C-BOW uses\n",
      "important tricks to facilitate the learning of word embeddings. C-BOW first uses frequent word subsampling on the\n",
      "sentences, deciding to discard each token w with probability qp (w) or alike (small variations exist across implementations). Subsampling prevents the generation of n-grams\n",
      "features, and deprives the sentence of an important part of\n",
      "its syntactical features. It also shortens the distance between subsampled words, implicitly increasing the span of\n",
      "the context window. A second trick consists of using dynamic context windows: for each subsampled word w, the\n",
      "size of its associated context window is sampled uniformly\n",
      "between 1 and ws. Using dynamic context windows is\n",
      "equivalent to weighing by the distance from the focus word\n",
      "w divided by the window size (Levy et al., 2015). This\n",
      "makes the prediction task local, and go against our objective of creating sentence embeddings as we want to learn\n",
      "how to compose all n-gram features present in a sentence.\n",
      "In the results section, we report a significant improvement\n",
      "of our method over C-BOW.\n",
      "2.4. Model Training\n",
      "Three different datasets have been used to train our models:\n",
      "the Toronto book corpus3 , Wikipedia sentences and tweets.\n",
      "The Wikipedia and Toronto books sentences have been tokenized using the Stanford NLP library (Manning et al.,\n",
      "2014), while for tweets we used the NLTK tweets tokenizer\n",
      "(Bird et al., 2009). For training, we select a sentence randomly from the dataset and then proceed to select all the\n",
      "3\n",
      "\n",
      "http://www.cs.toronto.edu/mbweb/\n",
      "\n",
      "possible target unigrams using subsampling. We update the\n",
      "weights using SGD with a linearly decaying learning rate.\n",
      "Also, to prevent overfitting, for each sentence we use\n",
      "dropout on its list of n-grams R(S) \\\\ {U (S)}, where U (S)\n",
      "is the set of all unigrams contained in sentence S. After\n",
      "empirically trying multiple dropout schemes, we find that\n",
      "dropping K n-grams (n > 1) for each sentence is giving superior results compared to dropping each token with some\n",
      "fixed probability. This dropout mechanism would negatively impact shorter sentences. The regularization can be\n",
      "pushed further by applying L1 regularization to the word\n",
      "vectors. Encouraging sparsity in the embedding vectors is\n",
      "particularly beneficial for high dimension h. The additional\n",
      "soft thresholding in every SGD step adds negligible computational cost. See also Appendix B.\n",
      "We train two models on each dataset, one with unigrams\n",
      "only and one with unigrams and bigrams. All training\n",
      "parameters for the models are provided in Table 5 in the\n",
      "supplementary material. Our C++ implementation builds\n",
      "upon the FastText library (Joulin et al., 2017; Bojanowski\n",
      "et al., 2017). We will make our code and pre-trained models available open-source.\n",
      "\n",
      "3. Related Work\n",
      "We discuss existing models which have been proposed to\n",
      "construct sentence embeddings. While there is a large body\n",
      "of works in this direction  several among these using e.g.\n",
      "labelled datasets of paraphrase pairs to obtain sentence embeddings in a supervised manner (Wieting et al., 2016b;a)\n",
      " we here focus on unsupervised, task-independent models. While some methods require ordered raw text i.e., a\n",
      "coherent corpus where the next sentence is a logical continuation of the previous sentence, others rely only on raw\n",
      "text i.e., an unordered collection of sentences. Finally we\n",
      "also discuss alternative models built from structured data\n",
      "sources.\n",
      "3.1. Unsupervised Models Independent of Sentence\n",
      "Ordering\n",
      "The ParagraphVector DBOW model (Le & Mikolov,\n",
      "2014) is a log-linear model which is trained to learn sentence as well as word embeddings and then use a softmax distribution to predict words contained in the sentence\n",
      "given the sentence vector representation. They also propose a different model ParagraphVector DM where they\n",
      "use n-grams of consecutive words along with the sentence\n",
      "vector representation to predict the next word.\n",
      "(Hill et al., 2016a) propose a Sequential (Denoising) Autoencoder, S(D)AE. This model first introduces noise in\n",
      "the input data: Firstly each word is deleted with probability p0 , then for each non-overlapping bigram, words\n",
      "\n",
      "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features\n",
      "\n",
      "are swapped with probability px . The model then uses an\n",
      "LSTM-based architecture to retrieve the original sentence\n",
      "from the corrupted version. The model can then be used\n",
      "to encode new sentences into vector representations. In the\n",
      "case of p0 = px = 0, the model simply becomes a Sequential Autoencoder. (Hill et al., 2016a) also propose a variant\n",
      "(S(D)AE + embs.) in which the words are represented by\n",
      "fixed pre-trained word vector embeddings.\n",
      "(Arora et al., 2017) propose a model in which sentences\n",
      "are represented as a weighted average of fixed (pre-trained)\n",
      "word vectors, followed by post-processing step of subtracting the principal component. Using the generative model\n",
      "of (Arora et al., 2016), words are generated conditioned on\n",
      "a sentence discourse vector cs :\n",
      "P r[w | cs ] = fw + (1  )\n",
      "\n",
      "exp(c>\n",
      "s vw )\n",
      ",\n",
      "Zcs\n",
      "\n",
      ">\n",
      "wV exp(cs v w )\n",
      "\n",
      "P\n",
      "\n",
      "where Zcs :=\n",
      "and cs := c0 + (1 \n",
      ")cs and ,  are scalars. c0 is the common discourse vector, representing a shared component among all discourses,\n",
      "mainly related to syntax. It allows the model to better generate syntactical features. The fw term is here to enable\n",
      "the model to generate some frequent words even if their\n",
      "matching with the discourse vector cs is low.\n",
      "Therefore, this model tries to generate sentences as a mixture of three type of words: words matching the sentence\n",
      "discourse vector cs , syntactical words matching c0 , and\n",
      "words with high fw . (Arora et al., 2017) demonstrated\n",
      "thatPfor this model, the MLE of cs can be approximated\n",
      "by wS fwa+a v w , where a is a scalar. The sentence discourse vector can hence be obtained by subtracting c0 estimated by the first principal component of cs s on a set\n",
      "of sentences. In other words, the sentence embeddings are\n",
      "obtained by a weighted average of the word vectors stripping away the syntax by subtracting the common discourse\n",
      "vector and down-weighting frequent tokens. They generate sentence embeddings from diverse pre-trained word\n",
      "embeddings among which are unsupervised word embeddings such as GloVe (Pennington et al., 2014) as well as supervised word embeddings such as paragram-SL999 (PSL)\n",
      "(Wieting et al., 2015) trained on the Paraphrase Database\n",
      "(Ganitkevitch et al., 2013).\n",
      "In a very different line of work, C-PHRASE (Pham et al.,\n",
      "2015) relies on additional information from the syntactic\n",
      "parse tree of each sentence, which is incorporated into the\n",
      "C-BOW training objective.\n",
      "(Huang & Anandkumar, 2016) show that single layer\n",
      "CNNs can be modeled using a tensor decomposition approach. While building on an unsupervised objective, the\n",
      "employed dictionary learning step for obtaining phrase\n",
      "templates is task-specific (for each use-case), not resulting\n",
      "in general-purpose embeddings.\n",
      "\n",
      "3.2. Unsupervised Models Depending on Sentence\n",
      "Ordering\n",
      "The SkipThought model (Kiros et al., 2015) combines\n",
      "sentence level models with recurrent neural networks.\n",
      "Given a sentence Si from an ordered corpus, the model is\n",
      "trained to predict Si1 and Si+1 .\n",
      "FastSent (Hill et al., 2016a) is a sentence-level log-linear\n",
      "bag-of-words model. Like SkipThought, it uses adjacent\n",
      "sentences as the prediction target and is trained in an unsupervised fashion. Using word sequences allows the model\n",
      "to improve over the earlier work of paragraph2vec (Le &\n",
      "Mikolov, 2014). (Hill et al., 2016a) augment FastSent further by training it to predict the constituent words of the\n",
      "sentence as well. This model is named FastSent + AE in\n",
      "our comparisons.\n",
      "Compared to our approach, Siamese C-BOW (Kenter\n",
      "et al., 2016) shares the idea of learning to average word embeddings over a sentence. However, it relies on a Siamese\n",
      "neural network architecture to predict surrounding sentences, contrasting our simpler unsupervised objective.\n",
      "Note that on the character sequence level instead of word\n",
      "sequences, FastText (Bojanowski et al., 2017) uses the\n",
      "same conceptual model to obtain better word embeddings.\n",
      "This is most similar to our proposed model, with two\n",
      "key differences: Firstly, we predict from source word sequences to target words, as opposed to character sequences\n",
      "to target words, and secondly, our model is averaging the\n",
      "source embeddings instead of summing them.\n",
      "3.3. Models requiring structured data\n",
      "DictRep (Hill et al., 2016b) is trained to map dictionary\n",
      "definitions of the words to the pre-trained word embeddings of these words. They use two different architectures,\n",
      "namely BOW and RNN (LSTM) with the choice of learning the input word embeddings or using them pre-trained.\n",
      "A similar architecture is used by the CaptionRep variant,\n",
      "but here the task is the mapping of given image captions to\n",
      "a pre-trained vector representation of these images.\n",
      "\n",
      "4. Evaluation Tasks\n",
      "We use a standard set of supervised as well as unsupervised benchmark tasks from the literature to evaluate our\n",
      "trained models, following (Hill et al., 2016a). The breadth\n",
      "of tasks allows to fairly measure generalization to a wide\n",
      "area of different domains, testing the general-purpose quality (universality) of all competing sentence embeddings.\n",
      "For downstream supervised evaluations, sentence embeddings are combined with logistic regression to predict target labels. In the unsupervised evaluation for sentence similarity, correlation of the cosine similarity between two em-\n",
      "\n",
      "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features\n",
      "\n",
      "beddings is compared to human annotators.\n",
      "Downstream Supervised Evaluation. Sentence embeddings are evaluated for various supervised classification\n",
      "tasks as follows. We evaluate paraphrase identification\n",
      "(MSRP) (Dolan et al., 2004), classification of movie review\n",
      "sentiment (MR) (Pang & Lee, 2005), product reviews (CR)\n",
      "(Hu & Liu, 2004), subjectivity classification (SUBJ)(Pang\n",
      "& Lee, 2004), opinion polarity (MPQA) (Wiebe et al.,\n",
      "2005) and question type classification (TREC) (Voorhees,\n",
      "2002). To classify, we use the code provided by (Kiros\n",
      "et al., 2015) in the same manner as in (Hill et al.,\n",
      "2016a). For the MSRP dataset, containing pairs of sentences (S1 , S2 ) with associated paraphrase label, we generate feature vectors by concatenating their Sent2Vec representations |v S1  v S2 | with the component-wise product v S1  v S2 . The predefined training split is used to\n",
      "tune the L2 penalty parameter using cross-validation and\n",
      "the accuracy and F1 scores are computed on the test set.\n",
      "For the remaining 5 datasets, Sent2Vec embeddings are inferred from input sentences and directly fed to a logistic\n",
      "regression classifier. Accuracy scores are obtained using\n",
      "10-fold cross-validation for the MR, CR, SUBJ and MPQA\n",
      "datasets. For those datasets nested cross-validation is used\n",
      "to tune the L2 penalty. For the TREC dataset, as for the\n",
      "MRSP dataset, the L2 penalty is tuned on the predefined\n",
      "train split using 10-fold cross-validation, and the accuracy\n",
      "is computed on the test set.\n",
      "Unsupervised Similarity Evaluation. We perform unsupervised evaluation of the the learnt sentence embeddings using the sentence cosine similarity, on the STS\n",
      "2014 (Agirre et al., 2014) and SICK 2014 (Marelli et al.,\n",
      "2014) datasets. These similarity scores are compared to the\n",
      "gold-standard human judgements using Pearsons r (Pearson, 1895) and Spearmans  (Spearman, 1904) correlation\n",
      "scores. The SICK dataset consists of about 10,000 sentence\n",
      "pairs along with relatedness scores of the pairs. The STS\n",
      "2014 dataset contains 3,770 pairs, divided into six different categories on the basis of origin of sentences/phrases\n",
      "namely Twitter, headlines, news, forum, WordNet and images. See (Agirre et al., 2014) for more precise information\n",
      "on how the pairs have been created.\n",
      "\n",
      "5. Results and Discussion\n",
      "In Tables 1 and 2, we compare our results with those obtained by (Hill et al., 2016a) on different models. Along\n",
      "with the models discussed in Section 3, this also includes\n",
      "the sentence embedding baselines obtained by simple averaging of word embeddings over the sentence, in both the\n",
      "C-BOW and skip-gram variants. TF-IDF BOW is a representation consisting of the counts of the 200,000 most common feature-words, weighed by their TF-IDF frequencies.\n",
      "To ensure coherence, we only include unsupervised mod-\n",
      "\n",
      "els in the main paper. Performance of supervised and semisupervised models on these evaluations can be observed in\n",
      "Tables 6 and 7 in the supplementary material.\n",
      "Downstream Supervised Evaluation Results. On running supervised evaluations and observing the results in\n",
      "Table 1, we find that on an average our models are second only to SkipThought vectors. Also, both our models\n",
      "achieve state of the art results on the CR task. We also observe that on half of the supervised tasks, our unigrams +\n",
      "bigram model is the the best model after SkipThought. Our\n",
      "models are weaker on the MSRP task (which consists of the\n",
      "identification of labelled paraphrases) compared to stateof-the-art methods. However, we observe that the models\n",
      "which perform extremely well on this task end up faring\n",
      "very poorly on the other tasks, indicating a lack of generalizability.\n",
      "On rest of the tasks, our models perform extremely well.\n",
      "The SkipThought model is able to outperform our models\n",
      "on most of the tasks as it is trained to predict the previous\n",
      "and next sentences and a lot of tasks are able to make use of\n",
      "this contextual information missing in our Sent2Vec models. For example, the TREC task is a poor measure of how\n",
      "one predicts the content of the sentence (the question) but\n",
      "a good measure of how the next sentence in the sequence\n",
      "(the answer) is predicted.\n",
      "Unsupervised Similarity Evaluation Results. In Table 2,\n",
      "we see that our Sent2Vec models are state-of-the-art on the\n",
      "majority of tasks when comparing to all the unsupervised\n",
      "models trained on the Toronto corpus, and clearly achieve\n",
      "the best averaged performance. Our Sent2Vec models also\n",
      "on average outperform or are at par with the C-PHRASE\n",
      "model, despite significantly lagging behind on the STS\n",
      "2014 WordNet and News subtasks. This observation can\n",
      "be attributed to the fact that a big chunk of the data that\n",
      "the C-PHRASE model is trained on comes from English\n",
      "Wikipedia, helping it to perform well on datasets involving definition and news items. Also, C-PHRASE uses data\n",
      "three times the size of the Toronto book corpus. Interestingly, our model outperforms C-PHRASE when trained on\n",
      "Wikipedia, as shown in Table 3, despite the fact that we use\n",
      "no parse tree information.\n",
      "In the official results of the more recent edition of the STS\n",
      "2017 benchmark (Cer et al., 2017), our model also significantly outperforms C-PHRASE, and delivers the best unsupervised baseline method.\n",
      "Macro Average. To summarize our contributions on both\n",
      "supervised and unsupervised tasks, in Table 3 we present\n",
      "the results in terms of the macro average over the averages\n",
      "4\n",
      "For the Siamese C-BOW model trained on the Toronto corpus, supervised evaluation as well as similarity evaluation results\n",
      "on the SICK 2014 dataset are unavailable.\n",
      "\n",
      "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features\n",
      "Data\n",
      "\n",
      "Unordered Sentences:\n",
      "(Toronto Books;\n",
      "70 million sentences,\n",
      "0.9 Billion Words)\n",
      "\n",
      "Ordered Sentences:\n",
      "Toronto Books\n",
      "2.8 Billion words\n",
      "\n",
      "Model\n",
      "SAE\n",
      "SAE + embs.\n",
      "SDAE\n",
      "SDAE + embs.\n",
      "ParagraphVec DBOW\n",
      "ParagraphVec DM\n",
      "Skipgram\n",
      "C-BOW\n",
      "Unigram TFIDF\n",
      "Sent2Vec uni.\n",
      "Sent2Vec uni. + bi.\n",
      "SkipThought\n",
      "FastSent\n",
      "FastSent+AE\n",
      "C-PHRASE\n",
      "\n",
      "MSRP (Acc / F1)\n",
      "74.3 / 81.7\n",
      "70.6 / 77.9\n",
      "76.4 / 83.4\n",
      "73.7 / 80.7\n",
      "72.9 / 81.1\n",
      "73.6 / 81.9\n",
      "69.3 / 77.2\n",
      "67.6 / 76.1\n",
      "73.6 / 81.7\n",
      "72.2 / 80.3\n",
      "72.5 / 80.8\n",
      "73.0 / 82.0\n",
      "72.2 / 80.3\n",
      "71.2 / 79.1\n",
      "72.2 / 79.6\n",
      "\n",
      "MR\n",
      "62.6\n",
      "73.2\n",
      "67.6\n",
      "74.6\n",
      "60.2\n",
      "61.5\n",
      "73.6\n",
      "73.6\n",
      "73.7\n",
      "75.1\n",
      "75.8\n",
      "76.5\n",
      "70.8\n",
      "71.8\n",
      "75.7\n",
      "\n",
      "CR\n",
      "68.0\n",
      "75.3\n",
      "74.0\n",
      "78.0\n",
      "66.9\n",
      "68.6\n",
      "77.3\n",
      "77.3\n",
      "79.2\n",
      "80.2\n",
      "80.3\n",
      "80.1\n",
      "78.4\n",
      "76.7\n",
      "78.8\n",
      "\n",
      "SUBJ\n",
      "86.1\n",
      "89.8\n",
      "89.3\n",
      "90.8\n",
      "76.3\n",
      "76.4\n",
      "89.2\n",
      "89.1\n",
      "90.3\n",
      "90.6\n",
      "91.2\n",
      "93.6\n",
      "88.7\n",
      "88.8\n",
      "91.1\n",
      "\n",
      "MPQA\n",
      "76.8\n",
      "86.2\n",
      "81.3\n",
      "86.9\n",
      "70.7\n",
      "78.1\n",
      "85.0\n",
      "85.0\n",
      "82.4\n",
      "86.3\n",
      "85.9\n",
      "87.1\n",
      "80.6\n",
      "81.5\n",
      "86.2\n",
      "\n",
      "TREC\n",
      "80.2\n",
      "80.4\n",
      "77.7\n",
      "78.4\n",
      "59.4\n",
      "55.8\n",
      "82.2\n",
      "82.2\n",
      "85.0\n",
      "83.8\n",
      "86.4\n",
      "92.2\n",
      "76.8\n",
      "80.4\n",
      "78.8\n",
      "\n",
      "Average\n",
      "74.7\n",
      "79.3\n",
      "78.3\n",
      "80.4\n",
      "67.7\n",
      "69.0\n",
      "78.5\n",
      "79.1\n",
      "80.7\n",
      "81.4\n",
      "82.0\n",
      "83.8\n",
      "77.9\n",
      "78.4\n",
      "80.5\n",
      "\n",
      "Table 1: Comparison of the performance of different models on different supervised evaluation tasks. An underline\n",
      "indicates the best performance for the dataset. Top 3 performances in each data category are shown in bold. The average\n",
      "is calculated as the average of accuracy for each category (For MSRP, we take the average of two entries.)\n",
      "Model\n",
      "SAE\n",
      "SAE + embs.\n",
      "SDAE\n",
      "SDAE + embs.\n",
      "ParagraphVec DBOW\n",
      "ParagraphVec DM\n",
      "Skipgram\n",
      "C-BOW\n",
      "Unigram TF-IDF\n",
      "Sent2Vec uni.\n",
      "Sent2Vec uni. + bi.\n",
      "SkipThought\n",
      "FastSent\n",
      "FastSent+AE\n",
      "Siamese C-BOW4\n",
      "C-PHRASE\n",
      "\n",
      "News\n",
      ".17/.16\n",
      ".52/.54\n",
      ".07/.04\n",
      ".51/.54\n",
      ".31/.34\n",
      ".42/.46\n",
      ".56/.59\n",
      ".57/.61\n",
      ".48/.48\n",
      ".62/.67\n",
      ".62/.67\n",
      ".44/.45\n",
      ".58/.59\n",
      ".56/.59\n",
      ".58/.59\n",
      ".69/.71\n",
      "\n",
      "Forum\n",
      ".12/.12\n",
      ".22/.23\n",
      ".11/.13\n",
      ".29/.29\n",
      ".32/.32\n",
      ".33/.34\n",
      ".42/.42\n",
      ".43/.44\n",
      ".40/.38\n",
      ".49/.49\n",
      ".51/.51\n",
      ".14/.15\n",
      ".41/.36\n",
      ".41/.40\n",
      ".42/.41\n",
      ".43/.41\n",
      "\n",
      "STS 2014\n",
      "WordNet\n",
      "Twitter\n",
      ".30/.23\n",
      ".28/.22\n",
      ".60/.55\n",
      ".60/.60\n",
      ".33/.24\n",
      ".44/.42\n",
      ".56/.50\n",
      ".57/.58\n",
      ".53/.50\n",
      ".43/.46\n",
      ".51/.48\n",
      ".54/.57\n",
      ".73/.70\n",
      ".71/.74\n",
      ".72/.69\n",
      ".71/.75\n",
      ".60/.59\n",
      ".63/.65\n",
      ".75/.72\n",
      ".70/.75\n",
      ".71/.68\n",
      ".70/.75\n",
      ".39/.34\n",
      ".42/.43\n",
      ".74/.70\n",
      ".63/.66\n",
      ".69/.64\n",
      ".70/.74\n",
      ".66/.61\n",
      ".71/.73\n",
      ".76/.73\n",
      ".60/.65\n",
      "\n",
      "Images\n",
      ".49/.46\n",
      ".64/.64\n",
      ".44/.38\n",
      ".59/.59\n",
      ".46/.44\n",
      ".32/.30\n",
      ".65/.67\n",
      ".71/.73\n",
      ".72/.74\n",
      ".78/.82\n",
      ".75/.79\n",
      ".55/.60\n",
      ".74/.78\n",
      ".63/.65\n",
      ".65/.65\n",
      ".75/.79\n",
      "\n",
      "Headlines\n",
      ".13/.11\n",
      ".41/.41\n",
      ".36/.36\n",
      ".43/.44\n",
      ".39/.41\n",
      ".46/.47\n",
      ".55/.58\n",
      ".55/.59\n",
      ".49/.49\n",
      ".61/.63\n",
      ".59/.62\n",
      ".43/.44\n",
      ".57/.59\n",
      ".58/.60\n",
      ".63/.64\n",
      ".60/.65\n",
      "\n",
      "SICK 2014\n",
      "Test + Train\n",
      ".32/.31\n",
      ".47/.49\n",
      ".46/.46\n",
      ".46/.46\n",
      ".42/.46\n",
      ".44/.40\n",
      ".60/.69\n",
      ".60/.69\n",
      ".52/.58\n",
      ".61/.70\n",
      ".62/.70\n",
      ".57/.60\n",
      ".61/.72\n",
      ".60/.65\n",
      "\n",
      ".60/.72\n",
      "\n",
      "Average\n",
      ".26/.23\n",
      ".50/.49\n",
      ".31/.29\n",
      ".49/.49\n",
      ".41/.42\n",
      ".43/.43\n",
      ".60/.63\n",
      ".60/.65\n",
      ".55/.56\n",
      ".65/.68\n",
      ".65/.67\n",
      ".42/.43\n",
      ".61/.63\n",
      ".60/.61\n",
      "\n",
      ".63/.67\n",
      "\n",
      "Table 2: Unsupervised Evaluation Tasks: Comparison of the performance of different models on Spearman/Pearson\n",
      "correlation measures. An underline indicates the best performance for the dataset. Top 3 performances in each data\n",
      "category are shown in bold. The average is calculated as the average of entries for each correlation measure.\n",
      "of both supervised and unsupervised tasks along with the\n",
      "training times of the models5 . For unsupervised tasks, averages are taken over both Spearman and Pearson scores.\n",
      "The comparison includes the best performing unsupervised\n",
      "and semi-supervised methods described in Section 3. For\n",
      "models trained on the Toronto books dataset, we report a\n",
      "3.8 % points improvement over the state of the art. Considering all supervised, semi-supervised methods and all\n",
      "datasets compared in (Hill et al., 2016a), we report a 2.2 %\n",
      "points improvement.\n",
      "We also see a noticeable improvement in accuracy as we\n",
      "use larger datasets like twitter and Wikipedia dump. We\n",
      "can also see that the Sent2Vec models are also faster to train\n",
      "when compared to methods like SkipThought and DictRep\n",
      "owing to the SGD step allowing a high degree of parallelizability.\n",
      "We can clearly see Sent2Vec outperforming other unsupervised and even semi-supervised methods. This can be at5\n",
      "\n",
      "time taken to train C-PHRASE models is unavailable\n",
      "\n",
      "tributed to the superior generalizability of our model across\n",
      "supervised and unsupervised tasks.\n",
      "Comparison with Arora et al. (2017). In Table 4, we\n",
      "report an experimental comparison to the model of Arora\n",
      "et al. (2017), which is particularly tailored to sentence similarity tasks. In the table, the suffix W indicates that their\n",
      "down-weighting scheme has been used, while the suffix\n",
      "R indicates the removal of the first principal component.\n",
      "They report values of a  [104 , 103 ] as giving the best\n",
      "results and used a = 103 for all their experiments. Their\n",
      "down-weighting scheme hints us to reduce the importance\n",
      "of syntactical features. To do so, we use a simple blacklist containing the 25 most frequent tokens in the Twitter\n",
      "corpus and discard them before averaging. Results are also\n",
      "reported in Table 4.\n",
      "We observe that our results are competitive with the embeddings of Arora et al. (2017) for purely unsupervised methods. We confirm their empirical finding that reducing the\n",
      "influence of the syntax helps performance on semantic sim-\n",
      "\n",
      "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features\n",
      "Type\n",
      "\n",
      "Training corpus\n",
      "\n",
      "Method\n",
      "\n",
      "unsupervised\n",
      "unsupervised\n",
      "unsupervised\n",
      "unsupervised\n",
      "unsupervised\n",
      "unsupervised\n",
      "semi-supervised\n",
      "unsupervised\n",
      "unsupervised\n",
      "unsupervised\n",
      "unsupervised\n",
      "\n",
      "twitter (19.7B words)\n",
      "twitter (19.7B words)\n",
      "Wikipedia (1.7B words)\n",
      "Wikipedia (1.7B words)\n",
      "Toronto books (0.9B words)\n",
      "Toronto books (0.9B words)\n",
      "structured dictionary dataset\n",
      "2.8B words + parse info.\n",
      "Toronto books (0.9B words)\n",
      "Toronto books (0.9B words)\n",
      "Toronto books (0.9B words)\n",
      "\n",
      "Sent2Vec uni. + bi.\n",
      "Sent2Vec uni.\n",
      "Sent2Vec uni. + bi.\n",
      "Sent2Vec uni.\n",
      "Sent2Vec books uni.\n",
      "Sent2Vec books uni. + bi.\n",
      "DictRep BOW + emb\n",
      "C-PHRASE\n",
      "C-BOW\n",
      "FastSent\n",
      "SkipThought\n",
      "\n",
      "Supervised\n",
      "average\n",
      "83.5\n",
      "82.2\n",
      "83.3\n",
      "82.4\n",
      "81.4\n",
      "82.0\n",
      "80.5\n",
      "80.5\n",
      "79.1\n",
      "77.9\n",
      "83.8\n",
      "\n",
      "Unsupervised\n",
      "average\n",
      "68.3\n",
      "69.0\n",
      "66.2\n",
      "66.3\n",
      "66.7\n",
      "65.9\n",
      "66.9\n",
      "64.9\n",
      "62.8\n",
      "62.0\n",
      "42.5\n",
      "\n",
      "Macro\n",
      "average\n",
      "75.9\n",
      "75.6\n",
      "74.8\n",
      "74.3\n",
      "74.0\n",
      "74.0\n",
      "73.7\n",
      "72.7\n",
      "70.2\n",
      "70.0\n",
      "63.1\n",
      "\n",
      "Training time\n",
      "(in hours)\n",
      "6.5*\n",
      "3*\n",
      "2*\n",
      "3.5*\n",
      "1*\n",
      "1.2*\n",
      "24**\n",
      "\n",
      "2\n",
      "2\n",
      "336**\n",
      "\n",
      "Table 3: Best unsupervised and semi-supervised methods ranked by macro average along with their training times. ** indicates trained on GPU. * indicates trained on a single node using 30 cores. Training times for non-Sent2Vec models are\n",
      "due to (Hill et al., 2016a)\n",
      "Dataset\n",
      "STS 2014\n",
      "SICK 2014\n",
      "\n",
      "Unsupervised\n",
      "GloVe + W\n",
      "0.594\n",
      "0.705\n",
      "\n",
      "Unsupervised\n",
      "GloVe + WR\n",
      "0.685\n",
      "0.722\n",
      "\n",
      "Semi-supervised\n",
      "PSL + WR\n",
      "0.735\n",
      "0.729\n",
      "\n",
      "Sent2Vec Unigrams\n",
      "Tweets Model\n",
      "0.710\n",
      "0.710\n",
      "\n",
      "Sent2Vec Unigrams\n",
      "Tweets Model With Blacklist\n",
      "0.718\n",
      "0.719\n",
      "\n",
      "Table 4: Comparison of the performance of the unsupervised and semi-supervised sentence embeddings by (Arora et al.,\n",
      "2017) with our models, in terms of Pearsons correlation.\n",
      "to unsupervised evaluations but gives a significant boost-up\n",
      "in accuracy on supervised tasks.\n",
      "\n",
      "Figure 1: Left figure: the profile of the word vector L2 norms as a function of log(fw ) for each vocabulary word\n",
      "w, as learnt by our unigram model trained on Toronto\n",
      "books. Right figure: down-weighting scheme proposed by\n",
      "a\n",
      ".\n",
      "Arora et al. (2017): weight(w) = a+f\n",
      "w\n",
      "\n",
      "ilarity tasks, and we show that applying a simple blacklist\n",
      "already yields a noticeable amelioration. It is important to\n",
      "note that the scores obtained from supervised task-specific\n",
      "PSL embeddings trained for the purpose of semantic similarity outperform our method on both SICK and average\n",
      "STS 2014, which is expected as our model is trained purely\n",
      "unsupervised.\n",
      "The effect of datasets and n-grams. Despite being trained\n",
      "on three very different datasets, all of our models generalize well to sometimes very specific domains. Models\n",
      "trained on Toronto Corpus are the state-of-the art on the\n",
      "STS 2014 images dataset even beating the supervised CaptionRep model trained on images. We also see that addition\n",
      "of bigrams to our models doesnt help much when it comes\n",
      "\n",
      "On learning the importance and the direction of the\n",
      "word vectors. Our model  by learning how to generate\n",
      "and compose word vectors  has to learn both the direction\n",
      "of the word embeddings as well as their norm. Considering the norms of the used word vectors as by our averaging over the sentence, we observe an interesting distribution of the importance of each word. In Figure 1 we\n",
      "show the profile of the L2 -norm as a function of log(fw ) for\n",
      "each w  V, and compare it to the static down-weighting\n",
      "mechanism of Arora et al. (2017). We can observe that our\n",
      "model is learning to down-weight frequent tokens by itself.\n",
      "It is also down-weighting rare tokens and the norm profile\n",
      "seems to roughly follow Luhns hypothesis (Luhn, 1958),\n",
      "a well known information retrieval paradigm, stating that\n",
      "mid-rank terms are the most significant to discriminate content. Modifying the objective function would change the\n",
      "weighting scheme learnt. From a more semantic oriented\n",
      "objective, it should be possible to learn to attribute lower\n",
      "norms for very frequent terms, to more specifically fit sentence similarity tasks.\n",
      "\n",
      "6. Conclusion\n",
      "In this paper, we introduced a novel unsupervised and computationally efficient method to train and infer sentence\n",
      "embeddings. On supervised evaluations, our method, on\n",
      "an average, achieves better performance than all other unsupervised competitors except the SkipThought vectors.\n",
      "However, SkipThought vectors show an extremely poor\n",
      "performance on sentence similarity tasks while our model\n",
      "\n",
      "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features\n",
      "\n",
      "is state-of-the-art for these evaluations on average. Future\n",
      "work could focus on augmenting the model to exploit data\n",
      "with ordered sentences. Furthermore, we would like to further investigate the models ability as giving pre-trained embeddings to enable downstream transfer learning tasks.\n",
      "Acknowledgments. We are indebted to Piotr Bojanowski\n",
      "and Armand Joulin for helpful discussions.\n",
      "\n",
      "References\n",
      "Agirre, Eneko, Banea, Carmen, Cardie, Claire, Cer, Daniel, Diab,\n",
      "Mona, Gonzalez-Agirre, Aitor, Guo, Weiwei, Mihalcea, Rada,\n",
      "Rigau, German, and Wiebe, Janyce. Semeval-2014 task 10:\n",
      "Multilingual semantic textual similarity. In Proceedings of the\n",
      "8th international workshop on semantic evaluation (SemEval\n",
      "2014), pp. 8191. Association for Computational Linguistics\n",
      "Dublin, Ireland, 2014.\n",
      "Arora, Sanjeev, Li, Yuanzhi, Liang, Yingyu, Ma, Tengyu, and\n",
      "Risteski, Andrej. A Latent Variable Model Approach to PMIbased Word Embeddings. In Transactions of the Association\n",
      "for Computational Linguistics, pp. 385399, July 2016.\n",
      "Arora, Sanjeev, Liang, Yingyu, and Ma, Tengyu. A simple but\n",
      "tough-to-beat baseline for sentence embeddings. In International Conference on Learning Representations (ICLR), 2017.\n",
      "Bird, Steven, Klein, Ewan, and Loper, Edward. Natural language\n",
      "processing with Python: analyzing text with the natural language toolkit.  OReilly Media, Inc., 2009.\n",
      "Bojanowski, Piotr, Grave, Edouard, Joulin, Armand, and\n",
      "Mikolov, Tomas. Enriching Word Vectors with Subword Information. Transactions of the Association for Computational\n",
      "Linguistics, 5:135146, 2017.\n",
      "Cer, Daniel, Diab, Mona, Agirre, Eneko, Lopez-Gazpio, Inigo,\n",
      "and Specia, Lucia. SemEval-2017 Task 1: Semantic Textual\n",
      "Similarity Multilingual and Cross-lingual Focused Evaluation.\n",
      "In SemEval-2017 - Proceedings of the 11th International Workshop on Semantic Evaluations, pp. 114, Vancouver, Canada,\n",
      "August 2017. Association for Computational Linguistics.\n",
      "Dolan, Bill, Quirk, Chris, and Brockett, Chris. Unsupervised\n",
      "construction of large paraphrase corpora: Exploiting massively\n",
      "parallel news sources. In Proceedings of the 20th international\n",
      "conference on Computational Linguistics, pp. 350. Association\n",
      "for Computational Linguistics, 2004.\n",
      "Ganitkevitch, Juri, Van Durme, Benjamin, and Callison-Burch,\n",
      "Chris. Ppdb: The paraphrase database. In HLT-NAACL, pp.\n",
      "758764, 2013.\n",
      "Goldberg, Yoav and Levy, Omer. word2vec Explained: deriving\n",
      "Mikolov et al.s negative-sampling word-embedding method.\n",
      "arXiv, February 2014.\n",
      "\n",
      "Hu, Minqing and Liu, Bing. Mining and summarizing customer\n",
      "reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,\n",
      "pp. 168177. ACM, 2004.\n",
      "Huang, Furong and Anandkumar, Animashree. Unsupervised\n",
      "Learning of Word-Sequence Representations from Scratch via\n",
      "Convolutional Tensor Decomposition. arXiv, 2016.\n",
      "Joulin, Armand, Grave, Edouard, Bojanowski, Piotr, and\n",
      "Mikolov, Tomas. Bag of Tricks for Efficient Text Classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Short Papers, pp. 427431, Valencia, Spain, 2017.\n",
      "Kenter, Tom, Borisov, Alexey, and de Rijke, Maarten. Siamese\n",
      "CBOW: Optimizing Word Embeddings for Sentence Representations. In ACL - Proceedings of the 54th Annual Meeting of\n",
      "the Association for Computational Linguistics, pp. 941951,\n",
      "Berlin, Germany, 2016.\n",
      "Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan R, Zemel,\n",
      "Richard, Urtasun, Raquel, Torralba, Antonio, and Fidler, Sanja.\n",
      "Skip-Thought Vectors. In NIPS 2015 - Advances in Neural Information Processing Systems 28, pp. 32943302, 2015.\n",
      "Le, Quoc V and Mikolov, Tomas. Distributed Representations\n",
      "of Sentences and Documents. In ICML 2014 - Proceedings of\n",
      "the 31st International Conference on Machine Learning, volume 14, pp. 11881196, 2014.\n",
      "Levy, Omer, Goldberg, Yoav, and Dagan, Ido. Improving distributional similarity with lessons learned from word embeddings.\n",
      "Transactions of the Association for Computational Linguistics,\n",
      "3:211225, 2015.\n",
      "Luhn, Hans Peter. The automatic creation of literature abstracts. IBM Journal of research and development, 2(2):159\n",
      "165, 1958.\n",
      "Manning, Christopher D, Surdeanu, Mihai, Bauer, John, Finkel,\n",
      "Jenny Rose, Bethard, Steven, and McClosky, David. The stanford corenlp natural language processing toolkit. In ACL (System Demonstrations), pp. 5560, 2014.\n",
      "Marelli, Marco, Menini, Stefano, Baroni, Marco, Bentivogli,\n",
      "Luisa, Bernardi, Raffaella, and Zamparelli, Roberto. A sick\n",
      "cure for the evaluation of compositional distributional semantic models. In LREC, pp. 216223, 2014.\n",
      "Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey.\n",
      "Efficient estimation of word representations in vector space.\n",
      "arXiv preprint arXiv:1301.3781, 2013a.\n",
      "Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and\n",
      "Dean, Jeff. Distributed Representations of Words and Phrases\n",
      "and their Compositionality. In NIPS - Advances in Neural Information Processing Systems 26, pp. 31113119, 2013b.\n",
      "\n",
      "Hill, Felix, Cho, Kyunghyun, and Korhonen, Anna. Learning Distributed Representations of Sentences from Unlabelled Data. In\n",
      "Proceedings of NAACL-HLT, February 2016a.\n",
      "\n",
      "Pang, Bo and Lee, Lillian. A sentimental education: Sentiment\n",
      "analysis using subjectivity summarization based on minimum\n",
      "cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, pp. 271. Association for\n",
      "Computational Linguistics, 2004.\n",
      "\n",
      "Hill, Felix, Cho, KyungHyun, Korhonen, Anna, and Bengio, Yoshua.\n",
      "Learning to understand phrases by embedding the dictionary. TACL, 4:1730, 2016b. URL\n",
      "https://tacl2013.cs.columbia.edu/ojs/\n",
      "index.php/tacl/article/view/711.\n",
      "\n",
      "Pang, Bo and Lee, Lillian. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.\n",
      "In Proceedings of the 43rd annual meeting on association for\n",
      "computational linguistics, pp. 115124. Association for Computational Linguistics, 2005.\n",
      "\n",
      "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features\n",
      "Pearson, Karl. Note on regression and inheritance in the case of\n",
      "two parents. Proceedings of the Royal Society of London, 58:\n",
      "240242, 1895.\n",
      "Pennington, Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: Global vectors for word representation. In\n",
      "EMNLP, volume 14, pp. 15321543, 2014.\n",
      "Pham, NT, Kruszewski, G, Lazaridou, A, and Baroni, M. Jointly\n",
      "optimizing word representations for lexical and sentential tasks\n",
      "with the c-phrase model. ACL/IJCNLP, 2015.\n",
      "Rockafellar, R Tyrrell. Monotone operators and the proximal\n",
      "point algorithm. SIAM journal on control and optimization,\n",
      "14(5):877898, 1976.\n",
      "Spearman, Charles. The proof and measurement of association\n",
      "between two things. The American journal of psychology, 15\n",
      "(1):72101, 1904.\n",
      "Voorhees, Ellen M. Overview of the trec 2001 question answering\n",
      "track. In NIST special publication, pp. 4251, 2002.\n",
      "Wiebe, Janyce, Wilson, Theresa, and Cardie, Claire. Annotating\n",
      "expressions of opinions and emotions in language. Language\n",
      "resources and evaluation, 39(2):165210, 2005.\n",
      "Wieting, John, Bansal, Mohit, Gimpel, Kevin, Livescu, Karen,\n",
      "and Roth, Dan. From paraphrase database to compositional\n",
      "paraphrase model and back. In TACL - Transactions of the\n",
      "Association for Computational Linguistics, 2015.\n",
      "Wieting, John, Bansal, Mohit, Gimpel, Kevin, and Livescu,\n",
      "Karen.\n",
      "Towards universal paraphrastic sentence embeddings. In International Conference on Learning Representations (ICLR), 2016a.\n",
      "Wieting, John, Bansal, Mohit, Gimpel, Kevin, and Livescu,\n",
      "Karen. Charagram: Embedding Words and Sentences via\n",
      "Character n-grams. In EMNLP - Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 15041515, Stroudsburg, PA, USA, 2016b. Association for Computational Linguistics.\n",
      "\n",
      "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features\n",
      "\n",
      "Supplementary Material\n",
      "A. Parameters for training models\n",
      "Model\n",
      "Book corpus\n",
      "Sent2Vec\n",
      "unigrams\n",
      "Book corpus\n",
      "Sent2Vec\n",
      "unigrams + bigrams\n",
      "Wiki Sent2Vec\n",
      "unigrams\n",
      "Wiki Sent2Vec\n",
      "unigrams + bigrams\n",
      "Twitter Sent2Vec\n",
      "unigrams\n",
      "Twitter Sent2Vec\n",
      "unigrams + bigrams\n",
      "\n",
      "Embedding\n",
      "Dimensions\n",
      "\n",
      "Minimum\n",
      "word count\n",
      "\n",
      "Minimum\n",
      "Target word\n",
      "Count\n",
      "\n",
      "Initial\n",
      "Lear ning\n",
      "Rate\n",
      "\n",
      "Epochs\n",
      "\n",
      "Subsampling\n",
      "hyper-parameter\n",
      "\n",
      "Bigrams\n",
      "Dropped\n",
      "per sentence\n",
      "\n",
      "Number of\n",
      "negatives\n",
      "sampled\n",
      "\n",
      "700\n",
      "\n",
      "5\n",
      "\n",
      "8\n",
      "\n",
      "0.2\n",
      "\n",
      "13\n",
      "\n",
      "1  105\n",
      "\n",
      "-\n",
      "\n",
      "10\n",
      "\n",
      "700\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "0.2\n",
      "\n",
      "12\n",
      "\n",
      "5  106\n",
      "\n",
      "7\n",
      "\n",
      "10\n",
      "\n",
      "600\n",
      "\n",
      "8\n",
      "\n",
      "20\n",
      "\n",
      "0.2\n",
      "\n",
      "9\n",
      "\n",
      "1  105\n",
      "\n",
      "-\n",
      "\n",
      "10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "10\n",
      "\n",
      "6\n",
      "\n",
      "700\n",
      "\n",
      "8\n",
      "\n",
      "20\n",
      "\n",
      "0.2\n",
      "\n",
      "5  10\n",
      "\n",
      "700\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "0.2\n",
      "\n",
      "3\n",
      "\n",
      "1  10\n",
      "\n",
      "-\n",
      "\n",
      "10\n",
      "\n",
      "700\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "0.2\n",
      "\n",
      "3\n",
      "\n",
      "1  106\n",
      "\n",
      "3\n",
      "\n",
      "10\n",
      "\n",
      "Table 5: Training parameters for the Sent2Vec models\n",
      "\n",
      "B. L1 regularization of models\n",
      "Optionally, our model can be additionally improved by adding an L1 regularizer term in the objective function, leading to\n",
      "slightly better generalization performance. Additionally, encouraging sparsity in the embedding vectors is beneficial for\n",
      "memory reasons, allowing higher embedding dimensions h.\n",
      "We propose to apply L1 regularization individually to each word (and n-gram) vector (both source and target vectors).\n",
      "Formally, the training objective function (3) then becomes\n",
      "\n",
      "\n",
      "X X\n",
      "\n",
      "(4)\n",
      "min\n",
      "qp (wt ) ` u>\n",
      "wt v S\\\\{wt } +  (kuwt k1 + kv S\\\\{wt } k1 ) +\n",
      "U ,V\n",
      "\n",
      "SC wt S\n",
      "\n",
      "|Nwt |\n",
      "\n",
      "X\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">\n",
      "qn (w ) `  uw0 v S\\\\{wt } +  (kuw0 k1 )\n",
      "0\n",
      "\n",
      "w0 V\n",
      "\n",
      "where  is the regularization parameter.\n",
      "Now, in order to minimize a function of the form f (z) + g(z) where g(z) is not differentiable over the domain, we can use\n",
      "the basic proximal-gradient scheme. In this iterative method, after doing a gradient descent step on f (z) with learning rate\n",
      ", we update z as\n",
      "zn+1 = prox,g (zn+ 12 )\n",
      "\n",
      "(5)\n",
      "\n",
      "1\n",
      "where prox,g (x) = arg miny {g(y) + 2\n",
      "ky  xk22 } is called the proximal function(Rockafellar, 1976) of g with  being\n",
      "the proximal parameter and zn+ 21 is the value of z after a gradient (or SGD) step on zn .\n",
      "\n",
      "In our case, g(z) = kzk1 and the corresponding proximal operator is given by\n",
      "prox,g (x) = sign(x)  max(|xn |  , 0)\n",
      "\n",
      "(6)\n",
      "\n",
      "where  corresponds to element-wise product.\n",
      "Similar to the proximal-gradient scheme, in our case we can optionally use the thresholding operator on the updated word\n",
      " lr 0\n",
      "and n-gram vectors after an SGD step. The soft thresholding parameter used for this update is |R(S\\\\{w\n",
      "and   lr0 for\n",
      "t })|\n",
      "0\n",
      "the source and target vectors respectively where lr is the current learning rate,  is the L1 regularization parameter and S\n",
      "is the sentence on which SGD is being run.\n",
      "\n",
      "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features\n",
      "\n",
      "We observe that L1 regularization using the proximal step gives our models a small boost in performance. Also, applying\n",
      "the thresholding operator takes only |R(S\\\\{wt })|h floating point operations for the updating the word vectors corresponding to the sentence and (|N | + 1)  h for updating the target as well as the negative word vectors, where |N | is the number of\n",
      "negatives sampled and h is the embedding dimension. Thus, performing L1 regularization using soft-thresholding operator\n",
      "comes with a small computational overhead.\n",
      "We set  to be 0.0005 for both the Wikipedia and the Toronto Book Corpus unigrams + bigrams models.\n",
      "\n",
      "C. Performance comparison with Sent2Vec models trained on different corpora\n",
      "Data\n",
      "Unordered Sentences:\n",
      "(Toronto Books)\n",
      "Unordered sentences: Wikipedia\n",
      "(69 million sentences; 1.7 B words)\n",
      "Unordered sentences: Twitter\n",
      "(1.2 billion sentences; 19.7 B words)\n",
      "\n",
      "Other structured\n",
      "Data Sources\n",
      "\n",
      "Model\n",
      "Sent2Vec uni.\n",
      "Sent2Vec uni. + bi.\n",
      "Sent2Vec uni. + bi.L1reg\n",
      "Sent2Vec uni.\n",
      "Sent2Vec uni. + bi.\n",
      "Sent2Vec uni. + bi.L1reg\n",
      "Sent2Vec uni.\n",
      "Sent2Vec uni. + bi.\n",
      "CaptionRep BOW\n",
      "CaptionRep RNN\n",
      "DictRep BOW\n",
      "DictRep BOW+embs\n",
      "DictRep RNN\n",
      "DictRep RNN+embs.\n",
      "\n",
      "MSRP (Acc / F1)\n",
      "72.2 / 80.3\n",
      "72.5 / 80.8\n",
      "71.6 / 80.1\n",
      "71.8 / 80.2\n",
      "72.4 / 80.8\n",
      "73.6 / 81.5\n",
      "71.5 / 80.0\n",
      "72.4 / 80.6\n",
      "73.6 / 81.9\n",
      "72.6 / 81.1\n",
      "73.7 / 81.6\n",
      "68.4 / 76.8\n",
      "73.2 / 81.6\n",
      "66.8 / 76.0\n",
      "\n",
      "MR\n",
      "75.1\n",
      "75.8\n",
      "76.1\n",
      "77.3\n",
      "77.9\n",
      "78.1\n",
      "77.1\n",
      "78.0\n",
      "61.9\n",
      "55.0\n",
      "71.3\n",
      "76.7\n",
      "67.8\n",
      "72.5\n",
      "\n",
      "CR\n",
      "80.2\n",
      "80.3\n",
      "80.9\n",
      "80.3\n",
      "80.9\n",
      "81.5\n",
      "81.3\n",
      "82.1\n",
      "69.3\n",
      "64.9\n",
      "75.6\n",
      "78.7\n",
      "72.7\n",
      "73.5\n",
      "\n",
      "SUBJ\n",
      "90.6\n",
      "91.2\n",
      "91.1\n",
      "92.0\n",
      "92.6\n",
      "92.8\n",
      "90.8\n",
      "91.8\n",
      "77.4\n",
      "64.9\n",
      "86.6\n",
      "90.7\n",
      "81.4\n",
      "85.6\n",
      "\n",
      "MPQA\n",
      "86.3\n",
      "85.9\n",
      "86.1\n",
      "87.4\n",
      "86.9\n",
      "87.2\n",
      "87.3\n",
      "86.7\n",
      "70.8\n",
      "71.0\n",
      "82.5\n",
      "87.2\n",
      "82.5\n",
      "85.7\n",
      "\n",
      "TREC\n",
      "83.8\n",
      "86.4\n",
      "86.8\n",
      "85.4\n",
      "89.2\n",
      "87.4\n",
      "85.4\n",
      "89.8\n",
      "72.2\n",
      "62.4\n",
      "73.8\n",
      "81.0\n",
      "75.8\n",
      "72.0\n",
      "\n",
      "Average\n",
      "81.4\n",
      "82.0\n",
      "82.1\n",
      "82.4\n",
      "83.3\n",
      "83.4\n",
      "82.2\n",
      "83.5\n",
      "70.9\n",
      "65.1\n",
      "77.3\n",
      "80.5\n",
      "75.6\n",
      "76.0\n",
      "\n",
      "Table 6: Comparison of the performance of different Sent2Vec models with different semi-supervised/supervised models\n",
      "on different downstream supervised evaluation tasks. An underline indicates the best performance for the dataset and\n",
      "Sent2Vec model performances are bold if they perform as well or better than all other non-Sent2Vec models, including\n",
      "those presented in Table 1.\n",
      "\n",
      "Model\n",
      "Sent2Vec book corpus uni.\n",
      "Sent2Vec book corpus uni. + bi.\n",
      "Sent2Vec book corpus uni. + bi. L1 reg\n",
      "Sent2Vec wiki uni.\n",
      "Sent2Vec wiki uni. + bi.\n",
      "Sent2Vec wiki uni. + bi. L1 reg\n",
      "Sent2Vec twitter uni.\n",
      "Sent2Vec twitter uni. + bi.\n",
      "CaptionRep BOW\n",
      "CaptionRep RNN\n",
      "DictRep BOW\n",
      "DictRep BOW + embs.\n",
      "DictRep RNN\n",
      "DictRep RNN + embs.\n",
      "\n",
      "News\n",
      ".62/.67\n",
      ".62/.67\n",
      ".62/.68\n",
      ".66/.71\n",
      ".68/.74\n",
      ".69/.75\n",
      ".67/.74\n",
      ".68/.74\n",
      ".26/.26\n",
      ".05/.05\n",
      ".62/.67\n",
      ".65/.72\n",
      ".40/.46\n",
      ".51/.60\n",
      "\n",
      "Forum\n",
      ".49/.49\n",
      ".51/.51\n",
      ".51/.52\n",
      ".47/.47\n",
      ".50/.50\n",
      ".52/.52\n",
      ".52/.53\n",
      ".54/.54\n",
      ".29/.22\n",
      ".13/.09\n",
      ".42/.40\n",
      ".49/.47\n",
      ".26/.23\n",
      ".29/.27\n",
      "\n",
      "STS 2014\n",
      "WordNet\n",
      "Twitter\n",
      ".75/.72.\n",
      ".70/.75\n",
      ".71/.68\n",
      ".70/.75\n",
      ".72/.70\n",
      ".69/.75\n",
      ".70/.68\n",
      ".68/.72\n",
      ".66/.64\n",
      ".67/.72\n",
      ".72/.69\n",
      ".67/.72\n",
      ".75/.72\n",
      ".72/.78\n",
      ".72/.69\n",
      ".70/.77\n",
      ".50/.35\n",
      ".37/.31\n",
      ".40/.33\n",
      ".36/.30\n",
      ".81/.81\n",
      ".62/.66\n",
      ".85/.86\n",
      ".67/.72\n",
      ".78/.78\n",
      ".42/.42\n",
      ".80/.81\n",
      ".44/.47\n",
      "\n",
      "Images\n",
      ".78/.82\n",
      ".75/.79\n",
      ".76/.81\n",
      ".76/.79\n",
      ".75/.79\n",
      ".76/.80\n",
      ".77/.81\n",
      ".76/.79\n",
      ".78/.81\n",
      ".76/.82\n",
      ".66/.68\n",
      ".71/.74\n",
      ".56/.56\n",
      ".65/.70\n",
      "\n",
      "Headlines\n",
      ".61/.63\n",
      ".59/.62\n",
      ".60/.63\n",
      ".63/.67\n",
      ".62/.67\n",
      ".61/.66\n",
      ".64/.68\n",
      ".62/.67\n",
      ".39/.36\n",
      ".30/.28\n",
      ".53/.58\n",
      ".57/.61\n",
      ".38/.40\n",
      ".42/.46\n",
      "\n",
      "SICK 2014\n",
      "Test + Train\n",
      ".61/.70\n",
      ".62/.70\n",
      ".62/.71\n",
      ".64/.71\n",
      ".63/.71\n",
      ".63/.72\n",
      ".62/.71\n",
      ".63/.72\n",
      ".45/.44\n",
      ".36/.35\n",
      ".61/.63\n",
      ".61/.70\n",
      ".47/.49\n",
      ".52/.56\n",
      "\n",
      "Average\n",
      ".65/.68\n",
      ".65/.67\n",
      ".66/.68\n",
      ".65/.68\n",
      ".65/.68\n",
      ".66/.69\n",
      ".67/.71\n",
      ".66/.70\n",
      ".54/.62\n",
      ".51/.59\n",
      ".58/.66\n",
      ".62/.70\n",
      ".49/.55\n",
      ".49/.59\n",
      "\n",
      "Table 7: Unsupervised Evaluation: Comparison of the performance of different Sent2Vec models with semisupervised/supervised models on Spearman/Pearson correlation measures. An underline indicates the best performance\n",
      "for the dataset and Sent2Vec model performances are bold if they perform as well or better than all other non-Sent2Vec\n",
      "models, including those presented in Table 2.\n",
      "\n",
      "D. Dataset Description\n",
      "Sentence Length\n",
      "Average\n",
      "Standard Deviation\n",
      "\n",
      "News\n",
      "17.23\n",
      "8.66\n",
      "\n",
      "Forum\n",
      "10.12\n",
      "3.30\n",
      "\n",
      "STS 2014\n",
      "WordNet\n",
      "Twitter\n",
      "8.85\n",
      "11.64\n",
      "3.10\n",
      "5.28\n",
      "\n",
      "Images\n",
      "10.17\n",
      "2.77\n",
      "\n",
      "Headlines\n",
      "7.82\n",
      "2.21\n",
      "\n",
      "SICK 2014\n",
      "Test + Train\n",
      "9.67\n",
      "3.75\n",
      "\n",
      "Wikipedia\n",
      "Dataset\n",
      "25.25\n",
      "12.56\n",
      "\n",
      "Twitter\n",
      "Dataset\n",
      "16.31\n",
      "7.22\n",
      "\n",
      "Table 8: Average sentence lengths for the datasets used in the comparison.\n",
      "\n",
      "Book Corpus\n",
      "Dataset\n",
      "13.32\n",
      "8.94\n",
      "\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "# Remove Unicode\n",
    "import re\n",
    "lines = str(raw_text).split('\\\\n')\n",
    "for i in range(len(lines)):\n",
    "    lines[i] = re.sub(\"\\\\\\\\x..\",\"\",lines[i])\n",
    "    print(lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"b'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram \"\n",
      " 'Features',\n",
      " 'Matteo Pagliardini * 1 Prakhar Gupta * 2 Martin Jaggi 2',\n",
      " 'arXiv:1703.02507v2 [cs.CL] 10 Jul 2017',\n",
      " 'Abstract The recent tremendous success of unsupervised word embeddings in a '\n",
      " 'multitude of applications raises the obvious question if similar methods '\n",
      " 'could be derived to improve embeddings (i.e. semantic representations) of '\n",
      " 'word sequences as well. We present a simple but efficient unsupervised '\n",
      " 'objective to train distributed representations of sentences. Our method '\n",
      " 'outperforms the state-of-the-art unsupervised models on most benchmark '\n",
      " 'tasks, highlighting the robustness of the produced general-purpose sentence '\n",
      " 'embeddings.',\n",
      " '1. Introduction Improving unsupervised learning is of key importance for '\n",
      " 'advancing machine learning methods, as to unlock access to almost unlimited '\n",
      " 'amounts of data to be used as training resources. The majority of recent '\n",
      " 'success stories of deep learning does not fall into this category but '\n",
      " 'instead relied on supervised training (in particular in the vision domain). '\n",
      " 'A very notable exception comes from the text and natural language processing '\n",
      " 'domain, in the form of semantic word embeddings trained unsupervised '\n",
      " '(Mikolov et al., 2013b;a; Pennington et al., 2014). Within only a few years '\n",
      " 'from their invention, such word representations  which are based on a simple '\n",
      " 'matrix factorization model as we formalize below  are now routinely trained '\n",
      " 'on very large amounts of raw text data, and have become ubiquitous building '\n",
      " 'blocks of a majority of current state-of-the-art NLP applications.',\n",
      " 'learning for NLP leads towards increasingly powerful and complex models, '\n",
      " 'such as recurrent neural networks (RNNs), LSTMs, attention models and even '\n",
      " 'Neural Turing Machine architectures. While extremely strong in '\n",
      " 'expressiveness, the increased model complexity makes such models much slower '\n",
      " 'to train on larger datasets. On the other end of the spectrum, simpler '\n",
      " 'shallow models such as matrix factorizations (or bilinear models) can '\n",
      " 'benefit from training on much larger sets of data, which can be a key '\n",
      " 'advantage, especially in the unsupervised setting. Surprisingly, for '\n",
      " 'constructing sentence embeddings, naively using averaged word vectors was '\n",
      " 'recently shown to outperform LSTMs (see (Wieting et al., 2016a) for plain '\n",
      " 'averaging, and (Arora et al., 2017) for weighted averaging). This example '\n",
      " 'shows potential in exploiting the tradeoff between model complexity and '\n",
      " 'ability to process huge amounts of text using scalable algorithms, towards '\n",
      " 'the simpler side. In view of this trade-off, our work here further advances '\n",
      " 'unsupervised learning of sentence embeddings. Our proposed model can be seen '\n",
      " 'as an extension of the CBOW (Mikolov et al., 2013b;a) training objective to '\n",
      " 'train sentence instead of word embeddings. We demonstrate that the empirical '\n",
      " 'performance of our resulting general-purpose sentence embeddings very '\n",
      " 'significantly exceeds the state of the art, while keeping the model '\n",
      " 'simplicity as well as training and inference complexity exactly as low as in '\n",
      " 'averaging methods (Wieting et al., 2016a; Arora et al., 2017), thereby also '\n",
      " 'putting the title of (Arora et al., 2017) in perspective. Contributions. The '\n",
      " 'main contributions in this work can be summarized as follows:1',\n",
      " 'While very useful semantic representations are available for words, it '\n",
      " 'remains challenging to produce and learn such semantic embeddings for longer '\n",
      " 'pieces of text, such as sentences, paragraphs or entire documents. Even more '\n",
      " 'so, it remains a key goal to learn such general-purpose representations in '\n",
      " 'an unsupervised way.',\n",
      " ' Model. We propose Sent2Vec, a simple unsupervised model allowing to compose '\n",
      " 'sentence embeddings using the word vectors along with n-gram embeddings, '\n",
      " 'simultaneously training composition and the embedding vectors themselves.',\n",
      " 'Currently, two contrary research trends have emerged in text understanding: '\n",
      " 'On one hand, a strong trend in deep-',\n",
      " ' Scalability. The computational complexity of our embeddings is only O(1) '\n",
      " 'vector operations per word processed, both during training and inference of '\n",
      " 'the sen-',\n",
      " '*',\n",
      " 'Equal contribution 1 Iprova SA, Switzerland 2 Computer and Communication '\n",
      " 'Sciences, EPFL, Switzerland. Correspondence to: Martin Jaggi '\n",
      " '<martin.jaggi@epfl.ch>.',\n",
      " '1 All our code and pre-trained models are publicly available on '\n",
      " 'http://github.com/epfml/sent2vec.',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " 'tence embeddings. This strongly contrasts all neural network based '\n",
      " 'approaches, and allows our model to learn from extremely large datasets, '\n",
      " 'which is a crucial advantage in the unsupervised setting.  Performance. Our '\n",
      " 'method shows significant performance improvements compared to the current '\n",
      " 'stateof-the-art unsupervised and even semi-supervised models. The resulting '\n",
      " 'general-purpose embeddings show strong robustness when transferred to a wide '\n",
      " 'range of prediction benchmarks.',\n",
      " 'Formally, we learn source v w and target uw embeddings for each word w in '\n",
      " 'the vocabulary, with embedding dimension h and k = |V| as in (1). The '\n",
      " 'sentence embedding is defined as the average of the source word embeddings '\n",
      " 'of its constituent words, as in (2). We augment this model furthermore by '\n",
      " 'also learning source embeddings for not only unigrams but also n-grams '\n",
      " 'present in each sentence, and averaging the n-gram embeddings along with the '\n",
      " 'words, i.e., the sentence embedding v S for S is modeled as v S :=',\n",
      " '1 |R(S)| V R(S)',\n",
      " '=',\n",
      " '1 |R(S)|',\n",
      " 'X',\n",
      " 'vw',\n",
      " '(2)',\n",
      " 'wR(S)',\n",
      " '2. Model Our model is inspired by simple matrix factor models (bilinear '\n",
      " 'models) such as recently very successfully used in unsupervised learning of '\n",
      " 'word embeddings (Mikolov et al., 2013b;a; Pennington et al., 2014; '\n",
      " 'Bojanowski et al., 2017) as well as supervised of sentence classification '\n",
      " '(Joulin et al., 2017). More precisely, these models are formalized as an '\n",
      " 'optimization problem of the form min',\n",
      " 'U ,V',\n",
      " 'X',\n",
      " 'fS (U V S )',\n",
      " '(1)',\n",
      " 'where R(S) is the list of n-grams (including unigrams) present in sentence '\n",
      " 'S. In order to predict a missing word from the context, our objective models '\n",
      " 'the softmax output approximated by negative sampling following (Mikolov et '\n",
      " 'al., 2013b). For the large number of output classes |V| to be predicted, '\n",
      " 'negative sampling is known to significantly improve training efficiency, see '\n",
      " 'also (Goldberg & Levy, 2014). Given the binary logistic loss function ` : x '\n",
      " '7 log (1 + ex ) coupled with negative sampling, our unsupervised training '\n",
      " 'objective is formulated as follows:',\n",
      " 'SC',\n",
      " 'for two parameter matrices U  Rkh and V  Rh|V| , where V denotes the '\n",
      " 'vocabulary. In all models studied, the columns of the matrix V will collect '\n",
      " 'the learned word vectors, having h dimensions. For a given sentence S, which '\n",
      " 'can be of arbitrary length, the indicator vector S  {0, 1}|V| is a binary '\n",
      " 'vector encoding S (bag of words encoding). Fixed-length context windows S '\n",
      " 'running over the corpus are used in word embedding methods as in C-BOW '\n",
      " '(Mikolov et al., 2013b;a) and GloVe (Pennington et al., 2014). Here we have '\n",
      " 'k = |V| and each cost function fS : Rk  R only depends on a single row of '\n",
      " 'its input, describing the observed target word for the given fixed-length '\n",
      " 'context S. In contrast, for sentence embeddings which are the focus of our '\n",
      " 'paper here, S will be entire sentences or documents (therefore variable '\n",
      " 'length). This property is shared with the supervised FastText classifier '\n",
      " '(Joulin et al., 2017), which however uses soft-max with k  |V| being the '\n",
      " 'number of class labels. 2.1. Proposed Unsupervised Model We propose a new '\n",
      " 'unsupervised model, Sent2Vec, for learning universal sentence embeddings. '\n",
      " 'Conceptually, the model can be interpreted as a natural extension of the '\n",
      " 'wordcontexts from C-BOW (Mikolov et al., 2013b;a) to a larger sentence '\n",
      " 'context, with the sentence words being specifically optimized towards '\n",
      " 'additive combination over the sentence, by means of the unsupervised '\n",
      " 'objective function.',\n",
      " 'min',\n",
      " 'U ,V',\n",
      " '',\n",
      " 'X X ',\n",
      " ' X > ` u> v + `',\n",
      " 'u v 0 wt S\\\\\\\\{wt } w S\\\\\\\\{wt } w0 Nwt',\n",
      " 'SC wt S',\n",
      " 'where S corresponds to the current sentence and Nwt is the set of words '\n",
      " 'sampled negatively for the word wt  S. The negatives are sampled2 following '\n",
      " 'a multinomial distribution where with a probability  word',\n",
      " ' each P w ispassociated fwi , where fw is the norqn (w) := fw wi V malized '\n",
      " 'frequency of w in the corpus. To select the possible target unigrams '\n",
      " '(positives), we use subsampling as in (Joulin et al., 2017; Bojanowski et '\n",
      " 'al., 2017), each word w being discarded 1  p with probability \\\\t qp (w) '\n",
      " 'where qp (w) := min 1, t/fw + t/fw . Where t is the subsampling '\n",
      " 'hyper-parameter. Subsampling prevents very frequent words of having too much '\n",
      " 'influence in the learning as they would introduce strong biases in the '\n",
      " 'prediction task. With positives subsampling and respecting the negative '\n",
      " 'sampling distribution, the precise training objective function becomes X X',\n",
      " 'min qp (wt )` u> (3) wt v S\\\\\\\\{wt } U ,V',\n",
      " 'SC wt S',\n",
      " '+ |Nwt |',\n",
      " 'X',\n",
      " '0',\n",
      " 'qn (w )` ',\n",
      " 'u> w0 v S\\\\\\\\{wt }',\n",
      " '',\n",
      " '',\n",
      " '',\n",
      " '',\n",
      " 'w0 V 2',\n",
      " 'To efficiently sample negatives, a pre-processing table is constructed, '\n",
      " 'containing the words corresponding to the square root of their corpora '\n",
      " 'frequency. Then, the negatives Nwt are sampled uniformly at random from the '\n",
      " 'negatives table except the target wt itself, following (Joulin et al., 2017; '\n",
      " 'Bojanowski et al., 2017).',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " '2.2. Computational Efficiency In contrast to more complex neural network '\n",
      " 'based models, one of the core advantages of the proposed technique is the '\n",
      " 'low computational cost for both inference and training. Given a sentence S '\n",
      " 'and a trained model, computing the sentence representation v S only requires '\n",
      " '|S|  h floating point operations (or |R(S)|  h to be precise for the n-gram '\n",
      " 'case, see (2)), where h is the embedding dimension. The same holds for the '\n",
      " 'cost of training with SGD on the objective (3), per sentence seen in the '\n",
      " 'training corpus. Due to the simplicity of the model, parallel training is '\n",
      " 'straight-forward using parallelized or distributed SGD. 2.3. Comparison to '\n",
      " 'C-BOW C-BOW (Mikolov et al., 2013b;a) tries to predict a chosen target word '\n",
      " 'given its fixed-size context window, the context being defined by the '\n",
      " 'average of the vectors associated with the words at a distance less than the '\n",
      " 'window size hyperparameter ws. If our system, when restricted to unigram '\n",
      " 'features, can be seen as an extension of C-BOW where the context window '\n",
      " 'includes the entire sentence, in practice there are few important '\n",
      " 'differences as C-BOW uses important tricks to facilitate the learning of '\n",
      " 'word embeddings. C-BOW first uses frequent word subsampling on the '\n",
      " 'sentences, deciding to discard each token w with probability qp (w) or alike '\n",
      " '(small variations exist across implementations). Subsampling prevents the '\n",
      " 'generation of n-grams features, and deprives the sentence of an important '\n",
      " 'part of its syntactical features. It also shortens the distance between '\n",
      " 'subsampled words, implicitly increasing the span of the context window. A '\n",
      " 'second trick consists of using dynamic context windows: for each subsampled '\n",
      " 'word w, the size of its associated context window is sampled uniformly '\n",
      " 'between 1 and ws. Using dynamic context windows is equivalent to weighing by '\n",
      " 'the distance from the focus word w divided by the window size (Levy et al., '\n",
      " '2015). This makes the prediction task local, and go against our objective of '\n",
      " 'creating sentence embeddings as we want to learn how to compose all n-gram '\n",
      " 'features present in a sentence. In the results section, we report a '\n",
      " 'significant improvement of our method over C-BOW. 2.4. Model Training Three '\n",
      " 'different datasets have been used to train our models: the Toronto book '\n",
      " 'corpus3 , Wikipedia sentences and tweets. The Wikipedia and Toronto books '\n",
      " 'sentences have been tokenized using the Stanford NLP library (Manning et '\n",
      " 'al., 2014), while for tweets we used the NLTK tweets tokenizer (Bird et al., '\n",
      " '2009). For training, we select a sentence randomly from the dataset and then '\n",
      " 'proceed to select all the 3',\n",
      " 'http://www.cs.toronto.edu/mbweb/',\n",
      " 'possible target unigrams using subsampling. We update the weights using SGD '\n",
      " 'with a linearly decaying learning rate. Also, to prevent overfitting, for '\n",
      " 'each sentence we use dropout on its list of n-grams R(S) \\\\\\\\ {U (S)}, where '\n",
      " 'U (S) is the set of all unigrams contained in sentence S. After empirically '\n",
      " 'trying multiple dropout schemes, we find that dropping K n-grams (n > 1) for '\n",
      " 'each sentence is giving superior results compared to dropping each token '\n",
      " 'with some fixed probability. This dropout mechanism would negatively impact '\n",
      " 'shorter sentences. The regularization can be pushed further by applying L1 '\n",
      " 'regularization to the word vectors. Encouraging sparsity in the embedding '\n",
      " 'vectors is particularly beneficial for high dimension h. The additional soft '\n",
      " 'thresholding in every SGD step adds negligible computational cost. See also '\n",
      " 'Appendix B. We train two models on each dataset, one with unigrams only and '\n",
      " 'one with unigrams and bigrams. All training parameters for the models are '\n",
      " 'provided in Table 5 in the supplementary material. Our C++ implementation '\n",
      " 'builds upon the FastText library (Joulin et al., 2017; Bojanowski et al., '\n",
      " '2017). We will make our code and pre-trained models available open-source.',\n",
      " '3. Related Work We discuss existing models which have been proposed to '\n",
      " 'construct sentence embeddings. While there is a large body of works in this '\n",
      " 'direction  several among these using e.g. labelled datasets of paraphrase '\n",
      " 'pairs to obtain sentence embeddings in a supervised manner (Wieting et al., '\n",
      " '2016b;a)  we here focus on unsupervised, task-independent models. While some '\n",
      " 'methods require ordered raw text i.e., a coherent corpus where the next '\n",
      " 'sentence is a logical continuation of the previous sentence, others rely '\n",
      " 'only on raw text i.e., an unordered collection of sentences. Finally we also '\n",
      " 'discuss alternative models built from structured data sources. 3.1. '\n",
      " 'Unsupervised Models Independent of Sentence Ordering The ParagraphVector '\n",
      " 'DBOW model (Le & Mikolov, 2014) is a log-linear model which is trained to '\n",
      " 'learn sentence as well as word embeddings and then use a softmax '\n",
      " 'distribution to predict words contained in the sentence given the sentence '\n",
      " 'vector representation. They also propose a different model ParagraphVector '\n",
      " 'DM where they use n-grams of consecutive words along with the sentence '\n",
      " 'vector representation to predict the next word. (Hill et al., 2016a) propose '\n",
      " 'a Sequential (Denoising) Autoencoder, S(D)AE. This model first introduces '\n",
      " 'noise in the input data: Firstly each word is deleted with probability p0 , '\n",
      " 'then for each non-overlapping bigram, words',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " 'are swapped with probability px . The model then uses an LSTM-based '\n",
      " 'architecture to retrieve the original sentence from the corrupted version. '\n",
      " 'The model can then be used to encode new sentences into vector '\n",
      " 'representations. In the case of p0 = px = 0, the model simply becomes a '\n",
      " 'Sequential Autoencoder. (Hill et al., 2016a) also propose a variant (S(D)AE '\n",
      " '+ embs.) in which the words are represented by fixed pre-trained word vector '\n",
      " 'embeddings. (Arora et al., 2017) propose a model in which sentences are '\n",
      " 'represented as a weighted average of fixed (pre-trained) word vectors, '\n",
      " 'followed by post-processing step of subtracting the principal component. '\n",
      " 'Using the generative model of (Arora et al., 2016), words are generated '\n",
      " 'conditioned on a sentence discourse vector cs : P r[w | cs ] = fw + (1  )',\n",
      " 'exp(c> s vw ) , Zcs',\n",
      " '> wV exp(cs v w )',\n",
      " 'P',\n",
      " 'where Zcs := and cs := c0 + (1  )cs and ,  are scalars. c0 is the common '\n",
      " 'discourse vector, representing a shared component among all discourses, '\n",
      " 'mainly related to syntax. It allows the model to better generate syntactical '\n",
      " 'features. The fw term is here to enable the model to generate some frequent '\n",
      " 'words even if their matching with the discourse vector cs is low. Therefore, '\n",
      " 'this model tries to generate sentences as a mixture of three type of words: '\n",
      " 'words matching the sentence discourse vector cs , syntactical words matching '\n",
      " 'c0 , and words with high fw . (Arora et al., 2017) demonstrated thatPfor '\n",
      " 'this model, the MLE of cs can be approximated by wS fwa+a v w , where a is a '\n",
      " 'scalar. The sentence discourse vector can hence be obtained by subtracting '\n",
      " 'c0 estimated by the first principal component of cs s on a set of sentences. '\n",
      " 'In other words, the sentence embeddings are obtained by a weighted average '\n",
      " 'of the word vectors stripping away the syntax by subtracting the common '\n",
      " 'discourse vector and down-weighting frequent tokens. They generate sentence '\n",
      " 'embeddings from diverse pre-trained word embeddings among which are '\n",
      " 'unsupervised word embeddings such as GloVe (Pennington et al., 2014) as well '\n",
      " 'as supervised word embeddings such as paragram-SL999 (PSL) (Wieting et al., '\n",
      " '2015) trained on the Paraphrase Database (Ganitkevitch et al., 2013). In a '\n",
      " 'very different line of work, C-PHRASE (Pham et al., 2015) relies on '\n",
      " 'additional information from the syntactic parse tree of each sentence, which '\n",
      " 'is incorporated into the C-BOW training objective. (Huang & Anandkumar, '\n",
      " '2016) show that single layer CNNs can be modeled using a tensor '\n",
      " 'decomposition approach. While building on an unsupervised objective, the '\n",
      " 'employed dictionary learning step for obtaining phrase templates is '\n",
      " 'task-specific (for each use-case), not resulting in general-purpose '\n",
      " 'embeddings.',\n",
      " '3.2. Unsupervised Models Depending on Sentence Ordering The SkipThought '\n",
      " 'model (Kiros et al., 2015) combines sentence level models with recurrent '\n",
      " 'neural networks. Given a sentence Si from an ordered corpus, the model is '\n",
      " 'trained to predict Si1 and Si+1 . FastSent (Hill et al., 2016a) is a '\n",
      " 'sentence-level log-linear bag-of-words model. Like SkipThought, it uses '\n",
      " 'adjacent sentences as the prediction target and is trained in an '\n",
      " 'unsupervised fashion. Using word sequences allows the model to improve over '\n",
      " 'the earlier work of paragraph2vec (Le & Mikolov, 2014). (Hill et al., 2016a) '\n",
      " 'augment FastSent further by training it to predict the constituent words of '\n",
      " 'the sentence as well. This model is named FastSent + AE in our comparisons. '\n",
      " 'Compared to our approach, Siamese C-BOW (Kenter et al., 2016) shares the '\n",
      " 'idea of learning to average word embeddings over a sentence. However, it '\n",
      " 'relies on a Siamese neural network architecture to predict surrounding '\n",
      " 'sentences, contrasting our simpler unsupervised objective. Note that on the '\n",
      " 'character sequence level instead of word sequences, FastText (Bojanowski et '\n",
      " 'al., 2017) uses the same conceptual model to obtain better word embeddings. '\n",
      " 'This is most similar to our proposed model, with two key differences: '\n",
      " 'Firstly, we predict from source word sequences to target words, as opposed '\n",
      " 'to character sequences to target words, and secondly, our model is averaging '\n",
      " 'the source embeddings instead of summing them. 3.3. Models requiring '\n",
      " 'structured data DictRep (Hill et al., 2016b) is trained to map dictionary '\n",
      " 'definitions of the words to the pre-trained word embeddings of these words. '\n",
      " 'They use two different architectures, namely BOW and RNN (LSTM) with the '\n",
      " 'choice of learning the input word embeddings or using them pre-trained. A '\n",
      " 'similar architecture is used by the CaptionRep variant, but here the task is '\n",
      " 'the mapping of given image captions to a pre-trained vector representation '\n",
      " 'of these images.',\n",
      " '4. Evaluation Tasks We use a standard set of supervised as well as '\n",
      " 'unsupervised benchmark tasks from the literature to evaluate our trained '\n",
      " 'models, following (Hill et al., 2016a). The breadth of tasks allows to '\n",
      " 'fairly measure generalization to a wide area of different domains, testing '\n",
      " 'the general-purpose quality (universality) of all competing sentence '\n",
      " 'embeddings. For downstream supervised evaluations, sentence embeddings are '\n",
      " 'combined with logistic regression to predict target labels. In the '\n",
      " 'unsupervised evaluation for sentence similarity, correlation of the cosine '\n",
      " 'similarity between two em-',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " 'beddings is compared to human annotators. Downstream Supervised Evaluation. '\n",
      " 'Sentence embeddings are evaluated for various supervised classification '\n",
      " 'tasks as follows. We evaluate paraphrase identification (MSRP) (Dolan et '\n",
      " 'al., 2004), classification of movie review sentiment (MR) (Pang & Lee, '\n",
      " '2005), product reviews (CR) (Hu & Liu, 2004), subjectivity classification '\n",
      " '(SUBJ)(Pang & Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and '\n",
      " 'question type classification (TREC) (Voorhees, 2002). To classify, we use '\n",
      " 'the code provided by (Kiros et al., 2015) in the same manner as in (Hill et '\n",
      " 'al., 2016a). For the MSRP dataset, containing pairs of sentences (S1 , S2 ) '\n",
      " 'with associated paraphrase label, we generate feature vectors by '\n",
      " 'concatenating their Sent2Vec representations |v S1  v S2 | with the '\n",
      " 'component-wise product v S1  v S2 . The predefined training split is used to '\n",
      " 'tune the L2 penalty parameter using cross-validation and the accuracy and F1 '\n",
      " 'scores are computed on the test set. For the remaining 5 datasets, Sent2Vec '\n",
      " 'embeddings are inferred from input sentences and directly fed to a logistic '\n",
      " 'regression classifier. Accuracy scores are obtained using 10-fold '\n",
      " 'cross-validation for the MR, CR, SUBJ and MPQA datasets. For those datasets '\n",
      " 'nested cross-validation is used to tune the L2 penalty. For the TREC '\n",
      " 'dataset, as for the MRSP dataset, the L2 penalty is tuned on the predefined '\n",
      " 'train split using 10-fold cross-validation, and the accuracy is computed on '\n",
      " 'the test set. Unsupervised Similarity Evaluation. We perform unsupervised '\n",
      " 'evaluation of the the learnt sentence embeddings using the sentence cosine '\n",
      " 'similarity, on the STS 2014 (Agirre et al., 2014) and SICK 2014 (Marelli et '\n",
      " 'al., 2014) datasets. These similarity scores are compared to the '\n",
      " 'gold-standard human judgements using Pearsons r (Pearson, 1895) and '\n",
      " 'Spearmans  (Spearman, 1904) correlation scores. The SICK dataset consists of '\n",
      " 'about 10,000 sentence pairs along with relatedness scores of the pairs. The '\n",
      " 'STS 2014 dataset contains 3,770 pairs, divided into six different categories '\n",
      " 'on the basis of origin of sentences/phrases namely Twitter, headlines, news, '\n",
      " 'forum, WordNet and images. See (Agirre et al., 2014) for more precise '\n",
      " 'information on how the pairs have been created.',\n",
      " '5. Results and Discussion In Tables 1 and 2, we compare our results with '\n",
      " 'those obtained by (Hill et al., 2016a) on different models. Along with the '\n",
      " 'models discussed in Section 3, this also includes the sentence embedding '\n",
      " 'baselines obtained by simple averaging of word embeddings over the sentence, '\n",
      " 'in both the C-BOW and skip-gram variants. TF-IDF BOW is a representation '\n",
      " 'consisting of the counts of the 200,000 most common feature-words, weighed '\n",
      " 'by their TF-IDF frequencies. To ensure coherence, we only include '\n",
      " 'unsupervised mod-',\n",
      " 'els in the main paper. Performance of supervised and semisupervised models '\n",
      " 'on these evaluations can be observed in Tables 6 and 7 in the supplementary '\n",
      " 'material. Downstream Supervised Evaluation Results. On running supervised '\n",
      " 'evaluations and observing the results in Table 1, we find that on an average '\n",
      " 'our models are second only to SkipThought vectors. Also, both our models '\n",
      " 'achieve state of the art results on the CR task. We also observe that on '\n",
      " 'half of the supervised tasks, our unigrams + bigram model is the the best '\n",
      " 'model after SkipThought. Our models are weaker on the MSRP task (which '\n",
      " 'consists of the identification of labelled paraphrases) compared to '\n",
      " 'stateof-the-art methods. However, we observe that the models which perform '\n",
      " 'extremely well on this task end up faring very poorly on the other tasks, '\n",
      " 'indicating a lack of generalizability. On rest of the tasks, our models '\n",
      " 'perform extremely well. The SkipThought model is able to outperform our '\n",
      " 'models on most of the tasks as it is trained to predict the previous and '\n",
      " 'next sentences and a lot of tasks are able to make use of this contextual '\n",
      " 'information missing in our Sent2Vec models. For example, the TREC task is a '\n",
      " 'poor measure of how one predicts the content of the sentence (the question) '\n",
      " 'but a good measure of how the next sentence in the sequence (the answer) is '\n",
      " 'predicted. Unsupervised Similarity Evaluation Results. In Table 2, we see '\n",
      " 'that our Sent2Vec models are state-of-the-art on the majority of tasks when '\n",
      " 'comparing to all the unsupervised models trained on the Toronto corpus, and '\n",
      " 'clearly achieve the best averaged performance. Our Sent2Vec models also on '\n",
      " 'average outperform or are at par with the C-PHRASE model, despite '\n",
      " 'significantly lagging behind on the STS 2014 WordNet and News subtasks. This '\n",
      " 'observation can be attributed to the fact that a big chunk of the data that '\n",
      " 'the C-PHRASE model is trained on comes from English Wikipedia, helping it to '\n",
      " 'perform well on datasets involving definition and news items. Also, C-PHRASE '\n",
      " 'uses data three times the size of the Toronto book corpus. Interestingly, '\n",
      " 'our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table '\n",
      " '3, despite the fact that we use no parse tree information. In the official '\n",
      " 'results of the more recent edition of the STS 2017 benchmark (Cer et al., '\n",
      " '2017), our model also significantly outperforms C-PHRASE, and delivers the '\n",
      " 'best unsupervised baseline method. Macro Average. To summarize our '\n",
      " 'contributions on both supervised and unsupervised tasks, in Table 3 we '\n",
      " 'present the results in terms of the macro average over the averages 4 For '\n",
      " 'the Siamese C-BOW model trained on the Toronto corpus, supervised evaluation '\n",
      " 'as well as similarity evaluation results on the SICK 2014 dataset are '\n",
      " 'unavailable.',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features Data',\n",
      " 'Unordered Sentences: (Toronto Books; 70 million sentences, 0.9 Billion '\n",
      " 'Words)',\n",
      " 'Ordered Sentences: Toronto Books 2.8 Billion words',\n",
      " 'Model SAE SAE + embs. SDAE SDAE + embs. ParagraphVec DBOW ParagraphVec DM '\n",
      " 'Skipgram C-BOW Unigram TFIDF Sent2Vec uni. Sent2Vec uni. + bi. SkipThought '\n",
      " 'FastSent FastSent+AE C-PHRASE',\n",
      " 'MSRP (Acc / F1) 74.3 / 81.7 70.6 / 77.9 76.4 / 83.4 73.7 / 80.7 72.9 / 81.1 '\n",
      " '73.6 / 81.9 69.3 / 77.2 67.6 / 76.1 73.6 / 81.7 72.2 / 80.3 72.5 / 80.8 73.0 '\n",
      " '/ 82.0 72.2 / 80.3 71.2 / 79.1 72.2 / 79.6',\n",
      " 'MR 62.6 73.2 67.6 74.6 60.2 61.5 73.6 73.6 73.7 75.1 75.8 76.5 70.8 71.8 '\n",
      " '75.7',\n",
      " 'CR 68.0 75.3 74.0 78.0 66.9 68.6 77.3 77.3 79.2 80.2 80.3 80.1 78.4 76.7 '\n",
      " '78.8',\n",
      " 'SUBJ 86.1 89.8 89.3 90.8 76.3 76.4 89.2 89.1 90.3 90.6 91.2 93.6 88.7 88.8 '\n",
      " '91.1',\n",
      " 'MPQA 76.8 86.2 81.3 86.9 70.7 78.1 85.0 85.0 82.4 86.3 85.9 87.1 80.6 81.5 '\n",
      " '86.2',\n",
      " 'TREC 80.2 80.4 77.7 78.4 59.4 55.8 82.2 82.2 85.0 83.8 86.4 92.2 76.8 80.4 '\n",
      " '78.8',\n",
      " 'Average 74.7 79.3 78.3 80.4 67.7 69.0 78.5 79.1 80.7 81.4 82.0 83.8 77.9 '\n",
      " '78.4 80.5',\n",
      " 'Table 1: Comparison of the performance of different models on different '\n",
      " 'supervised evaluation tasks. An underline indicates the best performance for '\n",
      " 'the dataset. Top 3 performances in each data category are shown in bold. The '\n",
      " 'average is calculated as the average of accuracy for each category (For '\n",
      " 'MSRP, we take the average of two entries.) Model SAE SAE + embs. SDAE SDAE + '\n",
      " 'embs. ParagraphVec DBOW ParagraphVec DM Skipgram C-BOW Unigram TF-IDF '\n",
      " 'Sent2Vec uni. Sent2Vec uni. + bi. SkipThought FastSent FastSent+AE Siamese '\n",
      " 'C-BOW4 C-PHRASE',\n",
      " 'News .17/.16 .52/.54 .07/.04 .51/.54 .31/.34 .42/.46 .56/.59 .57/.61 .48/.48 '\n",
      " '.62/.67 .62/.67 .44/.45 .58/.59 .56/.59 .58/.59 .69/.71',\n",
      " 'Forum .12/.12 .22/.23 .11/.13 .29/.29 .32/.32 .33/.34 .42/.42 .43/.44 '\n",
      " '.40/.38 .49/.49 .51/.51 .14/.15 .41/.36 .41/.40 .42/.41 .43/.41',\n",
      " 'STS 2014 WordNet Twitter .30/.23 .28/.22 .60/.55 .60/.60 .33/.24 .44/.42 '\n",
      " '.56/.50 .57/.58 .53/.50 .43/.46 .51/.48 .54/.57 .73/.70 .71/.74 .72/.69 '\n",
      " '.71/.75 .60/.59 .63/.65 .75/.72 .70/.75 .71/.68 .70/.75 .39/.34 .42/.43 '\n",
      " '.74/.70 .63/.66 .69/.64 .70/.74 .66/.61 .71/.73 .76/.73 .60/.65',\n",
      " 'Images .49/.46 .64/.64 .44/.38 .59/.59 .46/.44 .32/.30 .65/.67 .71/.73 '\n",
      " '.72/.74 .78/.82 .75/.79 .55/.60 .74/.78 .63/.65 .65/.65 .75/.79',\n",
      " 'Headlines .13/.11 .41/.41 .36/.36 .43/.44 .39/.41 .46/.47 .55/.58 .55/.59 '\n",
      " '.49/.49 .61/.63 .59/.62 .43/.44 .57/.59 .58/.60 .63/.64 .60/.65',\n",
      " 'SICK 2014 Test + Train .32/.31 .47/.49 .46/.46 .46/.46 .42/.46 .44/.40 '\n",
      " '.60/.69 .60/.69 .52/.58 .61/.70 .62/.70 .57/.60 .61/.72 .60/.65',\n",
      " '.60/.72',\n",
      " 'Average .26/.23 .50/.49 .31/.29 .49/.49 .41/.42 .43/.43 .60/.63 .60/.65 '\n",
      " '.55/.56 .65/.68 .65/.67 .42/.43 .61/.63 .60/.61',\n",
      " '.63/.67',\n",
      " 'Table 2: Unsupervised Evaluation Tasks: Comparison of the performance of '\n",
      " 'different models on Spearman/Pearson correlation measures. An underline '\n",
      " 'indicates the best performance for the dataset. Top 3 performances in each '\n",
      " 'data category are shown in bold. The average is calculated as the average of '\n",
      " 'entries for each correlation measure. of both supervised and unsupervised '\n",
      " 'tasks along with the training times of the models5 . For unsupervised tasks, '\n",
      " 'averages are taken over both Spearman and Pearson scores. The comparison '\n",
      " 'includes the best performing unsupervised and semi-supervised methods '\n",
      " 'described in Section 3. For models trained on the Toronto books dataset, we '\n",
      " 'report a 3.8 % points improvement over the state of the art. Considering all '\n",
      " 'supervised, semi-supervised methods and all datasets compared in (Hill et '\n",
      " 'al., 2016a), we report a 2.2 % points improvement. We also see a noticeable '\n",
      " 'improvement in accuracy as we use larger datasets like twitter and Wikipedia '\n",
      " 'dump. We can also see that the Sent2Vec models are also faster to train when '\n",
      " 'compared to methods like SkipThought and DictRep owing to the SGD step '\n",
      " 'allowing a high degree of parallelizability. We can clearly see Sent2Vec '\n",
      " 'outperforming other unsupervised and even semi-supervised methods. This can '\n",
      " 'be at5',\n",
      " 'time taken to train C-PHRASE models is unavailable',\n",
      " 'tributed to the superior generalizability of our model across supervised and '\n",
      " 'unsupervised tasks. Comparison with Arora et al. (2017). In Table 4, we '\n",
      " 'report an experimental comparison to the model of Arora et al. (2017), which '\n",
      " 'is particularly tailored to sentence similarity tasks. In the table, the '\n",
      " 'suffix W indicates that their down-weighting scheme has been used, while the '\n",
      " 'suffix R indicates the removal of the first principal component. They report '\n",
      " 'values of a  [104 , 103 ] as giving the best results and used a = 103 for '\n",
      " 'all their experiments. Their down-weighting scheme hints us to reduce the '\n",
      " 'importance of syntactical features. To do so, we use a simple blacklist '\n",
      " 'containing the 25 most frequent tokens in the Twitter corpus and discard '\n",
      " 'them before averaging. Results are also reported in Table 4. We observe that '\n",
      " 'our results are competitive with the embeddings of Arora et al. (2017) for '\n",
      " 'purely unsupervised methods. We confirm their empirical finding that '\n",
      " 'reducing the influence of the syntax helps performance on semantic sim-',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features Type',\n",
      " 'Training corpus',\n",
      " 'Method',\n",
      " 'unsupervised unsupervised unsupervised unsupervised unsupervised '\n",
      " 'unsupervised semi-supervised unsupervised unsupervised unsupervised '\n",
      " 'unsupervised',\n",
      " 'twitter (19.7B words) twitter (19.7B words) Wikipedia (1.7B words) Wikipedia '\n",
      " '(1.7B words) Toronto books (0.9B words) Toronto books (0.9B words) '\n",
      " 'structured dictionary dataset 2.8B words + parse info. Toronto books (0.9B '\n",
      " 'words) Toronto books (0.9B words) Toronto books (0.9B words)',\n",
      " 'Sent2Vec uni. + bi. Sent2Vec uni. Sent2Vec uni. + bi. Sent2Vec uni. Sent2Vec '\n",
      " 'books uni. Sent2Vec books uni. + bi. DictRep BOW + emb C-PHRASE C-BOW '\n",
      " 'FastSent SkipThought',\n",
      " 'Supervised average 83.5 82.2 83.3 82.4 81.4 82.0 80.5 80.5 79.1 77.9 83.8',\n",
      " 'Unsupervised average 68.3 69.0 66.2 66.3 66.7 65.9 66.9 64.9 62.8 62.0 42.5',\n",
      " 'Macro average 75.9 75.6 74.8 74.3 74.0 74.0 73.7 72.7 70.2 70.0 63.1',\n",
      " 'Training time (in hours) 6.5* 3* 2* 3.5* 1* 1.2* 24**',\n",
      " '2 2 336**',\n",
      " 'Table 3: Best unsupervised and semi-supervised methods ranked by macro '\n",
      " 'average along with their training times. ** indicates trained on GPU. * '\n",
      " 'indicates trained on a single node using 30 cores. Training times for '\n",
      " 'non-Sent2Vec models are due to (Hill et al., 2016a) Dataset STS 2014 SICK '\n",
      " '2014',\n",
      " 'Unsupervised GloVe + W 0.594 0.705',\n",
      " 'Unsupervised GloVe + WR 0.685 0.722',\n",
      " 'Semi-supervised PSL + WR 0.735 0.729',\n",
      " 'Sent2Vec Unigrams Tweets Model 0.710 0.710',\n",
      " 'Sent2Vec Unigrams Tweets Model With Blacklist 0.718 0.719',\n",
      " 'Table 4: Comparison of the performance of the unsupervised and '\n",
      " 'semi-supervised sentence embeddings by (Arora et al., 2017) with our models, '\n",
      " 'in terms of Pearsons correlation. to unsupervised evaluations but gives a '\n",
      " 'significant boost-up in accuracy on supervised tasks.',\n",
      " 'Figure 1: Left figure: the profile of the word vector L2 norms as a function '\n",
      " 'of log(fw ) for each vocabulary word w, as learnt by our unigram model '\n",
      " 'trained on Toronto books. Right figure: down-weighting scheme proposed by a '\n",
      " '. Arora et al. (2017): weight(w) = a+f w',\n",
      " 'ilarity tasks, and we show that applying a simple blacklist already yields a '\n",
      " 'noticeable amelioration. It is important to note that the scores obtained '\n",
      " 'from supervised task-specific PSL embeddings trained for the purpose of '\n",
      " 'semantic similarity outperform our method on both SICK and average STS 2014, '\n",
      " 'which is expected as our model is trained purely unsupervised. The effect of '\n",
      " 'datasets and n-grams. Despite being trained on three very different '\n",
      " 'datasets, all of our models generalize well to sometimes very specific '\n",
      " 'domains. Models trained on Toronto Corpus are the state-of-the art on the '\n",
      " 'STS 2014 images dataset even beating the supervised CaptionRep model trained '\n",
      " 'on images. We also see that addition of bigrams to our models doesnt help '\n",
      " 'much when it comes',\n",
      " 'On learning the importance and the direction of the word vectors. Our model  '\n",
      " 'by learning how to generate and compose word vectors  has to learn both the '\n",
      " 'direction of the word embeddings as well as their norm. Considering the '\n",
      " 'norms of the used word vectors as by our averaging over the sentence, we '\n",
      " 'observe an interesting distribution of the importance of each word. In '\n",
      " 'Figure 1 we show the profile of the L2 -norm as a function of log(fw ) for '\n",
      " 'each w  V, and compare it to the static down-weighting mechanism of Arora et '\n",
      " 'al. (2017). We can observe that our model is learning to down-weight '\n",
      " 'frequent tokens by itself. It is also down-weighting rare tokens and the '\n",
      " 'norm profile seems to roughly follow Luhns hypothesis (Luhn, 1958), a well '\n",
      " 'known information retrieval paradigm, stating that mid-rank terms are the '\n",
      " 'most significant to discriminate content. Modifying the objective function '\n",
      " 'would change the weighting scheme learnt. From a more semantic oriented '\n",
      " 'objective, it should be possible to learn to attribute lower norms for very '\n",
      " 'frequent terms, to more specifically fit sentence similarity tasks.',\n",
      " '6. Conclusion In this paper, we introduced a novel unsupervised and '\n",
      " 'computationally efficient method to train and infer sentence embeddings. On '\n",
      " 'supervised evaluations, our method, on an average, achieves better '\n",
      " 'performance than all other unsupervised competitors except the SkipThought '\n",
      " 'vectors. However, SkipThought vectors show an extremely poor performance on '\n",
      " 'sentence similarity tasks while our model',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " 'is state-of-the-art for these evaluations on average. Future work could '\n",
      " 'focus on augmenting the model to exploit data with ordered sentences. '\n",
      " 'Furthermore, we would like to further investigate the models ability as '\n",
      " 'giving pre-trained embeddings to enable downstream transfer learning tasks. '\n",
      " 'Acknowledgments. We are indebted to Piotr Bojanowski and Armand Joulin for '\n",
      " 'helpful discussions.',\n",
      " 'References Agirre, Eneko, Banea, Carmen, Cardie, Claire, Cer, Daniel, Diab, '\n",
      " 'Mona, Gonzalez-Agirre, Aitor, Guo, Weiwei, Mihalcea, Rada, Rigau, German, '\n",
      " 'and Wiebe, Janyce. Semeval-2014 task 10: Multilingual semantic textual '\n",
      " 'similarity. In Proceedings of the 8th international workshop on semantic '\n",
      " 'evaluation (SemEval 2014), pp. 8191. Association for Computational '\n",
      " 'Linguistics Dublin, Ireland, 2014. Arora, Sanjeev, Li, Yuanzhi, Liang, '\n",
      " 'Yingyu, Ma, Tengyu, and Risteski, Andrej. A Latent Variable Model Approach '\n",
      " 'to PMIbased Word Embeddings. In Transactions of the Association for '\n",
      " 'Computational Linguistics, pp. 385399, July 2016. Arora, Sanjeev, Liang, '\n",
      " 'Yingyu, and Ma, Tengyu. A simple but tough-to-beat baseline for sentence '\n",
      " 'embeddings. In International Conference on Learning Representations (ICLR), '\n",
      " '2017. Bird, Steven, Klein, Ewan, and Loper, Edward. Natural language '\n",
      " 'processing with Python: analyzing text with the natural language toolkit.  '\n",
      " 'OReilly Media, Inc., 2009. Bojanowski, Piotr, Grave, Edouard, Joulin, '\n",
      " 'Armand, and Mikolov, Tomas. Enriching Word Vectors with Subword Information. '\n",
      " 'Transactions of the Association for Computational Linguistics, 5:135146, '\n",
      " '2017. Cer, Daniel, Diab, Mona, Agirre, Eneko, Lopez-Gazpio, Inigo, and '\n",
      " 'Specia, Lucia. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual '\n",
      " 'and Cross-lingual Focused Evaluation. In SemEval-2017 - Proceedings of the '\n",
      " '11th International Workshop on Semantic Evaluations, pp. 114, Vancouver, '\n",
      " 'Canada, August 2017. Association for Computational Linguistics. Dolan, Bill, '\n",
      " 'Quirk, Chris, and Brockett, Chris. Unsupervised construction of large '\n",
      " 'paraphrase corpora: Exploiting massively parallel news sources. In '\n",
      " 'Proceedings of the 20th international conference on Computational '\n",
      " 'Linguistics, pp. 350. Association for Computational Linguistics, 2004. '\n",
      " 'Ganitkevitch, Juri, Van Durme, Benjamin, and Callison-Burch, Chris. Ppdb: '\n",
      " 'The paraphrase database. In HLT-NAACL, pp. 758764, 2013. Goldberg, Yoav and '\n",
      " 'Levy, Omer. word2vec Explained: deriving Mikolov et al.s negative-sampling '\n",
      " 'word-embedding method. arXiv, February 2014.',\n",
      " 'Hu, Minqing and Liu, Bing. Mining and summarizing customer reviews. In '\n",
      " 'Proceedings of the tenth ACM SIGKDD international conference on Knowledge '\n",
      " 'discovery and data mining, pp. 168177. ACM, 2004. Huang, Furong and '\n",
      " 'Anandkumar, Animashree. Unsupervised Learning of Word-Sequence '\n",
      " 'Representations from Scratch via Convolutional Tensor Decomposition. arXiv, '\n",
      " '2016. Joulin, Armand, Grave, Edouard, Bojanowski, Piotr, and Mikolov, Tomas. '\n",
      " 'Bag of Tricks for Efficient Text Classification. In Proceedings of the 15th '\n",
      " 'Conference of the European Chapter of the Association for Computational '\n",
      " 'Linguistics, Short Papers, pp. 427431, Valencia, Spain, 2017. Kenter, Tom, '\n",
      " 'Borisov, Alexey, and de Rijke, Maarten. Siamese CBOW: Optimizing Word '\n",
      " 'Embeddings for Sentence Representations. In ACL - Proceedings of the 54th '\n",
      " 'Annual Meeting of the Association for Computational Linguistics, pp. 941951, '\n",
      " 'Berlin, Germany, 2016. Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan R, '\n",
      " 'Zemel, Richard, Urtasun, Raquel, Torralba, Antonio, and Fidler, Sanja. '\n",
      " 'Skip-Thought Vectors. In NIPS 2015 - Advances in Neural Information '\n",
      " 'Processing Systems 28, pp. 32943302, 2015. Le, Quoc V and Mikolov, Tomas. '\n",
      " 'Distributed Representations of Sentences and Documents. In ICML 2014 - '\n",
      " 'Proceedings of the 31st International Conference on Machine Learning, volume '\n",
      " '14, pp. 11881196, 2014. Levy, Omer, Goldberg, Yoav, and Dagan, Ido. '\n",
      " 'Improving distributional similarity with lessons learned from word '\n",
      " 'embeddings. Transactions of the Association for Computational Linguistics, '\n",
      " '3:211225, 2015. Luhn, Hans Peter. The automatic creation of literature '\n",
      " 'abstracts. IBM Journal of research and development, 2(2):159 165, 1958. '\n",
      " 'Manning, Christopher D, Surdeanu, Mihai, Bauer, John, Finkel, Jenny Rose, '\n",
      " 'Bethard, Steven, and McClosky, David. The stanford corenlp natural language '\n",
      " 'processing toolkit. In ACL (System Demonstrations), pp. 5560, 2014. Marelli, '\n",
      " 'Marco, Menini, Stefano, Baroni, Marco, Bentivogli, Luisa, Bernardi, '\n",
      " 'Raffaella, and Zamparelli, Roberto. A sick cure for the evaluation of '\n",
      " 'compositional distributional semantic models. In LREC, pp. 216223, 2014. '\n",
      " 'Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efficient '\n",
      " 'estimation of word representations in vector space. arXiv preprint '\n",
      " 'arXiv:1301.3781, 2013a. Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, '\n",
      " 'Greg S, and Dean, Jeff. Distributed Representations of Words and Phrases and '\n",
      " 'their Compositionality. In NIPS - Advances in Neural Information Processing '\n",
      " 'Systems 26, pp. 31113119, 2013b.',\n",
      " 'Hill, Felix, Cho, Kyunghyun, and Korhonen, Anna. Learning Distributed '\n",
      " 'Representations of Sentences from Unlabelled Data. In Proceedings of '\n",
      " 'NAACL-HLT, February 2016a.',\n",
      " 'Pang, Bo and Lee, Lillian. A sentimental education: Sentiment analysis using '\n",
      " 'subjectivity summarization based on minimum cuts. In Proceedings of the 42nd '\n",
      " 'annual meeting on Association for Computational Linguistics, pp. 271. '\n",
      " 'Association for Computational Linguistics, 2004.',\n",
      " 'Hill, Felix, Cho, KyungHyun, Korhonen, Anna, and Bengio, Yoshua. Learning to '\n",
      " 'understand phrases by embedding the dictionary. TACL, 4:1730, 2016b. URL '\n",
      " 'https://tacl2013.cs.columbia.edu/ojs/ index.php/tacl/article/view/711.',\n",
      " 'Pang, Bo and Lee, Lillian. Seeing stars: Exploiting class relationships for '\n",
      " 'sentiment categorization with respect to rating scales. In Proceedings of '\n",
      " 'the 43rd annual meeting on association for computational linguistics, pp. '\n",
      " '115124. Association for Computational Linguistics, 2005.',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features Pearson, Karl. Note on regression and inheritance in the case of '\n",
      " 'two parents. Proceedings of the Royal Society of London, 58: 240242, 1895. '\n",
      " 'Pennington, Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: '\n",
      " 'Global vectors for word representation. In EMNLP, volume 14, pp. 15321543, '\n",
      " '2014. Pham, NT, Kruszewski, G, Lazaridou, A, and Baroni, M. Jointly '\n",
      " 'optimizing word representations for lexical and sentential tasks with the '\n",
      " 'c-phrase model. ACL/IJCNLP, 2015. Rockafellar, R Tyrrell. Monotone operators '\n",
      " 'and the proximal point algorithm. SIAM journal on control and optimization, '\n",
      " '14(5):877898, 1976. Spearman, Charles. The proof and measurement of '\n",
      " 'association between two things. The American journal of psychology, 15 '\n",
      " '(1):72101, 1904. Voorhees, Ellen M. Overview of the trec 2001 question '\n",
      " 'answering track. In NIST special publication, pp. 4251, 2002. Wiebe, Janyce, '\n",
      " 'Wilson, Theresa, and Cardie, Claire. Annotating expressions of opinions and '\n",
      " 'emotions in language. Language resources and evaluation, 39(2):165210, 2005. '\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, Livescu, Karen, and Roth, Dan. '\n",
      " 'From paraphrase database to compositional paraphrase model and back. In TACL '\n",
      " '- Transactions of the Association for Computational Linguistics, 2015. '\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen. Towards '\n",
      " 'universal paraphrastic sentence embeddings. In International Conference on '\n",
      " 'Learning Representations (ICLR), 2016a. Wieting, John, Bansal, Mohit, '\n",
      " 'Gimpel, Kevin, and Livescu, Karen. Charagram: Embedding Words and Sentences '\n",
      " 'via Character n-grams. In EMNLP - Proceedings of the 2016 Conference on '\n",
      " 'Empirical Methods in Natural Language Processing, pp. 15041515, Stroudsburg, '\n",
      " 'PA, USA, 2016b. Association for Computational Linguistics.',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " 'Supplementary Material A. Parameters for training models Model Book corpus '\n",
      " 'Sent2Vec unigrams Book corpus Sent2Vec unigrams + bigrams Wiki Sent2Vec '\n",
      " 'unigrams Wiki Sent2Vec unigrams + bigrams Twitter Sent2Vec unigrams Twitter '\n",
      " 'Sent2Vec unigrams + bigrams',\n",
      " 'Embedding Dimensions',\n",
      " 'Minimum word count',\n",
      " 'Minimum Target word Count',\n",
      " 'Initial Lear ning Rate',\n",
      " 'Epochs',\n",
      " 'Subsampling hyper-parameter',\n",
      " 'Bigrams Dropped per sentence',\n",
      " 'Number of negatives sampled',\n",
      " '700',\n",
      " '5',\n",
      " '8',\n",
      " '0.2',\n",
      " '13',\n",
      " '1  105',\n",
      " '-',\n",
      " '10',\n",
      " '700',\n",
      " '5',\n",
      " '5',\n",
      " '0.2',\n",
      " '12',\n",
      " '5  106',\n",
      " '7',\n",
      " '10',\n",
      " '600',\n",
      " '8',\n",
      " '20',\n",
      " '0.2',\n",
      " '9',\n",
      " '1  105',\n",
      " '-',\n",
      " '10',\n",
      " '9',\n",
      " '6',\n",
      " '4',\n",
      " '10',\n",
      " '6',\n",
      " '700',\n",
      " '8',\n",
      " '20',\n",
      " '0.2',\n",
      " '5  10',\n",
      " '700',\n",
      " '20',\n",
      " '20',\n",
      " '0.2',\n",
      " '3',\n",
      " '1  10',\n",
      " '-',\n",
      " '10',\n",
      " '700',\n",
      " '20',\n",
      " '20',\n",
      " '0.2',\n",
      " '3',\n",
      " '1  106',\n",
      " '3',\n",
      " '10',\n",
      " 'Table 5: Training parameters for the Sent2Vec models',\n",
      " 'B. L1 regularization of models Optionally, our model can be additionally '\n",
      " 'improved by adding an L1 regularizer term in the objective function, leading '\n",
      " 'to slightly better generalization performance. Additionally, encouraging '\n",
      " 'sparsity in the embedding vectors is beneficial for memory reasons, allowing '\n",
      " 'higher embedding dimensions h. We propose to apply L1 regularization '\n",
      " 'individually to each word (and n-gram) vector (both source and target '\n",
      " 'vectors). Formally, the training objective function (3) then becomes',\n",
      " '',\n",
      " 'X X',\n",
      " '(4) min qp (wt ) ` u> wt v S\\\\\\\\{wt } +  (kuwt k1 + kv S\\\\\\\\{wt } k1 ) + U '\n",
      " ',V',\n",
      " 'SC wt S',\n",
      " '|Nwt |',\n",
      " 'X',\n",
      " '',\n",
      " '',\n",
      " '',\n",
      " '> qn (w ) `  uw0 v S\\\\\\\\{wt } +  (kuw0 k1 ) 0',\n",
      " 'w0 V',\n",
      " 'where  is the regularization parameter. Now, in order to minimize a function '\n",
      " 'of the form f (z) + g(z) where g(z) is not differentiable over the domain, '\n",
      " 'we can use the basic proximal-gradient scheme. In this iterative method, '\n",
      " 'after doing a gradient descent step on f (z) with learning rate , we update '\n",
      " 'z as zn+1 = prox,g (zn+ 12 )',\n",
      " '(5)',\n",
      " '1 where prox,g (x) = arg miny {g(y) + 2 ky  xk22 } is called the proximal '\n",
      " 'function(Rockafellar, 1976) of g with  being the proximal parameter and zn+ '\n",
      " '21 is the value of z after a gradient (or SGD) step on zn .',\n",
      " 'In our case, g(z) = kzk1 and the corresponding proximal operator is given by '\n",
      " 'prox,g (x) = sign(x)  max(|xn |  , 0)',\n",
      " '(6)',\n",
      " 'where  corresponds to element-wise product. Similar to the proximal-gradient '\n",
      " 'scheme, in our case we can optionally use the thresholding operator on the '\n",
      " 'updated word  lr 0 and n-gram vectors after an SGD step. The soft '\n",
      " 'thresholding parameter used for this update is |R(S\\\\\\\\{w and   lr0 for t '\n",
      " '})| 0 the source and target vectors respectively where lr is the current '\n",
      " 'learning rate,  is the L1 regularization parameter and S is the sentence on '\n",
      " 'which SGD is being run.',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features',\n",
      " 'We observe that L1 regularization using the proximal step gives our models a '\n",
      " 'small boost in performance. Also, applying the thresholding operator takes '\n",
      " 'only |R(S\\\\\\\\{wt })|h floating point operations for the updating the word '\n",
      " 'vectors corresponding to the sentence and (|N | + 1)  h for updating the '\n",
      " 'target as well as the negative word vectors, where |N | is the number of '\n",
      " 'negatives sampled and h is the embedding dimension. Thus, performing L1 '\n",
      " 'regularization using soft-thresholding operator comes with a small '\n",
      " 'computational overhead. We set  to be 0.0005 for both the Wikipedia and the '\n",
      " 'Toronto Book Corpus unigrams + bigrams models.',\n",
      " 'C. Performance comparison with Sent2Vec models trained on different corpora '\n",
      " 'Data Unordered Sentences: (Toronto Books) Unordered sentences: Wikipedia (69 '\n",
      " 'million sentences; 1.7 B words) Unordered sentences: Twitter (1.2 billion '\n",
      " 'sentences; 19.7 B words)',\n",
      " 'Other structured Data Sources',\n",
      " 'Model Sent2Vec uni. Sent2Vec uni. + bi. Sent2Vec uni. + bi.L1reg Sent2Vec '\n",
      " 'uni. Sent2Vec uni. + bi. Sent2Vec uni. + bi.L1reg Sent2Vec uni. Sent2Vec '\n",
      " 'uni. + bi. CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW+embs '\n",
      " 'DictRep RNN DictRep RNN+embs.',\n",
      " 'MSRP (Acc / F1) 72.2 / 80.3 72.5 / 80.8 71.6 / 80.1 71.8 / 80.2 72.4 / 80.8 '\n",
      " '73.6 / 81.5 71.5 / 80.0 72.4 / 80.6 73.6 / 81.9 72.6 / 81.1 73.7 / 81.6 68.4 '\n",
      " '/ 76.8 73.2 / 81.6 66.8 / 76.0',\n",
      " 'MR 75.1 75.8 76.1 77.3 77.9 78.1 77.1 78.0 61.9 55.0 71.3 76.7 67.8 72.5',\n",
      " 'CR 80.2 80.3 80.9 80.3 80.9 81.5 81.3 82.1 69.3 64.9 75.6 78.7 72.7 73.5',\n",
      " 'SUBJ 90.6 91.2 91.1 92.0 92.6 92.8 90.8 91.8 77.4 64.9 86.6 90.7 81.4 85.6',\n",
      " 'MPQA 86.3 85.9 86.1 87.4 86.9 87.2 87.3 86.7 70.8 71.0 82.5 87.2 82.5 85.7',\n",
      " 'TREC 83.8 86.4 86.8 85.4 89.2 87.4 85.4 89.8 72.2 62.4 73.8 81.0 75.8 72.0',\n",
      " 'Average 81.4 82.0 82.1 82.4 83.3 83.4 82.2 83.5 70.9 65.1 77.3 80.5 75.6 '\n",
      " '76.0',\n",
      " 'Table 6: Comparison of the performance of different Sent2Vec models with '\n",
      " 'different semi-supervised/supervised models on different downstream '\n",
      " 'supervised evaluation tasks. An underline indicates the best performance for '\n",
      " 'the dataset and Sent2Vec model performances are bold if they perform as well '\n",
      " 'or better than all other non-Sent2Vec models, including those presented in '\n",
      " 'Table 1.',\n",
      " 'Model Sent2Vec book corpus uni. Sent2Vec book corpus uni. + bi. Sent2Vec '\n",
      " 'book corpus uni. + bi. L1 reg Sent2Vec wiki uni. Sent2Vec wiki uni. + bi. '\n",
      " 'Sent2Vec wiki uni. + bi. L1 reg Sent2Vec twitter uni. Sent2Vec twitter uni. '\n",
      " '+ bi. CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW + embs. DictRep '\n",
      " 'RNN DictRep RNN + embs.',\n",
      " 'News .62/.67 .62/.67 .62/.68 .66/.71 .68/.74 .69/.75 .67/.74 .68/.74 .26/.26 '\n",
      " '.05/.05 .62/.67 .65/.72 .40/.46 .51/.60',\n",
      " 'Forum .49/.49 .51/.51 .51/.52 .47/.47 .50/.50 .52/.52 .52/.53 .54/.54 '\n",
      " '.29/.22 .13/.09 .42/.40 .49/.47 .26/.23 .29/.27',\n",
      " 'STS 2014 WordNet Twitter .75/.72. .70/.75 .71/.68 .70/.75 .72/.70 .69/.75 '\n",
      " '.70/.68 .68/.72 .66/.64 .67/.72 .72/.69 .67/.72 .75/.72 .72/.78 .72/.69 '\n",
      " '.70/.77 .50/.35 .37/.31 .40/.33 .36/.30 .81/.81 .62/.66 .85/.86 .67/.72 '\n",
      " '.78/.78 .42/.42 .80/.81 .44/.47',\n",
      " 'Images .78/.82 .75/.79 .76/.81 .76/.79 .75/.79 .76/.80 .77/.81 .76/.79 '\n",
      " '.78/.81 .76/.82 .66/.68 .71/.74 .56/.56 .65/.70',\n",
      " 'Headlines .61/.63 .59/.62 .60/.63 .63/.67 .62/.67 .61/.66 .64/.68 .62/.67 '\n",
      " '.39/.36 .30/.28 .53/.58 .57/.61 .38/.40 .42/.46',\n",
      " 'SICK 2014 Test + Train .61/.70 .62/.70 .62/.71 .64/.71 .63/.71 .63/.72 '\n",
      " '.62/.71 .63/.72 .45/.44 .36/.35 .61/.63 .61/.70 .47/.49 .52/.56',\n",
      " 'Average .65/.68 .65/.67 .66/.68 .65/.68 .65/.68 .66/.69 .67/.71 .66/.70 '\n",
      " '.54/.62 .51/.59 .58/.66 .62/.70 .49/.55 .49/.59',\n",
      " 'Table 7: Unsupervised Evaluation: Comparison of the performance of different '\n",
      " 'Sent2Vec models with semisupervised/supervised models on Spearman/Pearson '\n",
      " 'correlation measures. An underline indicates the best performance for the '\n",
      " 'dataset and Sent2Vec model performances are bold if they perform as well or '\n",
      " 'better than all other non-Sent2Vec models, including those presented in '\n",
      " 'Table 2.',\n",
      " 'D. Dataset Description Sentence Length Average Standard Deviation',\n",
      " 'News 17.23 8.66',\n",
      " 'Forum 10.12 3.30',\n",
      " 'STS 2014 WordNet Twitter 8.85 11.64 3.10 5.28',\n",
      " 'Images 10.17 2.77',\n",
      " 'Headlines 7.82 2.21',\n",
      " 'SICK 2014 Test + Train 9.67 3.75',\n",
      " 'Wikipedia Dataset 25.25 12.56',\n",
      " 'Twitter Dataset 16.31 7.22',\n",
      " 'Table 8: Average sentence lengths for the datasets used in the comparison.',\n",
      " 'Book Corpus Dataset 13.32 8.94']\n"
     ]
    }
   ],
   "source": [
    "# Merge lines into parahgraphs\n",
    "parahgraphs = []\n",
    "p = []\n",
    "for line in lines:\n",
    "    if len(line) == 0:\n",
    "        parahgraphs.append(' '.join(p))\n",
    "        p = []\n",
    "    else:\n",
    "        p.append(line)\n",
    "pprint(parahgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abstract The recent tremendous success of unsupervised word embeddings in a '\n",
      " 'multitude of applications raises the obvious question if similar methods '\n",
      " 'could be derived to improve embeddings (i.e. semantic representations) of '\n",
      " 'word sequences as well. We present a simple but efficient unsupervised '\n",
      " 'objective to train distributed representations of sentences. Our method '\n",
      " 'outperforms the state-of-the-art unsupervised models on most benchmark '\n",
      " 'tasks, highlighting the robustness of the produced general-purpose sentence '\n",
      " 'embeddings.',\n",
      " '1. Introduction Improving unsupervised learning is of key importance for '\n",
      " 'advancing machine learning methods, as to unlock access to almost unlimited '\n",
      " 'amounts of data to be used as training resources. The majority of recent '\n",
      " 'success stories of deep learning does not fall into this category but '\n",
      " 'instead relied on supervised training (in particular in the vision domain). '\n",
      " 'A very notable exception comes from the text and natural language processing '\n",
      " 'domain, in the form of semantic word embeddings trained unsupervised '\n",
      " '(Mikolov et al., 2013b;a; Pennington et al., 2014). Within only a few years '\n",
      " 'from their invention, such word representations  which are based on a simple '\n",
      " 'matrix factorization model as we formalize below  are now routinely trained '\n",
      " 'on very large amounts of raw text data, and have become ubiquitous building '\n",
      " 'blocks of a majority of current state-of-the-art NLP applications.',\n",
      " 'learning for NLP leads towards increasingly powerful and complex models, '\n",
      " 'such as recurrent neural networks (RNNs), LSTMs, attention models and even '\n",
      " 'Neural Turing Machine architectures. While extremely strong in '\n",
      " 'expressiveness, the increased model complexity makes such models much slower '\n",
      " 'to train on larger datasets. On the other end of the spectrum, simpler '\n",
      " 'shallow models such as matrix factorizations (or bilinear models) can '\n",
      " 'benefit from training on much larger sets of data, which can be a key '\n",
      " 'advantage, especially in the unsupervised setting. Surprisingly, for '\n",
      " 'constructing sentence embeddings, naively using averaged word vectors was '\n",
      " 'recently shown to outperform LSTMs (see (Wieting et al., 2016a) for plain '\n",
      " 'averaging, and (Arora et al., 2017) for weighted averaging). This example '\n",
      " 'shows potential in exploiting the tradeoff between model complexity and '\n",
      " 'ability to process huge amounts of text using scalable algorithms, towards '\n",
      " 'the simpler side. In view of this trade-off, our work here further advances '\n",
      " 'unsupervised learning of sentence embeddings. Our proposed model can be seen '\n",
      " 'as an extension of the CBOW (Mikolov et al., 2013b;a) training objective to '\n",
      " 'train sentence instead of word embeddings. We demonstrate that the empirical '\n",
      " 'performance of our resulting general-purpose sentence embeddings very '\n",
      " 'significantly exceeds the state of the art, while keeping the model '\n",
      " 'simplicity as well as training and inference complexity exactly as low as in '\n",
      " 'averaging methods (Wieting et al., 2016a; Arora et al., 2017), thereby also '\n",
      " 'putting the title of (Arora et al., 2017) in perspective. Contributions. The '\n",
      " 'main contributions in this work can be summarized as follows:1',\n",
      " 'While very useful semantic representations are available for words, it '\n",
      " 'remains challenging to produce and learn such semantic embeddings for longer '\n",
      " 'pieces of text, such as sentences, paragraphs or entire documents. Even more '\n",
      " 'so, it remains a key goal to learn such general-purpose representations in '\n",
      " 'an unsupervised way.',\n",
      " ' Model. We propose Sent2Vec, a simple unsupervised model allowing to compose '\n",
      " 'sentence embeddings using the word vectors along with n-gram embeddings, '\n",
      " 'simultaneously training composition and the embedding vectors themselves.',\n",
      " 'Currently, two contrary research trends have emerged in text understanding: '\n",
      " 'On one hand, a strong trend in deep-',\n",
      " ' Scalability. The computational complexity of our embeddings is only O(1) '\n",
      " 'vector operations per word processed, both during training and inference of '\n",
      " 'the sen-',\n",
      " 'Equal contribution 1 Iprova SA, Switzerland 2 Computer and Communication '\n",
      " 'Sciences, EPFL, Switzerland. Correspondence to: Martin Jaggi '\n",
      " '<martin.jaggi@epfl.ch>.',\n",
      " 'tence embeddings. This strongly contrasts all neural network based '\n",
      " 'approaches, and allows our model to learn from extremely large datasets, '\n",
      " 'which is a crucial advantage in the unsupervised setting.  Performance. Our '\n",
      " 'method shows significant performance improvements compared to the current '\n",
      " 'stateof-the-art unsupervised and even semi-supervised models. The resulting '\n",
      " 'general-purpose embeddings show strong robustness when transferred to a wide '\n",
      " 'range of prediction benchmarks.',\n",
      " 'Formally, we learn source v w and target uw embeddings for each word w in '\n",
      " 'the vocabulary, with embedding dimension h and k = |V| as in (1). The '\n",
      " 'sentence embedding is defined as the average of the source word embeddings '\n",
      " 'of its constituent words, as in (2). We augment this model furthermore by '\n",
      " 'also learning source embeddings for not only unigrams but also n-grams '\n",
      " 'present in each sentence, and averaging the n-gram embeddings along with the '\n",
      " 'words, i.e., the sentence embedding v S for S is modeled as v S :=',\n",
      " '2. Model Our model is inspired by simple matrix factor models (bilinear '\n",
      " 'models) such as recently very successfully used in unsupervised learning of '\n",
      " 'word embeddings (Mikolov et al., 2013b;a; Pennington et al., 2014; '\n",
      " 'Bojanowski et al., 2017) as well as supervised of sentence classification '\n",
      " '(Joulin et al., 2017). More precisely, these models are formalized as an '\n",
      " 'optimization problem of the form min',\n",
      " 'where R(S) is the list of n-grams (including unigrams) present in sentence '\n",
      " 'S. In order to predict a missing word from the context, our objective models '\n",
      " 'the softmax output approximated by negative sampling following (Mikolov et '\n",
      " 'al., 2013b). For the large number of output classes |V| to be predicted, '\n",
      " 'negative sampling is known to significantly improve training efficiency, see '\n",
      " 'also (Goldberg & Levy, 2014). Given the binary logistic loss function ` : x '\n",
      " '7 log (1 + ex ) coupled with negative sampling, our unsupervised training '\n",
      " 'objective is formulated as follows:',\n",
      " 'for two parameter matrices U  Rkh and V  Rh|V| , where V denotes the '\n",
      " 'vocabulary. In all models studied, the columns of the matrix V will collect '\n",
      " 'the learned word vectors, having h dimensions. For a given sentence S, which '\n",
      " 'can be of arbitrary length, the indicator vector S  {0, 1}|V| is a binary '\n",
      " 'vector encoding S (bag of words encoding). Fixed-length context windows S '\n",
      " 'running over the corpus are used in word embedding methods as in C-BOW '\n",
      " '(Mikolov et al., 2013b;a) and GloVe (Pennington et al., 2014). Here we have '\n",
      " 'k = |V| and each cost function fS : Rk  R only depends on a single row of '\n",
      " 'its input, describing the observed target word for the given fixed-length '\n",
      " 'context S. In contrast, for sentence embeddings which are the focus of our '\n",
      " 'paper here, S will be entire sentences or documents (therefore variable '\n",
      " 'length). This property is shared with the supervised FastText classifier '\n",
      " '(Joulin et al., 2017), which however uses soft-max with k  |V| being the '\n",
      " 'number of class labels. 2.1. Proposed Unsupervised Model We propose a new '\n",
      " 'unsupervised model, Sent2Vec, for learning universal sentence embeddings. '\n",
      " 'Conceptually, the model can be interpreted as a natural extension of the '\n",
      " 'wordcontexts from C-BOW (Mikolov et al., 2013b;a) to a larger sentence '\n",
      " 'context, with the sentence words being specifically optimized towards '\n",
      " 'additive combination over the sentence, by means of the unsupervised '\n",
      " 'objective function.',\n",
      " 'where S corresponds to the current sentence and Nwt is the set of words '\n",
      " 'sampled negatively for the word wt  S. The negatives are sampled2 following '\n",
      " 'a multinomial distribution where with a probability  word',\n",
      " ' each P w ispassociated fwi , where fw is the norqn (w) := fw wi V malized '\n",
      " 'frequency of w in the corpus. To select the possible target unigrams '\n",
      " '(positives), we use subsampling as in (Joulin et al., 2017; Bojanowski et '\n",
      " 'al., 2017), each word w being discarded 1  p with probability \\\\t qp (w) '\n",
      " 'where qp (w) := min 1, t/fw + t/fw . Where t is the subsampling '\n",
      " 'hyper-parameter. Subsampling prevents very frequent words of having too much '\n",
      " 'influence in the learning as they would introduce strong biases in the '\n",
      " 'prediction task. With positives subsampling and respecting the negative '\n",
      " 'sampling distribution, the precise training objective function becomes X X',\n",
      " 'To efficiently sample negatives, a pre-processing table is constructed, '\n",
      " 'containing the words corresponding to the square root of their corpora '\n",
      " 'frequency. Then, the negatives Nwt are sampled uniformly at random from the '\n",
      " 'negatives table except the target wt itself, following (Joulin et al., 2017; '\n",
      " 'Bojanowski et al., 2017).',\n",
      " '2.2. Computational Efficiency In contrast to more complex neural network '\n",
      " 'based models, one of the core advantages of the proposed technique is the '\n",
      " 'low computational cost for both inference and training. Given a sentence S '\n",
      " 'and a trained model, computing the sentence representation v S only requires '\n",
      " '|S|  h floating point operations (or |R(S)|  h to be precise for the n-gram '\n",
      " 'case, see (2)), where h is the embedding dimension. The same holds for the '\n",
      " 'cost of training with SGD on the objective (3), per sentence seen in the '\n",
      " 'training corpus. Due to the simplicity of the model, parallel training is '\n",
      " 'straight-forward using parallelized or distributed SGD. 2.3. Comparison to '\n",
      " 'C-BOW C-BOW (Mikolov et al., 2013b;a) tries to predict a chosen target word '\n",
      " 'given its fixed-size context window, the context being defined by the '\n",
      " 'average of the vectors associated with the words at a distance less than the '\n",
      " 'window size hyperparameter ws. If our system, when restricted to unigram '\n",
      " 'features, can be seen as an extension of C-BOW where the context window '\n",
      " 'includes the entire sentence, in practice there are few important '\n",
      " 'differences as C-BOW uses important tricks to facilitate the learning of '\n",
      " 'word embeddings. C-BOW first uses frequent word subsampling on the '\n",
      " 'sentences, deciding to discard each token w with probability qp (w) or alike '\n",
      " '(small variations exist across implementations). Subsampling prevents the '\n",
      " 'generation of n-grams features, and deprives the sentence of an important '\n",
      " 'part of its syntactical features. It also shortens the distance between '\n",
      " 'subsampled words, implicitly increasing the span of the context window. A '\n",
      " 'second trick consists of using dynamic context windows: for each subsampled '\n",
      " 'word w, the size of its associated context window is sampled uniformly '\n",
      " 'between 1 and ws. Using dynamic context windows is equivalent to weighing by '\n",
      " 'the distance from the focus word w divided by the window size (Levy et al., '\n",
      " '2015). This makes the prediction task local, and go against our objective of '\n",
      " 'creating sentence embeddings as we want to learn how to compose all n-gram '\n",
      " 'features present in a sentence. In the results section, we report a '\n",
      " 'significant improvement of our method over C-BOW. 2.4. Model Training Three '\n",
      " 'different datasets have been used to train our models: the Toronto book '\n",
      " 'corpus3 , Wikipedia sentences and tweets. The Wikipedia and Toronto books '\n",
      " 'sentences have been tokenized using the Stanford NLP library (Manning et '\n",
      " 'al., 2014), while for tweets we used the NLTK tweets tokenizer (Bird et al., '\n",
      " '2009). For training, we select a sentence randomly from the dataset and then '\n",
      " 'proceed to select all the 3',\n",
      " 'possible target unigrams using subsampling. We update the weights using SGD '\n",
      " 'with a linearly decaying learning rate. Also, to prevent overfitting, for '\n",
      " 'each sentence we use dropout on its list of n-grams R(S) \\\\\\\\ {U (S)}, where '\n",
      " 'U (S) is the set of all unigrams contained in sentence S. After empirically '\n",
      " 'trying multiple dropout schemes, we find that dropping K n-grams (n > 1) for '\n",
      " 'each sentence is giving superior results compared to dropping each token '\n",
      " 'with some fixed probability. This dropout mechanism would negatively impact '\n",
      " 'shorter sentences. The regularization can be pushed further by applying L1 '\n",
      " 'regularization to the word vectors. Encouraging sparsity in the embedding '\n",
      " 'vectors is particularly beneficial for high dimension h. The additional soft '\n",
      " 'thresholding in every SGD step adds negligible computational cost. See also '\n",
      " 'Appendix B. We train two models on each dataset, one with unigrams only and '\n",
      " 'one with unigrams and bigrams. All training parameters for the models are '\n",
      " 'provided in Table 5 in the supplementary material. Our C++ implementation '\n",
      " 'builds upon the FastText library (Joulin et al., 2017; Bojanowski et al., '\n",
      " '2017). We will make our code and pre-trained models available open-source.',\n",
      " '3. Related Work We discuss existing models which have been proposed to '\n",
      " 'construct sentence embeddings. While there is a large body of works in this '\n",
      " 'direction  several among these using e.g. labelled datasets of paraphrase '\n",
      " 'pairs to obtain sentence embeddings in a supervised manner (Wieting et al., '\n",
      " '2016b;a)  we here focus on unsupervised, task-independent models. While some '\n",
      " 'methods require ordered raw text i.e., a coherent corpus where the next '\n",
      " 'sentence is a logical continuation of the previous sentence, others rely '\n",
      " 'only on raw text i.e., an unordered collection of sentences. Finally we also '\n",
      " 'discuss alternative models built from structured data sources. 3.1. '\n",
      " 'Unsupervised Models Independent of Sentence Ordering The ParagraphVector '\n",
      " 'DBOW model (Le & Mikolov, 2014) is a log-linear model which is trained to '\n",
      " 'learn sentence as well as word embeddings and then use a softmax '\n",
      " 'distribution to predict words contained in the sentence given the sentence '\n",
      " 'vector representation. They also propose a different model ParagraphVector '\n",
      " 'DM where they use n-grams of consecutive words along with the sentence '\n",
      " 'vector representation to predict the next word. (Hill et al., 2016a) propose '\n",
      " 'a Sequential (Denoising) Autoencoder, S(D)AE. This model first introduces '\n",
      " 'noise in the input data: Firstly each word is deleted with probability p0 , '\n",
      " 'then for each non-overlapping bigram, words',\n",
      " 'are swapped with probability px . The model then uses an LSTM-based '\n",
      " 'architecture to retrieve the original sentence from the corrupted version. '\n",
      " 'The model can then be used to encode new sentences into vector '\n",
      " 'representations. In the case of p0 = px = 0, the model simply becomes a '\n",
      " 'Sequential Autoencoder. (Hill et al., 2016a) also propose a variant (S(D)AE '\n",
      " '+ embs.) in which the words are represented by fixed pre-trained word vector '\n",
      " 'embeddings. (Arora et al., 2017) propose a model in which sentences are '\n",
      " 'represented as a weighted average of fixed (pre-trained) word vectors, '\n",
      " 'followed by post-processing step of subtracting the principal component. '\n",
      " 'Using the generative model of (Arora et al., 2016), words are generated '\n",
      " 'conditioned on a sentence discourse vector cs : P r[w | cs ] = fw + (1  )',\n",
      " 'where Zcs := and cs := c0 + (1  )cs and ,  are scalars. c0 is the common '\n",
      " 'discourse vector, representing a shared component among all discourses, '\n",
      " 'mainly related to syntax. It allows the model to better generate syntactical '\n",
      " 'features. The fw term is here to enable the model to generate some frequent '\n",
      " 'words even if their matching with the discourse vector cs is low. Therefore, '\n",
      " 'this model tries to generate sentences as a mixture of three type of words: '\n",
      " 'words matching the sentence discourse vector cs , syntactical words matching '\n",
      " 'c0 , and words with high fw . (Arora et al., 2017) demonstrated thatPfor '\n",
      " 'this model, the MLE of cs can be approximated by wS fwa+a v w , where a is a '\n",
      " 'scalar. The sentence discourse vector can hence be obtained by subtracting '\n",
      " 'c0 estimated by the first principal component of cs s on a set of sentences. '\n",
      " 'In other words, the sentence embeddings are obtained by a weighted average '\n",
      " 'of the word vectors stripping away the syntax by subtracting the common '\n",
      " 'discourse vector and down-weighting frequent tokens. They generate sentence '\n",
      " 'embeddings from diverse pre-trained word embeddings among which are '\n",
      " 'unsupervised word embeddings such as GloVe (Pennington et al., 2014) as well '\n",
      " 'as supervised word embeddings such as paragram-SL999 (PSL) (Wieting et al., '\n",
      " '2015) trained on the Paraphrase Database (Ganitkevitch et al., 2013). In a '\n",
      " 'very different line of work, C-PHRASE (Pham et al., 2015) relies on '\n",
      " 'additional information from the syntactic parse tree of each sentence, which '\n",
      " 'is incorporated into the C-BOW training objective. (Huang & Anandkumar, '\n",
      " '2016) show that single layer CNNs can be modeled using a tensor '\n",
      " 'decomposition approach. While building on an unsupervised objective, the '\n",
      " 'employed dictionary learning step for obtaining phrase templates is '\n",
      " 'task-specific (for each use-case), not resulting in general-purpose '\n",
      " 'embeddings.',\n",
      " '3.2. Unsupervised Models Depending on Sentence Ordering The SkipThought '\n",
      " 'model (Kiros et al., 2015) combines sentence level models with recurrent '\n",
      " 'neural networks. Given a sentence Si from an ordered corpus, the model is '\n",
      " 'trained to predict Si1 and Si+1 . FastSent (Hill et al., 2016a) is a '\n",
      " 'sentence-level log-linear bag-of-words model. Like SkipThought, it uses '\n",
      " 'adjacent sentences as the prediction target and is trained in an '\n",
      " 'unsupervised fashion. Using word sequences allows the model to improve over '\n",
      " 'the earlier work of paragraph2vec (Le & Mikolov, 2014). (Hill et al., 2016a) '\n",
      " 'augment FastSent further by training it to predict the constituent words of '\n",
      " 'the sentence as well. This model is named FastSent + AE in our comparisons. '\n",
      " 'Compared to our approach, Siamese C-BOW (Kenter et al., 2016) shares the '\n",
      " 'idea of learning to average word embeddings over a sentence. However, it '\n",
      " 'relies on a Siamese neural network architecture to predict surrounding '\n",
      " 'sentences, contrasting our simpler unsupervised objective. Note that on the '\n",
      " 'character sequence level instead of word sequences, FastText (Bojanowski et '\n",
      " 'al., 2017) uses the same conceptual model to obtain better word embeddings. '\n",
      " 'This is most similar to our proposed model, with two key differences: '\n",
      " 'Firstly, we predict from source word sequences to target words, as opposed '\n",
      " 'to character sequences to target words, and secondly, our model is averaging '\n",
      " 'the source embeddings instead of summing them. 3.3. Models requiring '\n",
      " 'structured data DictRep (Hill et al., 2016b) is trained to map dictionary '\n",
      " 'definitions of the words to the pre-trained word embeddings of these words. '\n",
      " 'They use two different architectures, namely BOW and RNN (LSTM) with the '\n",
      " 'choice of learning the input word embeddings or using them pre-trained. A '\n",
      " 'similar architecture is used by the CaptionRep variant, but here the task is '\n",
      " 'the mapping of given image captions to a pre-trained vector representation '\n",
      " 'of these images.',\n",
      " '4. Evaluation Tasks We use a standard set of supervised as well as '\n",
      " 'unsupervised benchmark tasks from the literature to evaluate our trained '\n",
      " 'models, following (Hill et al., 2016a). The breadth of tasks allows to '\n",
      " 'fairly measure generalization to a wide area of different domains, testing '\n",
      " 'the general-purpose quality (universality) of all competing sentence '\n",
      " 'embeddings. For downstream supervised evaluations, sentence embeddings are '\n",
      " 'combined with logistic regression to predict target labels. In the '\n",
      " 'unsupervised evaluation for sentence similarity, correlation of the cosine '\n",
      " 'similarity between two em-',\n",
      " 'beddings is compared to human annotators. Downstream Supervised Evaluation. '\n",
      " 'Sentence embeddings are evaluated for various supervised classification '\n",
      " 'tasks as follows. We evaluate paraphrase identification (MSRP) (Dolan et '\n",
      " 'al., 2004), classification of movie review sentiment (MR) (Pang & Lee, '\n",
      " '2005), product reviews (CR) (Hu & Liu, 2004), subjectivity classification '\n",
      " '(SUBJ)(Pang & Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and '\n",
      " 'question type classification (TREC) (Voorhees, 2002). To classify, we use '\n",
      " 'the code provided by (Kiros et al., 2015) in the same manner as in (Hill et '\n",
      " 'al., 2016a). For the MSRP dataset, containing pairs of sentences (S1 , S2 ) '\n",
      " 'with associated paraphrase label, we generate feature vectors by '\n",
      " 'concatenating their Sent2Vec representations |v S1  v S2 | with the '\n",
      " 'component-wise product v S1  v S2 . The predefined training split is used to '\n",
      " 'tune the L2 penalty parameter using cross-validation and the accuracy and F1 '\n",
      " 'scores are computed on the test set. For the remaining 5 datasets, Sent2Vec '\n",
      " 'embeddings are inferred from input sentences and directly fed to a logistic '\n",
      " 'regression classifier. Accuracy scores are obtained using 10-fold '\n",
      " 'cross-validation for the MR, CR, SUBJ and MPQA datasets. For those datasets '\n",
      " 'nested cross-validation is used to tune the L2 penalty. For the TREC '\n",
      " 'dataset, as for the MRSP dataset, the L2 penalty is tuned on the predefined '\n",
      " 'train split using 10-fold cross-validation, and the accuracy is computed on '\n",
      " 'the test set. Unsupervised Similarity Evaluation. We perform unsupervised '\n",
      " 'evaluation of the the learnt sentence embeddings using the sentence cosine '\n",
      " 'similarity, on the STS 2014 (Agirre et al., 2014) and SICK 2014 (Marelli et '\n",
      " 'al., 2014) datasets. These similarity scores are compared to the '\n",
      " 'gold-standard human judgements using Pearsons r (Pearson, 1895) and '\n",
      " 'Spearmans  (Spearman, 1904) correlation scores. The SICK dataset consists of '\n",
      " 'about 10,000 sentence pairs along with relatedness scores of the pairs. The '\n",
      " 'STS 2014 dataset contains 3,770 pairs, divided into six different categories '\n",
      " 'on the basis of origin of sentences/phrases namely Twitter, headlines, news, '\n",
      " 'forum, WordNet and images. See (Agirre et al., 2014) for more precise '\n",
      " 'information on how the pairs have been created.',\n",
      " '5. Results and Discussion In Tables 1 and 2, we compare our results with '\n",
      " 'those obtained by (Hill et al., 2016a) on different models. Along with the '\n",
      " 'models discussed in Section 3, this also includes the sentence embedding '\n",
      " 'baselines obtained by simple averaging of word embeddings over the sentence, '\n",
      " 'in both the C-BOW and skip-gram variants. TF-IDF BOW is a representation '\n",
      " 'consisting of the counts of the 200,000 most common feature-words, weighed '\n",
      " 'by their TF-IDF frequencies. To ensure coherence, we only include '\n",
      " 'unsupervised mod-',\n",
      " 'els in the main paper. Performance of supervised and semisupervised models '\n",
      " 'on these evaluations can be observed in Tables 6 and 7 in the supplementary '\n",
      " 'material. Downstream Supervised Evaluation Results. On running supervised '\n",
      " 'evaluations and observing the results in Table 1, we find that on an average '\n",
      " 'our models are second only to SkipThought vectors. Also, both our models '\n",
      " 'achieve state of the art results on the CR task. We also observe that on '\n",
      " 'half of the supervised tasks, our unigrams + bigram model is the the best '\n",
      " 'model after SkipThought. Our models are weaker on the MSRP task (which '\n",
      " 'consists of the identification of labelled paraphrases) compared to '\n",
      " 'stateof-the-art methods. However, we observe that the models which perform '\n",
      " 'extremely well on this task end up faring very poorly on the other tasks, '\n",
      " 'indicating a lack of generalizability. On rest of the tasks, our models '\n",
      " 'perform extremely well. The SkipThought model is able to outperform our '\n",
      " 'models on most of the tasks as it is trained to predict the previous and '\n",
      " 'next sentences and a lot of tasks are able to make use of this contextual '\n",
      " 'information missing in our Sent2Vec models. For example, the TREC task is a '\n",
      " 'poor measure of how one predicts the content of the sentence (the question) '\n",
      " 'but a good measure of how the next sentence in the sequence (the answer) is '\n",
      " 'predicted. Unsupervised Similarity Evaluation Results. In Table 2, we see '\n",
      " 'that our Sent2Vec models are state-of-the-art on the majority of tasks when '\n",
      " 'comparing to all the unsupervised models trained on the Toronto corpus, and '\n",
      " 'clearly achieve the best averaged performance. Our Sent2Vec models also on '\n",
      " 'average outperform or are at par with the C-PHRASE model, despite '\n",
      " 'significantly lagging behind on the STS 2014 WordNet and News subtasks. This '\n",
      " 'observation can be attributed to the fact that a big chunk of the data that '\n",
      " 'the C-PHRASE model is trained on comes from English Wikipedia, helping it to '\n",
      " 'perform well on datasets involving definition and news items. Also, C-PHRASE '\n",
      " 'uses data three times the size of the Toronto book corpus. Interestingly, '\n",
      " 'our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table '\n",
      " '3, despite the fact that we use no parse tree information. In the official '\n",
      " 'results of the more recent edition of the STS 2017 benchmark (Cer et al., '\n",
      " '2017), our model also significantly outperforms C-PHRASE, and delivers the '\n",
      " 'best unsupervised baseline method. Macro Average. To summarize our '\n",
      " 'contributions on both supervised and unsupervised tasks, in Table 3 we '\n",
      " 'present the results in terms of the macro average over the averages 4 For '\n",
      " 'the Siamese C-BOW model trained on the Toronto corpus, supervised evaluation '\n",
      " 'as well as similarity evaluation results on the SICK 2014 dataset are '\n",
      " 'unavailable.',\n",
      " 'Model SAE SAE + embs. SDAE SDAE + embs. ParagraphVec DBOW ParagraphVec DM '\n",
      " 'Skipgram C-BOW Unigram TFIDF Sent2Vec uni. Sent2Vec uni. + bi. SkipThought '\n",
      " 'FastSent FastSent+AE C-PHRASE',\n",
      " 'MSRP (Acc / F1) 74.3 / 81.7 70.6 / 77.9 76.4 / 83.4 73.7 / 80.7 72.9 / 81.1 '\n",
      " '73.6 / 81.9 69.3 / 77.2 67.6 / 76.1 73.6 / 81.7 72.2 / 80.3 72.5 / 80.8 73.0 '\n",
      " '/ 82.0 72.2 / 80.3 71.2 / 79.1 72.2 / 79.6',\n",
      " 'Table 1: Comparison of the performance of different models on different '\n",
      " 'supervised evaluation tasks. An underline indicates the best performance for '\n",
      " 'the dataset. Top 3 performances in each data category are shown in bold. The '\n",
      " 'average is calculated as the average of accuracy for each category (For '\n",
      " 'MSRP, we take the average of two entries.) Model SAE SAE + embs. SDAE SDAE + '\n",
      " 'embs. ParagraphVec DBOW ParagraphVec DM Skipgram C-BOW Unigram TF-IDF '\n",
      " 'Sent2Vec uni. Sent2Vec uni. + bi. SkipThought FastSent FastSent+AE Siamese '\n",
      " 'C-BOW4 C-PHRASE',\n",
      " 'News .17/.16 .52/.54 .07/.04 .51/.54 .31/.34 .42/.46 .56/.59 .57/.61 .48/.48 '\n",
      " '.62/.67 .62/.67 .44/.45 .58/.59 .56/.59 .58/.59 .69/.71',\n",
      " 'Forum .12/.12 .22/.23 .11/.13 .29/.29 .32/.32 .33/.34 .42/.42 .43/.44 '\n",
      " '.40/.38 .49/.49 .51/.51 .14/.15 .41/.36 .41/.40 .42/.41 .43/.41',\n",
      " 'STS 2014 WordNet Twitter .30/.23 .28/.22 .60/.55 .60/.60 .33/.24 .44/.42 '\n",
      " '.56/.50 .57/.58 .53/.50 .43/.46 .51/.48 .54/.57 .73/.70 .71/.74 .72/.69 '\n",
      " '.71/.75 .60/.59 .63/.65 .75/.72 .70/.75 .71/.68 .70/.75 .39/.34 .42/.43 '\n",
      " '.74/.70 .63/.66 .69/.64 .70/.74 .66/.61 .71/.73 .76/.73 .60/.65',\n",
      " 'Images .49/.46 .64/.64 .44/.38 .59/.59 .46/.44 .32/.30 .65/.67 .71/.73 '\n",
      " '.72/.74 .78/.82 .75/.79 .55/.60 .74/.78 .63/.65 .65/.65 .75/.79',\n",
      " 'Headlines .13/.11 .41/.41 .36/.36 .43/.44 .39/.41 .46/.47 .55/.58 .55/.59 '\n",
      " '.49/.49 .61/.63 .59/.62 .43/.44 .57/.59 .58/.60 .63/.64 .60/.65',\n",
      " 'SICK 2014 Test + Train .32/.31 .47/.49 .46/.46 .46/.46 .42/.46 .44/.40 '\n",
      " '.60/.69 .60/.69 .52/.58 .61/.70 .62/.70 .57/.60 .61/.72 .60/.65',\n",
      " 'Average .26/.23 .50/.49 .31/.29 .49/.49 .41/.42 .43/.43 .60/.63 .60/.65 '\n",
      " '.55/.56 .65/.68 .65/.67 .42/.43 .61/.63 .60/.61',\n",
      " 'Table 2: Unsupervised Evaluation Tasks: Comparison of the performance of '\n",
      " 'different models on Spearman/Pearson correlation measures. An underline '\n",
      " 'indicates the best performance for the dataset. Top 3 performances in each '\n",
      " 'data category are shown in bold. The average is calculated as the average of '\n",
      " 'entries for each correlation measure. of both supervised and unsupervised '\n",
      " 'tasks along with the training times of the models5 . For unsupervised tasks, '\n",
      " 'averages are taken over both Spearman and Pearson scores. The comparison '\n",
      " 'includes the best performing unsupervised and semi-supervised methods '\n",
      " 'described in Section 3. For models trained on the Toronto books dataset, we '\n",
      " 'report a 3.8 % points improvement over the state of the art. Considering all '\n",
      " 'supervised, semi-supervised methods and all datasets compared in (Hill et '\n",
      " 'al., 2016a), we report a 2.2 % points improvement. We also see a noticeable '\n",
      " 'improvement in accuracy as we use larger datasets like twitter and Wikipedia '\n",
      " 'dump. We can also see that the Sent2Vec models are also faster to train when '\n",
      " 'compared to methods like SkipThought and DictRep owing to the SGD step '\n",
      " 'allowing a high degree of parallelizability. We can clearly see Sent2Vec '\n",
      " 'outperforming other unsupervised and even semi-supervised methods. This can '\n",
      " 'be at5',\n",
      " 'tributed to the superior generalizability of our model across supervised and '\n",
      " 'unsupervised tasks. Comparison with Arora et al. (2017). In Table 4, we '\n",
      " 'report an experimental comparison to the model of Arora et al. (2017), which '\n",
      " 'is particularly tailored to sentence similarity tasks. In the table, the '\n",
      " 'suffix W indicates that their down-weighting scheme has been used, while the '\n",
      " 'suffix R indicates the removal of the first principal component. They report '\n",
      " 'values of a  [104 , 103 ] as giving the best results and used a = 103 for '\n",
      " 'all their experiments. Their down-weighting scheme hints us to reduce the '\n",
      " 'importance of syntactical features. To do so, we use a simple blacklist '\n",
      " 'containing the 25 most frequent tokens in the Twitter corpus and discard '\n",
      " 'them before averaging. Results are also reported in Table 4. We observe that '\n",
      " 'our results are competitive with the embeddings of Arora et al. (2017) for '\n",
      " 'purely unsupervised methods. We confirm their empirical finding that '\n",
      " 'reducing the influence of the syntax helps performance on semantic sim-',\n",
      " 'unsupervised unsupervised unsupervised unsupervised unsupervised '\n",
      " 'unsupervised semi-supervised unsupervised unsupervised unsupervised '\n",
      " 'unsupervised',\n",
      " 'twitter (19.7B words) twitter (19.7B words) Wikipedia (1.7B words) Wikipedia '\n",
      " '(1.7B words) Toronto books (0.9B words) Toronto books (0.9B words) '\n",
      " 'structured dictionary dataset 2.8B words + parse info. Toronto books (0.9B '\n",
      " 'words) Toronto books (0.9B words) Toronto books (0.9B words)',\n",
      " 'Sent2Vec uni. + bi. Sent2Vec uni. Sent2Vec uni. + bi. Sent2Vec uni. Sent2Vec '\n",
      " 'books uni. Sent2Vec books uni. + bi. DictRep BOW + emb C-PHRASE C-BOW '\n",
      " 'FastSent SkipThought',\n",
      " 'Table 3: Best unsupervised and semi-supervised methods ranked by macro '\n",
      " 'average along with their training times. ** indicates trained on GPU. * '\n",
      " 'indicates trained on a single node using 30 cores. Training times for '\n",
      " 'non-Sent2Vec models are due to (Hill et al., 2016a) Dataset STS 2014 SICK '\n",
      " '2014',\n",
      " 'Table 4: Comparison of the performance of the unsupervised and '\n",
      " 'semi-supervised sentence embeddings by (Arora et al., 2017) with our models, '\n",
      " 'in terms of Pearsons correlation. to unsupervised evaluations but gives a '\n",
      " 'significant boost-up in accuracy on supervised tasks.',\n",
      " 'Figure 1: Left figure: the profile of the word vector L2 norms as a function '\n",
      " 'of log(fw ) for each vocabulary word w, as learnt by our unigram model '\n",
      " 'trained on Toronto books. Right figure: down-weighting scheme proposed by a '\n",
      " '. Arora et al. (2017): weight(w) = a+f w',\n",
      " 'ilarity tasks, and we show that applying a simple blacklist already yields a '\n",
      " 'noticeable amelioration. It is important to note that the scores obtained '\n",
      " 'from supervised task-specific PSL embeddings trained for the purpose of '\n",
      " 'semantic similarity outperform our method on both SICK and average STS 2014, '\n",
      " 'which is expected as our model is trained purely unsupervised. The effect of '\n",
      " 'datasets and n-grams. Despite being trained on three very different '\n",
      " 'datasets, all of our models generalize well to sometimes very specific '\n",
      " 'domains. Models trained on Toronto Corpus are the state-of-the art on the '\n",
      " 'STS 2014 images dataset even beating the supervised CaptionRep model trained '\n",
      " 'on images. We also see that addition of bigrams to our models doesnt help '\n",
      " 'much when it comes',\n",
      " 'On learning the importance and the direction of the word vectors. Our model  '\n",
      " 'by learning how to generate and compose word vectors  has to learn both the '\n",
      " 'direction of the word embeddings as well as their norm. Considering the '\n",
      " 'norms of the used word vectors as by our averaging over the sentence, we '\n",
      " 'observe an interesting distribution of the importance of each word. In '\n",
      " 'Figure 1 we show the profile of the L2 -norm as a function of log(fw ) for '\n",
      " 'each w  V, and compare it to the static down-weighting mechanism of Arora et '\n",
      " 'al. (2017). We can observe that our model is learning to down-weight '\n",
      " 'frequent tokens by itself. It is also down-weighting rare tokens and the '\n",
      " 'norm profile seems to roughly follow Luhns hypothesis (Luhn, 1958), a well '\n",
      " 'known information retrieval paradigm, stating that mid-rank terms are the '\n",
      " 'most significant to discriminate content. Modifying the objective function '\n",
      " 'would change the weighting scheme learnt. From a more semantic oriented '\n",
      " 'objective, it should be possible to learn to attribute lower norms for very '\n",
      " 'frequent terms, to more specifically fit sentence similarity tasks.',\n",
      " '6. Conclusion In this paper, we introduced a novel unsupervised and '\n",
      " 'computationally efficient method to train and infer sentence embeddings. On '\n",
      " 'supervised evaluations, our method, on an average, achieves better '\n",
      " 'performance than all other unsupervised competitors except the SkipThought '\n",
      " 'vectors. However, SkipThought vectors show an extremely poor performance on '\n",
      " 'sentence similarity tasks while our model',\n",
      " 'is state-of-the-art for these evaluations on average. Future work could '\n",
      " 'focus on augmenting the model to exploit data with ordered sentences. '\n",
      " 'Furthermore, we would like to further investigate the models ability as '\n",
      " 'giving pre-trained embeddings to enable downstream transfer learning tasks. '\n",
      " 'Acknowledgments. We are indebted to Piotr Bojanowski and Armand Joulin for '\n",
      " 'helpful discussions.',\n",
      " 'References Agirre, Eneko, Banea, Carmen, Cardie, Claire, Cer, Daniel, Diab, '\n",
      " 'Mona, Gonzalez-Agirre, Aitor, Guo, Weiwei, Mihalcea, Rada, Rigau, German, '\n",
      " 'and Wiebe, Janyce. Semeval-2014 task 10: Multilingual semantic textual '\n",
      " 'similarity. In Proceedings of the 8th international workshop on semantic '\n",
      " 'evaluation (SemEval 2014), pp. 8191. Association for Computational '\n",
      " 'Linguistics Dublin, Ireland, 2014. Arora, Sanjeev, Li, Yuanzhi, Liang, '\n",
      " 'Yingyu, Ma, Tengyu, and Risteski, Andrej. A Latent Variable Model Approach '\n",
      " 'to PMIbased Word Embeddings. In Transactions of the Association for '\n",
      " 'Computational Linguistics, pp. 385399, July 2016. Arora, Sanjeev, Liang, '\n",
      " 'Yingyu, and Ma, Tengyu. A simple but tough-to-beat baseline for sentence '\n",
      " 'embeddings. In International Conference on Learning Representations (ICLR), '\n",
      " '2017. Bird, Steven, Klein, Ewan, and Loper, Edward. Natural language '\n",
      " 'processing with Python: analyzing text with the natural language toolkit.  '\n",
      " 'OReilly Media, Inc., 2009. Bojanowski, Piotr, Grave, Edouard, Joulin, '\n",
      " 'Armand, and Mikolov, Tomas. Enriching Word Vectors with Subword Information. '\n",
      " 'Transactions of the Association for Computational Linguistics, 5:135146, '\n",
      " '2017. Cer, Daniel, Diab, Mona, Agirre, Eneko, Lopez-Gazpio, Inigo, and '\n",
      " 'Specia, Lucia. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual '\n",
      " 'and Cross-lingual Focused Evaluation. In SemEval-2017 - Proceedings of the '\n",
      " '11th International Workshop on Semantic Evaluations, pp. 114, Vancouver, '\n",
      " 'Canada, August 2017. Association for Computational Linguistics. Dolan, Bill, '\n",
      " 'Quirk, Chris, and Brockett, Chris. Unsupervised construction of large '\n",
      " 'paraphrase corpora: Exploiting massively parallel news sources. In '\n",
      " 'Proceedings of the 20th international conference on Computational '\n",
      " 'Linguistics, pp. 350. Association for Computational Linguistics, 2004. '\n",
      " 'Ganitkevitch, Juri, Van Durme, Benjamin, and Callison-Burch, Chris. Ppdb: '\n",
      " 'The paraphrase database. In HLT-NAACL, pp. 758764, 2013. Goldberg, Yoav and '\n",
      " 'Levy, Omer. word2vec Explained: deriving Mikolov et al.s negative-sampling '\n",
      " 'word-embedding method. arXiv, February 2014.',\n",
      " 'Hu, Minqing and Liu, Bing. Mining and summarizing customer reviews. In '\n",
      " 'Proceedings of the tenth ACM SIGKDD international conference on Knowledge '\n",
      " 'discovery and data mining, pp. 168177. ACM, 2004. Huang, Furong and '\n",
      " 'Anandkumar, Animashree. Unsupervised Learning of Word-Sequence '\n",
      " 'Representations from Scratch via Convolutional Tensor Decomposition. arXiv, '\n",
      " '2016. Joulin, Armand, Grave, Edouard, Bojanowski, Piotr, and Mikolov, Tomas. '\n",
      " 'Bag of Tricks for Efficient Text Classification. In Proceedings of the 15th '\n",
      " 'Conference of the European Chapter of the Association for Computational '\n",
      " 'Linguistics, Short Papers, pp. 427431, Valencia, Spain, 2017. Kenter, Tom, '\n",
      " 'Borisov, Alexey, and de Rijke, Maarten. Siamese CBOW: Optimizing Word '\n",
      " 'Embeddings for Sentence Representations. In ACL - Proceedings of the 54th '\n",
      " 'Annual Meeting of the Association for Computational Linguistics, pp. 941951, '\n",
      " 'Berlin, Germany, 2016. Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan R, '\n",
      " 'Zemel, Richard, Urtasun, Raquel, Torralba, Antonio, and Fidler, Sanja. '\n",
      " 'Skip-Thought Vectors. In NIPS 2015 - Advances in Neural Information '\n",
      " 'Processing Systems 28, pp. 32943302, 2015. Le, Quoc V and Mikolov, Tomas. '\n",
      " 'Distributed Representations of Sentences and Documents. In ICML 2014 - '\n",
      " 'Proceedings of the 31st International Conference on Machine Learning, volume '\n",
      " '14, pp. 11881196, 2014. Levy, Omer, Goldberg, Yoav, and Dagan, Ido. '\n",
      " 'Improving distributional similarity with lessons learned from word '\n",
      " 'embeddings. Transactions of the Association for Computational Linguistics, '\n",
      " '3:211225, 2015. Luhn, Hans Peter. The automatic creation of literature '\n",
      " 'abstracts. IBM Journal of research and development, 2(2):159 165, 1958. '\n",
      " 'Manning, Christopher D, Surdeanu, Mihai, Bauer, John, Finkel, Jenny Rose, '\n",
      " 'Bethard, Steven, and McClosky, David. The stanford corenlp natural language '\n",
      " 'processing toolkit. In ACL (System Demonstrations), pp. 5560, 2014. Marelli, '\n",
      " 'Marco, Menini, Stefano, Baroni, Marco, Bentivogli, Luisa, Bernardi, '\n",
      " 'Raffaella, and Zamparelli, Roberto. A sick cure for the evaluation of '\n",
      " 'compositional distributional semantic models. In LREC, pp. 216223, 2014. '\n",
      " 'Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efficient '\n",
      " 'estimation of word representations in vector space. arXiv preprint '\n",
      " 'arXiv:1301.3781, 2013a. Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, '\n",
      " 'Greg S, and Dean, Jeff. Distributed Representations of Words and Phrases and '\n",
      " 'their Compositionality. In NIPS - Advances in Neural Information Processing '\n",
      " 'Systems 26, pp. 31113119, 2013b.',\n",
      " 'Hill, Felix, Cho, Kyunghyun, and Korhonen, Anna. Learning Distributed '\n",
      " 'Representations of Sentences from Unlabelled Data. In Proceedings of '\n",
      " 'NAACL-HLT, February 2016a.',\n",
      " 'Pang, Bo and Lee, Lillian. A sentimental education: Sentiment analysis using '\n",
      " 'subjectivity summarization based on minimum cuts. In Proceedings of the 42nd '\n",
      " 'annual meeting on Association for Computational Linguistics, pp. 271. '\n",
      " 'Association for Computational Linguistics, 2004.',\n",
      " 'Hill, Felix, Cho, KyungHyun, Korhonen, Anna, and Bengio, Yoshua. Learning to '\n",
      " 'understand phrases by embedding the dictionary. TACL, 4:1730, 2016b. URL '\n",
      " 'https://tacl2013.cs.columbia.edu/ojs/ index.php/tacl/article/view/711.',\n",
      " 'Pang, Bo and Lee, Lillian. Seeing stars: Exploiting class relationships for '\n",
      " 'sentiment categorization with respect to rating scales. In Proceedings of '\n",
      " 'the 43rd annual meeting on association for computational linguistics, pp. '\n",
      " '115124. Association for Computational Linguistics, 2005.',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features Pearson, Karl. Note on regression and inheritance in the case of '\n",
      " 'two parents. Proceedings of the Royal Society of London, 58: 240242, 1895. '\n",
      " 'Pennington, Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: '\n",
      " 'Global vectors for word representation. In EMNLP, volume 14, pp. 15321543, '\n",
      " '2014. Pham, NT, Kruszewski, G, Lazaridou, A, and Baroni, M. Jointly '\n",
      " 'optimizing word representations for lexical and sentential tasks with the '\n",
      " 'c-phrase model. ACL/IJCNLP, 2015. Rockafellar, R Tyrrell. Monotone operators '\n",
      " 'and the proximal point algorithm. SIAM journal on control and optimization, '\n",
      " '14(5):877898, 1976. Spearman, Charles. The proof and measurement of '\n",
      " 'association between two things. The American journal of psychology, 15 '\n",
      " '(1):72101, 1904. Voorhees, Ellen M. Overview of the trec 2001 question '\n",
      " 'answering track. In NIST special publication, pp. 4251, 2002. Wiebe, Janyce, '\n",
      " 'Wilson, Theresa, and Cardie, Claire. Annotating expressions of opinions and '\n",
      " 'emotions in language. Language resources and evaluation, 39(2):165210, 2005. '\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, Livescu, Karen, and Roth, Dan. '\n",
      " 'From paraphrase database to compositional paraphrase model and back. In TACL '\n",
      " '- Transactions of the Association for Computational Linguistics, 2015. '\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen. Towards '\n",
      " 'universal paraphrastic sentence embeddings. In International Conference on '\n",
      " 'Learning Representations (ICLR), 2016a. Wieting, John, Bansal, Mohit, '\n",
      " 'Gimpel, Kevin, and Livescu, Karen. Charagram: Embedding Words and Sentences '\n",
      " 'via Character n-grams. In EMNLP - Proceedings of the 2016 Conference on '\n",
      " 'Empirical Methods in Natural Language Processing, pp. 15041515, Stroudsburg, '\n",
      " 'PA, USA, 2016b. Association for Computational Linguistics.',\n",
      " 'Supplementary Material A. Parameters for training models Model Book corpus '\n",
      " 'Sent2Vec unigrams Book corpus Sent2Vec unigrams + bigrams Wiki Sent2Vec '\n",
      " 'unigrams Wiki Sent2Vec unigrams + bigrams Twitter Sent2Vec unigrams Twitter '\n",
      " 'Sent2Vec unigrams + bigrams',\n",
      " 'B. L1 regularization of models Optionally, our model can be additionally '\n",
      " 'improved by adding an L1 regularizer term in the objective function, leading '\n",
      " 'to slightly better generalization performance. Additionally, encouraging '\n",
      " 'sparsity in the embedding vectors is beneficial for memory reasons, allowing '\n",
      " 'higher embedding dimensions h. We propose to apply L1 regularization '\n",
      " 'individually to each word (and n-gram) vector (both source and target '\n",
      " 'vectors). Formally, the training objective function (3) then becomes',\n",
      " 'where  is the regularization parameter. Now, in order to minimize a function '\n",
      " 'of the form f (z) + g(z) where g(z) is not differentiable over the domain, '\n",
      " 'we can use the basic proximal-gradient scheme. In this iterative method, '\n",
      " 'after doing a gradient descent step on f (z) with learning rate , we update '\n",
      " 'z as zn+1 = prox,g (zn+ 12 )',\n",
      " '1 where prox,g (x) = arg miny {g(y) + 2 ky  xk22 } is called the proximal '\n",
      " 'function(Rockafellar, 1976) of g with  being the proximal parameter and zn+ '\n",
      " '21 is the value of z after a gradient (or SGD) step on zn .',\n",
      " 'In our case, g(z) = kzk1 and the corresponding proximal operator is given by '\n",
      " 'prox,g (x) = sign(x)  max(|xn |  , 0)',\n",
      " 'where  corresponds to element-wise product. Similar to the proximal-gradient '\n",
      " 'scheme, in our case we can optionally use the thresholding operator on the '\n",
      " 'updated word  lr 0 and n-gram vectors after an SGD step. The soft '\n",
      " 'thresholding parameter used for this update is |R(S\\\\\\\\{w and   lr0 for t '\n",
      " '})| 0 the source and target vectors respectively where lr is the current '\n",
      " 'learning rate,  is the L1 regularization parameter and S is the sentence on '\n",
      " 'which SGD is being run.',\n",
      " 'We observe that L1 regularization using the proximal step gives our models a '\n",
      " 'small boost in performance. Also, applying the thresholding operator takes '\n",
      " 'only |R(S\\\\\\\\{wt })|h floating point operations for the updating the word '\n",
      " 'vectors corresponding to the sentence and (|N | + 1)  h for updating the '\n",
      " 'target as well as the negative word vectors, where |N | is the number of '\n",
      " 'negatives sampled and h is the embedding dimension. Thus, performing L1 '\n",
      " 'regularization using soft-thresholding operator comes with a small '\n",
      " 'computational overhead. We set  to be 0.0005 for both the Wikipedia and the '\n",
      " 'Toronto Book Corpus unigrams + bigrams models.',\n",
      " 'C. Performance comparison with Sent2Vec models trained on different corpora '\n",
      " 'Data Unordered Sentences: (Toronto Books) Unordered sentences: Wikipedia (69 '\n",
      " 'million sentences; 1.7 B words) Unordered sentences: Twitter (1.2 billion '\n",
      " 'sentences; 19.7 B words)',\n",
      " 'Model Sent2Vec uni. Sent2Vec uni. + bi. Sent2Vec uni. + bi.L1reg Sent2Vec '\n",
      " 'uni. Sent2Vec uni. + bi. Sent2Vec uni. + bi.L1reg Sent2Vec uni. Sent2Vec '\n",
      " 'uni. + bi. CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW+embs '\n",
      " 'DictRep RNN DictRep RNN+embs.',\n",
      " 'MSRP (Acc / F1) 72.2 / 80.3 72.5 / 80.8 71.6 / 80.1 71.8 / 80.2 72.4 / 80.8 '\n",
      " '73.6 / 81.5 71.5 / 80.0 72.4 / 80.6 73.6 / 81.9 72.6 / 81.1 73.7 / 81.6 68.4 '\n",
      " '/ 76.8 73.2 / 81.6 66.8 / 76.0',\n",
      " 'Table 6: Comparison of the performance of different Sent2Vec models with '\n",
      " 'different semi-supervised/supervised models on different downstream '\n",
      " 'supervised evaluation tasks. An underline indicates the best performance for '\n",
      " 'the dataset and Sent2Vec model performances are bold if they perform as well '\n",
      " 'or better than all other non-Sent2Vec models, including those presented in '\n",
      " 'Table 1.',\n",
      " 'Model Sent2Vec book corpus uni. Sent2Vec book corpus uni. + bi. Sent2Vec '\n",
      " 'book corpus uni. + bi. L1 reg Sent2Vec wiki uni. Sent2Vec wiki uni. + bi. '\n",
      " 'Sent2Vec wiki uni. + bi. L1 reg Sent2Vec twitter uni. Sent2Vec twitter uni. '\n",
      " '+ bi. CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW + embs. DictRep '\n",
      " 'RNN DictRep RNN + embs.',\n",
      " 'News .62/.67 .62/.67 .62/.68 .66/.71 .68/.74 .69/.75 .67/.74 .68/.74 .26/.26 '\n",
      " '.05/.05 .62/.67 .65/.72 .40/.46 .51/.60',\n",
      " 'Forum .49/.49 .51/.51 .51/.52 .47/.47 .50/.50 .52/.52 .52/.53 .54/.54 '\n",
      " '.29/.22 .13/.09 .42/.40 .49/.47 .26/.23 .29/.27',\n",
      " 'STS 2014 WordNet Twitter .75/.72. .70/.75 .71/.68 .70/.75 .72/.70 .69/.75 '\n",
      " '.70/.68 .68/.72 .66/.64 .67/.72 .72/.69 .67/.72 .75/.72 .72/.78 .72/.69 '\n",
      " '.70/.77 .50/.35 .37/.31 .40/.33 .36/.30 .81/.81 .62/.66 .85/.86 .67/.72 '\n",
      " '.78/.78 .42/.42 .80/.81 .44/.47',\n",
      " 'Images .78/.82 .75/.79 .76/.81 .76/.79 .75/.79 .76/.80 .77/.81 .76/.79 '\n",
      " '.78/.81 .76/.82 .66/.68 .71/.74 .56/.56 .65/.70',\n",
      " 'Headlines .61/.63 .59/.62 .60/.63 .63/.67 .62/.67 .61/.66 .64/.68 .62/.67 '\n",
      " '.39/.36 .30/.28 .53/.58 .57/.61 .38/.40 .42/.46',\n",
      " 'SICK 2014 Test + Train .61/.70 .62/.70 .62/.71 .64/.71 .63/.71 .63/.72 '\n",
      " '.62/.71 .63/.72 .45/.44 .36/.35 .61/.63 .61/.70 .47/.49 .52/.56',\n",
      " 'Average .65/.68 .65/.67 .66/.68 .65/.68 .65/.68 .66/.69 .67/.71 .66/.70 '\n",
      " '.54/.62 .51/.59 .58/.66 .62/.70 .49/.55 .49/.59',\n",
      " 'Table 7: Unsupervised Evaluation: Comparison of the performance of different '\n",
      " 'Sent2Vec models with semisupervised/supervised models on Spearman/Pearson '\n",
      " 'correlation measures. An underline indicates the best performance for the '\n",
      " 'dataset and Sent2Vec model performances are bold if they perform as well or '\n",
      " 'better than all other non-Sent2Vec models, including those presented in '\n",
      " 'Table 2.']\n"
     ]
    }
   ],
   "source": [
    "# Remove too short parahgraphs (maybe author email, table description etc.)\n",
    "parahgraphs = [p for p in parahgraphs if len(p) > 100]\n",
    "pprint(parahgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abstract The recent tremendous success of unsupervised word embeddings in a '\n",
      " 'multitude of applications raises the obvious question if similar methods '\n",
      " 'could be derived to improve embeddings (i.e. semantic representations) of '\n",
      " 'word sequences as well. We present a simple but efficient unsupervised '\n",
      " 'objective to train distributed representations of sentences. Our method '\n",
      " 'outperforms the state-of-the-art unsupervised models on most benchmark '\n",
      " 'tasks, highlighting the robustness of the produced general-purpose sentence '\n",
      " 'embeddings.',\n",
      " '1. Introduction Improving unsupervised learning is of key importance for '\n",
      " 'advancing machine learning methods, as to unlock access to almost unlimited '\n",
      " 'amounts of data to be used as training resources. The majority of recent '\n",
      " 'success stories of deep learning does not fall into this category but '\n",
      " 'instead relied on supervised training (in particular in the vision domain). '\n",
      " 'A very notable exception comes from the text and natural language processing '\n",
      " 'domain, in the form of semantic word embeddings trained unsupervised '\n",
      " '(Mikolov et al., 2013b;a; Pennington et al., 2014). Within only a few years '\n",
      " 'from their invention, such word representations  which are based on a simple '\n",
      " 'matrix factorization model as we formalize below  are now routinely trained '\n",
      " 'on very large amounts of raw text data, and have become ubiquitous building '\n",
      " 'blocks of a majority of current state-of-the-art NLP applications.',\n",
      " 'learning for NLP leads towards increasingly powerful and complex models, '\n",
      " 'such as recurrent neural networks (RNNs), LSTMs, attention models and even '\n",
      " 'Neural Turing Machine architectures. While extremely strong in '\n",
      " 'expressiveness, the increased model complexity makes such models much slower '\n",
      " 'to train on larger datasets. On the other end of the spectrum, simpler '\n",
      " 'shallow models such as matrix factorizations (or bilinear models) can '\n",
      " 'benefit from training on much larger sets of data, which can be a key '\n",
      " 'advantage, especially in the unsupervised setting. Surprisingly, for '\n",
      " 'constructing sentence embeddings, naively using averaged word vectors was '\n",
      " 'recently shown to outperform LSTMs (see (Wieting et al., 2016a) for plain '\n",
      " 'averaging, and (Arora et al., 2017) for weighted averaging). This example '\n",
      " 'shows potential in exploiting the tradeoff between model complexity and '\n",
      " 'ability to process huge amounts of text using scalable algorithms, towards '\n",
      " 'the simpler side. In view of this trade-off, our work here further advances '\n",
      " 'unsupervised learning of sentence embeddings. Our proposed model can be seen '\n",
      " 'as an extension of the CBOW (Mikolov et al., 2013b;a) training objective to '\n",
      " 'train sentence instead of word embeddings. We demonstrate that the empirical '\n",
      " 'performance of our resulting general-purpose sentence embeddings very '\n",
      " 'significantly exceeds the state of the art, while keeping the model '\n",
      " 'simplicity as well as training and inference complexity exactly as low as in '\n",
      " 'averaging methods (Wieting et al., 2016a; Arora et al., 2017), thereby also '\n",
      " 'putting the title of (Arora et al., 2017) in perspective. Contributions. The '\n",
      " 'main contributions in this work can be summarized as follows:1',\n",
      " 'While very useful semantic representations are available for words, it '\n",
      " 'remains challenging to produce and learn such semantic embeddings for longer '\n",
      " 'pieces of text, such as sentences, paragraphs or entire documents. Even more '\n",
      " 'so, it remains a key goal to learn such general-purpose representations in '\n",
      " 'an unsupervised way.',\n",
      " ' Model. We propose Sent2Vec, a simple unsupervised model allowing to compose '\n",
      " 'sentence embeddings using the word vectors along with n-gram embeddings, '\n",
      " 'simultaneously training composition and the embedding vectors themselves.',\n",
      " 'Currently, two contrary research trends have emerged in text understanding: '\n",
      " 'On one hand, a strong trend in deep-',\n",
      " ' Scalability. The computational complexity of our embeddings is only O(1) '\n",
      " 'vector operations per word processed, both during training and inference of '\n",
      " 'the sen-',\n",
      " 'Equal contribution 1 Iprova SA, Switzerland 2 Computer and Communication '\n",
      " 'Sciences, EPFL, Switzerland. Correspondence to: Martin Jaggi '\n",
      " '<martin.jaggi@epfl.ch>.',\n",
      " 'tence embeddings. This strongly contrasts all neural network based '\n",
      " 'approaches, and allows our model to learn from extremely large datasets, '\n",
      " 'which is a crucial advantage in the unsupervised setting.  Performance. Our '\n",
      " 'method shows significant performance improvements compared to the current '\n",
      " 'stateof-the-art unsupervised and even semi-supervised models. The resulting '\n",
      " 'general-purpose embeddings show strong robustness when transferred to a wide '\n",
      " 'range of prediction benchmarks.',\n",
      " 'Formally, we learn source v w and target uw embeddings for each word w in '\n",
      " 'the vocabulary, with embedding dimension h and k = |V| as in (1). The '\n",
      " 'sentence embedding is defined as the average of the source word embeddings '\n",
      " 'of its constituent words, as in (2). We augment this model furthermore by '\n",
      " 'also learning source embeddings for not only unigrams but also n-grams '\n",
      " 'present in each sentence, and averaging the n-gram embeddings along with the '\n",
      " 'words, i.e., the sentence embedding v S for S is modeled as v S :=',\n",
      " '2. Model Our model is inspired by simple matrix factor models (bilinear '\n",
      " 'models) such as recently very successfully used in unsupervised learning of '\n",
      " 'word embeddings (Mikolov et al., 2013b;a; Pennington et al., 2014; '\n",
      " 'Bojanowski et al., 2017) as well as supervised of sentence classification '\n",
      " '(Joulin et al., 2017). More precisely, these models are formalized as an '\n",
      " 'optimization problem of the form min',\n",
      " 'where R(S) is the list of n-grams (including unigrams) present in sentence '\n",
      " 'S. In order to predict a missing word from the context, our objective models '\n",
      " 'the softmax output approximated by negative sampling following (Mikolov et '\n",
      " 'al., 2013b). For the large number of output classes |V| to be predicted, '\n",
      " 'negative sampling is known to significantly improve training efficiency, see '\n",
      " 'also (Goldberg & Levy, 2014). Given the binary logistic loss function ` : x '\n",
      " '7 log (1 + ex ) coupled with negative sampling, our unsupervised training '\n",
      " 'objective is formulated as follows:',\n",
      " 'for two parameter matrices U  Rkh and V  Rh|V| , where V denotes the '\n",
      " 'vocabulary. In all models studied, the columns of the matrix V will collect '\n",
      " 'the learned word vectors, having h dimensions. For a given sentence S, which '\n",
      " 'can be of arbitrary length, the indicator vector S  {0, 1}|V| is a binary '\n",
      " 'vector encoding S (bag of words encoding). Fixed-length context windows S '\n",
      " 'running over the corpus are used in word embedding methods as in C-BOW '\n",
      " '(Mikolov et al., 2013b;a) and GloVe (Pennington et al., 2014). Here we have '\n",
      " 'k = |V| and each cost function fS : Rk  R only depends on a single row of '\n",
      " 'its input, describing the observed target word for the given fixed-length '\n",
      " 'context S. In contrast, for sentence embeddings which are the focus of our '\n",
      " 'paper here, S will be entire sentences or documents (therefore variable '\n",
      " 'length). This property is shared with the supervised FastText classifier '\n",
      " '(Joulin et al., 2017), which however uses soft-max with k  |V| being the '\n",
      " 'number of class labels. 2.1. Proposed Unsupervised Model We propose a new '\n",
      " 'unsupervised model, Sent2Vec, for learning universal sentence embeddings. '\n",
      " 'Conceptually, the model can be interpreted as a natural extension of the '\n",
      " 'wordcontexts from C-BOW (Mikolov et al., 2013b;a) to a larger sentence '\n",
      " 'context, with the sentence words being specifically optimized towards '\n",
      " 'additive combination over the sentence, by means of the unsupervised '\n",
      " 'objective function.',\n",
      " 'where S corresponds to the current sentence and Nwt is the set of words '\n",
      " 'sampled negatively for the word wt  S. The negatives are sampled2 following '\n",
      " 'a multinomial distribution where with a probability  word',\n",
      " ' each P w ispassociated fwi , where fw is the norqn (w) := fw wi V malized '\n",
      " 'frequency of w in the corpus. To select the possible target unigrams '\n",
      " '(positives), we use subsampling as in (Joulin et al., 2017; Bojanowski et '\n",
      " 'al., 2017), each word w being discarded 1  p with probability \\\\t qp (w) '\n",
      " 'where qp (w) := min 1, t/fw + t/fw . Where t is the subsampling '\n",
      " 'hyper-parameter. Subsampling prevents very frequent words of having too much '\n",
      " 'influence in the learning as they would introduce strong biases in the '\n",
      " 'prediction task. With positives subsampling and respecting the negative '\n",
      " 'sampling distribution, the precise training objective function becomes X X',\n",
      " 'To efficiently sample negatives, a pre-processing table is constructed, '\n",
      " 'containing the words corresponding to the square root of their corpora '\n",
      " 'frequency. Then, the negatives Nwt are sampled uniformly at random from the '\n",
      " 'negatives table except the target wt itself, following (Joulin et al., 2017; '\n",
      " 'Bojanowski et al., 2017).',\n",
      " '2.2. Computational Efficiency In contrast to more complex neural network '\n",
      " 'based models, one of the core advantages of the proposed technique is the '\n",
      " 'low computational cost for both inference and training. Given a sentence S '\n",
      " 'and a trained model, computing the sentence representation v S only requires '\n",
      " '|S|  h floating point operations (or |R(S)|  h to be precise for the n-gram '\n",
      " 'case, see (2)), where h is the embedding dimension. The same holds for the '\n",
      " 'cost of training with SGD on the objective (3), per sentence seen in the '\n",
      " 'training corpus. Due to the simplicity of the model, parallel training is '\n",
      " 'straight-forward using parallelized or distributed SGD. 2.3. Comparison to '\n",
      " 'C-BOW C-BOW (Mikolov et al., 2013b;a) tries to predict a chosen target word '\n",
      " 'given its fixed-size context window, the context being defined by the '\n",
      " 'average of the vectors associated with the words at a distance less than the '\n",
      " 'window size hyperparameter ws. If our system, when restricted to unigram '\n",
      " 'features, can be seen as an extension of C-BOW where the context window '\n",
      " 'includes the entire sentence, in practice there are few important '\n",
      " 'differences as C-BOW uses important tricks to facilitate the learning of '\n",
      " 'word embeddings. C-BOW first uses frequent word subsampling on the '\n",
      " 'sentences, deciding to discard each token w with probability qp (w) or alike '\n",
      " '(small variations exist across implementations). Subsampling prevents the '\n",
      " 'generation of n-grams features, and deprives the sentence of an important '\n",
      " 'part of its syntactical features. It also shortens the distance between '\n",
      " 'subsampled words, implicitly increasing the span of the context window. A '\n",
      " 'second trick consists of using dynamic context windows: for each subsampled '\n",
      " 'word w, the size of its associated context window is sampled uniformly '\n",
      " 'between 1 and ws. Using dynamic context windows is equivalent to weighing by '\n",
      " 'the distance from the focus word w divided by the window size (Levy et al., '\n",
      " '2015). This makes the prediction task local, and go against our objective of '\n",
      " 'creating sentence embeddings as we want to learn how to compose all n-gram '\n",
      " 'features present in a sentence. In the results section, we report a '\n",
      " 'significant improvement of our method over C-BOW. 2.4. Model Training Three '\n",
      " 'different datasets have been used to train our models: the Toronto book '\n",
      " 'corpus3 , Wikipedia sentences and tweets. The Wikipedia and Toronto books '\n",
      " 'sentences have been tokenized using the Stanford NLP library (Manning et '\n",
      " 'al., 2014), while for tweets we used the NLTK tweets tokenizer (Bird et al., '\n",
      " '2009). For training, we select a sentence randomly from the dataset and then '\n",
      " 'proceed to select all the 3',\n",
      " 'possible target unigrams using subsampling. We update the weights using SGD '\n",
      " 'with a linearly decaying learning rate. Also, to prevent overfitting, for '\n",
      " 'each sentence we use dropout on its list of n-grams R(S) \\\\\\\\ {U (S)}, where '\n",
      " 'U (S) is the set of all unigrams contained in sentence S. After empirically '\n",
      " 'trying multiple dropout schemes, we find that dropping K n-grams (n > 1) for '\n",
      " 'each sentence is giving superior results compared to dropping each token '\n",
      " 'with some fixed probability. This dropout mechanism would negatively impact '\n",
      " 'shorter sentences. The regularization can be pushed further by applying L1 '\n",
      " 'regularization to the word vectors. Encouraging sparsity in the embedding '\n",
      " 'vectors is particularly beneficial for high dimension h. The additional soft '\n",
      " 'thresholding in every SGD step adds negligible computational cost. See also '\n",
      " 'Appendix B. We train two models on each dataset, one with unigrams only and '\n",
      " 'one with unigrams and bigrams. All training parameters for the models are '\n",
      " 'provided in Table 5 in the supplementary material. Our C++ implementation '\n",
      " 'builds upon the FastText library (Joulin et al., 2017; Bojanowski et al., '\n",
      " '2017). We will make our code and pre-trained models available open-source.',\n",
      " '3. Related Work We discuss existing models which have been proposed to '\n",
      " 'construct sentence embeddings. While there is a large body of works in this '\n",
      " 'direction  several among these using e.g. labelled datasets of paraphrase '\n",
      " 'pairs to obtain sentence embeddings in a supervised manner (Wieting et al., '\n",
      " '2016b;a)  we here focus on unsupervised, task-independent models. While some '\n",
      " 'methods require ordered raw text i.e., a coherent corpus where the next '\n",
      " 'sentence is a logical continuation of the previous sentence, others rely '\n",
      " 'only on raw text i.e., an unordered collection of sentences. Finally we also '\n",
      " 'discuss alternative models built from structured data sources. 3.1. '\n",
      " 'Unsupervised Models Independent of Sentence Ordering The ParagraphVector '\n",
      " 'DBOW model (Le & Mikolov, 2014) is a log-linear model which is trained to '\n",
      " 'learn sentence as well as word embeddings and then use a softmax '\n",
      " 'distribution to predict words contained in the sentence given the sentence '\n",
      " 'vector representation. They also propose a different model ParagraphVector '\n",
      " 'DM where they use n-grams of consecutive words along with the sentence '\n",
      " 'vector representation to predict the next word. (Hill et al., 2016a) propose '\n",
      " 'a Sequential (Denoising) Autoencoder, S(D)AE. This model first introduces '\n",
      " 'noise in the input data: Firstly each word is deleted with probability p0 , '\n",
      " 'then for each non-overlapping bigram, words',\n",
      " 'are swapped with probability px . The model then uses an LSTM-based '\n",
      " 'architecture to retrieve the original sentence from the corrupted version. '\n",
      " 'The model can then be used to encode new sentences into vector '\n",
      " 'representations. In the case of p0 = px = 0, the model simply becomes a '\n",
      " 'Sequential Autoencoder. (Hill et al., 2016a) also propose a variant (S(D)AE '\n",
      " '+ embs.) in which the words are represented by fixed pre-trained word vector '\n",
      " 'embeddings. (Arora et al., 2017) propose a model in which sentences are '\n",
      " 'represented as a weighted average of fixed (pre-trained) word vectors, '\n",
      " 'followed by post-processing step of subtracting the principal component. '\n",
      " 'Using the generative model of (Arora et al., 2016), words are generated '\n",
      " 'conditioned on a sentence discourse vector cs : P r[w | cs ] = fw + (1  )',\n",
      " 'where Zcs := and cs := c0 + (1  )cs and ,  are scalars. c0 is the common '\n",
      " 'discourse vector, representing a shared component among all discourses, '\n",
      " 'mainly related to syntax. It allows the model to better generate syntactical '\n",
      " 'features. The fw term is here to enable the model to generate some frequent '\n",
      " 'words even if their matching with the discourse vector cs is low. Therefore, '\n",
      " 'this model tries to generate sentences as a mixture of three type of words: '\n",
      " 'words matching the sentence discourse vector cs , syntactical words matching '\n",
      " 'c0 , and words with high fw . (Arora et al., 2017) demonstrated thatPfor '\n",
      " 'this model, the MLE of cs can be approximated by wS fwa+a v w , where a is a '\n",
      " 'scalar. The sentence discourse vector can hence be obtained by subtracting '\n",
      " 'c0 estimated by the first principal component of cs s on a set of sentences. '\n",
      " 'In other words, the sentence embeddings are obtained by a weighted average '\n",
      " 'of the word vectors stripping away the syntax by subtracting the common '\n",
      " 'discourse vector and down-weighting frequent tokens. They generate sentence '\n",
      " 'embeddings from diverse pre-trained word embeddings among which are '\n",
      " 'unsupervised word embeddings such as GloVe (Pennington et al., 2014) as well '\n",
      " 'as supervised word embeddings such as paragram-SL999 (PSL) (Wieting et al., '\n",
      " '2015) trained on the Paraphrase Database (Ganitkevitch et al., 2013). In a '\n",
      " 'very different line of work, C-PHRASE (Pham et al., 2015) relies on '\n",
      " 'additional information from the syntactic parse tree of each sentence, which '\n",
      " 'is incorporated into the C-BOW training objective. (Huang & Anandkumar, '\n",
      " '2016) show that single layer CNNs can be modeled using a tensor '\n",
      " 'decomposition approach. While building on an unsupervised objective, the '\n",
      " 'employed dictionary learning step for obtaining phrase templates is '\n",
      " 'task-specific (for each use-case), not resulting in general-purpose '\n",
      " 'embeddings.',\n",
      " '3.2. Unsupervised Models Depending on Sentence Ordering The SkipThought '\n",
      " 'model (Kiros et al., 2015) combines sentence level models with recurrent '\n",
      " 'neural networks. Given a sentence Si from an ordered corpus, the model is '\n",
      " 'trained to predict Si1 and Si+1 . FastSent (Hill et al., 2016a) is a '\n",
      " 'sentence-level log-linear bag-of-words model. Like SkipThought, it uses '\n",
      " 'adjacent sentences as the prediction target and is trained in an '\n",
      " 'unsupervised fashion. Using word sequences allows the model to improve over '\n",
      " 'the earlier work of paragraph2vec (Le & Mikolov, 2014). (Hill et al., 2016a) '\n",
      " 'augment FastSent further by training it to predict the constituent words of '\n",
      " 'the sentence as well. This model is named FastSent + AE in our comparisons. '\n",
      " 'Compared to our approach, Siamese C-BOW (Kenter et al., 2016) shares the '\n",
      " 'idea of learning to average word embeddings over a sentence. However, it '\n",
      " 'relies on a Siamese neural network architecture to predict surrounding '\n",
      " 'sentences, contrasting our simpler unsupervised objective. Note that on the '\n",
      " 'character sequence level instead of word sequences, FastText (Bojanowski et '\n",
      " 'al., 2017) uses the same conceptual model to obtain better word embeddings. '\n",
      " 'This is most similar to our proposed model, with two key differences: '\n",
      " 'Firstly, we predict from source word sequences to target words, as opposed '\n",
      " 'to character sequences to target words, and secondly, our model is averaging '\n",
      " 'the source embeddings instead of summing them. 3.3. Models requiring '\n",
      " 'structured data DictRep (Hill et al., 2016b) is trained to map dictionary '\n",
      " 'definitions of the words to the pre-trained word embeddings of these words. '\n",
      " 'They use two different architectures, namely BOW and RNN (LSTM) with the '\n",
      " 'choice of learning the input word embeddings or using them pre-trained. A '\n",
      " 'similar architecture is used by the CaptionRep variant, but here the task is '\n",
      " 'the mapping of given image captions to a pre-trained vector representation '\n",
      " 'of these images.',\n",
      " '4. Evaluation Tasks We use a standard set of supervised as well as '\n",
      " 'unsupervised benchmark tasks from the literature to evaluate our trained '\n",
      " 'models, following (Hill et al., 2016a). The breadth of tasks allows to '\n",
      " 'fairly measure generalization to a wide area of different domains, testing '\n",
      " 'the general-purpose quality (universality) of all competing sentence '\n",
      " 'embeddings. For downstream supervised evaluations, sentence embeddings are '\n",
      " 'combined with logistic regression to predict target labels. In the '\n",
      " 'unsupervised evaluation for sentence similarity, correlation of the cosine '\n",
      " 'similarity between two em-',\n",
      " 'beddings is compared to human annotators. Downstream Supervised Evaluation. '\n",
      " 'Sentence embeddings are evaluated for various supervised classification '\n",
      " 'tasks as follows. We evaluate paraphrase identification (MSRP) (Dolan et '\n",
      " 'al., 2004), classification of movie review sentiment (MR) (Pang & Lee, '\n",
      " '2005), product reviews (CR) (Hu & Liu, 2004), subjectivity classification '\n",
      " '(SUBJ)(Pang & Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and '\n",
      " 'question type classification (TREC) (Voorhees, 2002). To classify, we use '\n",
      " 'the code provided by (Kiros et al., 2015) in the same manner as in (Hill et '\n",
      " 'al., 2016a). For the MSRP dataset, containing pairs of sentences (S1 , S2 ) '\n",
      " 'with associated paraphrase label, we generate feature vectors by '\n",
      " 'concatenating their Sent2Vec representations |v S1  v S2 | with the '\n",
      " 'component-wise product v S1  v S2 . The predefined training split is used to '\n",
      " 'tune the L2 penalty parameter using cross-validation and the accuracy and F1 '\n",
      " 'scores are computed on the test set. For the remaining 5 datasets, Sent2Vec '\n",
      " 'embeddings are inferred from input sentences and directly fed to a logistic '\n",
      " 'regression classifier. Accuracy scores are obtained using 10-fold '\n",
      " 'cross-validation for the MR, CR, SUBJ and MPQA datasets. For those datasets '\n",
      " 'nested cross-validation is used to tune the L2 penalty. For the TREC '\n",
      " 'dataset, as for the MRSP dataset, the L2 penalty is tuned on the predefined '\n",
      " 'train split using 10-fold cross-validation, and the accuracy is computed on '\n",
      " 'the test set. Unsupervised Similarity Evaluation. We perform unsupervised '\n",
      " 'evaluation of the the learnt sentence embeddings using the sentence cosine '\n",
      " 'similarity, on the STS 2014 (Agirre et al., 2014) and SICK 2014 (Marelli et '\n",
      " 'al., 2014) datasets. These similarity scores are compared to the '\n",
      " 'gold-standard human judgements using Pearsons r (Pearson, 1895) and '\n",
      " 'Spearmans  (Spearman, 1904) correlation scores. The SICK dataset consists of '\n",
      " 'about 10,000 sentence pairs along with relatedness scores of the pairs. The '\n",
      " 'STS 2014 dataset contains 3,770 pairs, divided into six different categories '\n",
      " 'on the basis of origin of sentences/phrases namely Twitter, headlines, news, '\n",
      " 'forum, WordNet and images. See (Agirre et al., 2014) for more precise '\n",
      " 'information on how the pairs have been created.',\n",
      " '5. Results and Discussion In Tables 1 and 2, we compare our results with '\n",
      " 'those obtained by (Hill et al., 2016a) on different models. Along with the '\n",
      " 'models discussed in Section 3, this also includes the sentence embedding '\n",
      " 'baselines obtained by simple averaging of word embeddings over the sentence, '\n",
      " 'in both the C-BOW and skip-gram variants. TF-IDF BOW is a representation '\n",
      " 'consisting of the counts of the 200,000 most common feature-words, weighed '\n",
      " 'by their TF-IDF frequencies. To ensure coherence, we only include '\n",
      " 'unsupervised mod-',\n",
      " 'els in the main paper. Performance of supervised and semisupervised models '\n",
      " 'on these evaluations can be observed in Tables 6 and 7 in the supplementary '\n",
      " 'material. Downstream Supervised Evaluation Results. On running supervised '\n",
      " 'evaluations and observing the results in Table 1, we find that on an average '\n",
      " 'our models are second only to SkipThought vectors. Also, both our models '\n",
      " 'achieve state of the art results on the CR task. We also observe that on '\n",
      " 'half of the supervised tasks, our unigrams + bigram model is the the best '\n",
      " 'model after SkipThought. Our models are weaker on the MSRP task (which '\n",
      " 'consists of the identification of labelled paraphrases) compared to '\n",
      " 'stateof-the-art methods. However, we observe that the models which perform '\n",
      " 'extremely well on this task end up faring very poorly on the other tasks, '\n",
      " 'indicating a lack of generalizability. On rest of the tasks, our models '\n",
      " 'perform extremely well. The SkipThought model is able to outperform our '\n",
      " 'models on most of the tasks as it is trained to predict the previous and '\n",
      " 'next sentences and a lot of tasks are able to make use of this contextual '\n",
      " 'information missing in our Sent2Vec models. For example, the TREC task is a '\n",
      " 'poor measure of how one predicts the content of the sentence (the question) '\n",
      " 'but a good measure of how the next sentence in the sequence (the answer) is '\n",
      " 'predicted. Unsupervised Similarity Evaluation Results. In Table 2, we see '\n",
      " 'that our Sent2Vec models are state-of-the-art on the majority of tasks when '\n",
      " 'comparing to all the unsupervised models trained on the Toronto corpus, and '\n",
      " 'clearly achieve the best averaged performance. Our Sent2Vec models also on '\n",
      " 'average outperform or are at par with the C-PHRASE model, despite '\n",
      " 'significantly lagging behind on the STS 2014 WordNet and News subtasks. This '\n",
      " 'observation can be attributed to the fact that a big chunk of the data that '\n",
      " 'the C-PHRASE model is trained on comes from English Wikipedia, helping it to '\n",
      " 'perform well on datasets involving definition and news items. Also, C-PHRASE '\n",
      " 'uses data three times the size of the Toronto book corpus. Interestingly, '\n",
      " 'our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table '\n",
      " '3, despite the fact that we use no parse tree information. In the official '\n",
      " 'results of the more recent edition of the STS 2017 benchmark (Cer et al., '\n",
      " '2017), our model also significantly outperforms C-PHRASE, and delivers the '\n",
      " 'best unsupervised baseline method. Macro Average. To summarize our '\n",
      " 'contributions on both supervised and unsupervised tasks, in Table 3 we '\n",
      " 'present the results in terms of the macro average over the averages 4 For '\n",
      " 'the Siamese C-BOW model trained on the Toronto corpus, supervised evaluation '\n",
      " 'as well as similarity evaluation results on the SICK 2014 dataset are '\n",
      " 'unavailable.',\n",
      " 'Model SAE SAE + embs. SDAE SDAE + embs. ParagraphVec DBOW ParagraphVec DM '\n",
      " 'Skipgram C-BOW Unigram TFIDF Sent2Vec uni. Sent2Vec uni. + bi. SkipThought '\n",
      " 'FastSent FastSent+AE C-PHRASE',\n",
      " 'MSRP (Acc / F1) 74.3 / 81.7 70.6 / 77.9 76.4 / 83.4 73.7 / 80.7 72.9 / 81.1 '\n",
      " '73.6 / 81.9 69.3 / 77.2 67.6 / 76.1 73.6 / 81.7 72.2 / 80.3 72.5 / 80.8 73.0 '\n",
      " '/ 82.0 72.2 / 80.3 71.2 / 79.1 72.2 / 79.6',\n",
      " 'Table 1: Comparison of the performance of different models on different '\n",
      " 'supervised evaluation tasks. An underline indicates the best performance for '\n",
      " 'the dataset. Top 3 performances in each data category are shown in bold. The '\n",
      " 'average is calculated as the average of accuracy for each category (For '\n",
      " 'MSRP, we take the average of two entries.) Model SAE SAE + embs. SDAE SDAE + '\n",
      " 'embs. ParagraphVec DBOW ParagraphVec DM Skipgram C-BOW Unigram TF-IDF '\n",
      " 'Sent2Vec uni. Sent2Vec uni. + bi. SkipThought FastSent FastSent+AE Siamese '\n",
      " 'C-BOW4 C-PHRASE',\n",
      " 'News .17/.16 .52/.54 .07/.04 .51/.54 .31/.34 .42/.46 .56/.59 .57/.61 .48/.48 '\n",
      " '.62/.67 .62/.67 .44/.45 .58/.59 .56/.59 .58/.59 .69/.71',\n",
      " 'Forum .12/.12 .22/.23 .11/.13 .29/.29 .32/.32 .33/.34 .42/.42 .43/.44 '\n",
      " '.40/.38 .49/.49 .51/.51 .14/.15 .41/.36 .41/.40 .42/.41 .43/.41',\n",
      " 'STS 2014 WordNet Twitter .30/.23 .28/.22 .60/.55 .60/.60 .33/.24 .44/.42 '\n",
      " '.56/.50 .57/.58 .53/.50 .43/.46 .51/.48 .54/.57 .73/.70 .71/.74 .72/.69 '\n",
      " '.71/.75 .60/.59 .63/.65 .75/.72 .70/.75 .71/.68 .70/.75 .39/.34 .42/.43 '\n",
      " '.74/.70 .63/.66 .69/.64 .70/.74 .66/.61 .71/.73 .76/.73 .60/.65',\n",
      " 'Images .49/.46 .64/.64 .44/.38 .59/.59 .46/.44 .32/.30 .65/.67 .71/.73 '\n",
      " '.72/.74 .78/.82 .75/.79 .55/.60 .74/.78 .63/.65 .65/.65 .75/.79',\n",
      " 'Headlines .13/.11 .41/.41 .36/.36 .43/.44 .39/.41 .46/.47 .55/.58 .55/.59 '\n",
      " '.49/.49 .61/.63 .59/.62 .43/.44 .57/.59 .58/.60 .63/.64 .60/.65',\n",
      " 'SICK 2014 Test + Train .32/.31 .47/.49 .46/.46 .46/.46 .42/.46 .44/.40 '\n",
      " '.60/.69 .60/.69 .52/.58 .61/.70 .62/.70 .57/.60 .61/.72 .60/.65',\n",
      " 'Average .26/.23 .50/.49 .31/.29 .49/.49 .41/.42 .43/.43 .60/.63 .60/.65 '\n",
      " '.55/.56 .65/.68 .65/.67 .42/.43 .61/.63 .60/.61',\n",
      " 'Table 2: Unsupervised Evaluation Tasks: Comparison of the performance of '\n",
      " 'different models on Spearman/Pearson correlation measures. An underline '\n",
      " 'indicates the best performance for the dataset. Top 3 performances in each '\n",
      " 'data category are shown in bold. The average is calculated as the average of '\n",
      " 'entries for each correlation measure. of both supervised and unsupervised '\n",
      " 'tasks along with the training times of the models5 . For unsupervised tasks, '\n",
      " 'averages are taken over both Spearman and Pearson scores. The comparison '\n",
      " 'includes the best performing unsupervised and semi-supervised methods '\n",
      " 'described in Section 3. For models trained on the Toronto books dataset, we '\n",
      " 'report a 3.8 % points improvement over the state of the art. Considering all '\n",
      " 'supervised, semi-supervised methods and all datasets compared in (Hill et '\n",
      " 'al., 2016a), we report a 2.2 % points improvement. We also see a noticeable '\n",
      " 'improvement in accuracy as we use larger datasets like twitter and Wikipedia '\n",
      " 'dump. We can also see that the Sent2Vec models are also faster to train when '\n",
      " 'compared to methods like SkipThought and DictRep owing to the SGD step '\n",
      " 'allowing a high degree of parallelizability. We can clearly see Sent2Vec '\n",
      " 'outperforming other unsupervised and even semi-supervised methods. This can '\n",
      " 'be at5',\n",
      " 'tributed to the superior generalizability of our model across supervised and '\n",
      " 'unsupervised tasks. Comparison with Arora et al. (2017). In Table 4, we '\n",
      " 'report an experimental comparison to the model of Arora et al. (2017), which '\n",
      " 'is particularly tailored to sentence similarity tasks. In the table, the '\n",
      " 'suffix W indicates that their down-weighting scheme has been used, while the '\n",
      " 'suffix R indicates the removal of the first principal component. They report '\n",
      " 'values of a  [104 , 103 ] as giving the best results and used a = 103 for '\n",
      " 'all their experiments. Their down-weighting scheme hints us to reduce the '\n",
      " 'importance of syntactical features. To do so, we use a simple blacklist '\n",
      " 'containing the 25 most frequent tokens in the Twitter corpus and discard '\n",
      " 'them before averaging. Results are also reported in Table 4. We observe that '\n",
      " 'our results are competitive with the embeddings of Arora et al. (2017) for '\n",
      " 'purely unsupervised methods. We confirm their empirical finding that '\n",
      " 'reducing the influence of the syntax helps performance on semantic sim-',\n",
      " 'unsupervised unsupervised unsupervised unsupervised unsupervised '\n",
      " 'unsupervised semi-supervised unsupervised unsupervised unsupervised '\n",
      " 'unsupervised',\n",
      " 'twitter (19.7B words) twitter (19.7B words) Wikipedia (1.7B words) Wikipedia '\n",
      " '(1.7B words) Toronto books (0.9B words) Toronto books (0.9B words) '\n",
      " 'structured dictionary dataset 2.8B words + parse info. Toronto books (0.9B '\n",
      " 'words) Toronto books (0.9B words) Toronto books (0.9B words)',\n",
      " 'Sent2Vec uni. + bi. Sent2Vec uni. Sent2Vec uni. + bi. Sent2Vec uni. Sent2Vec '\n",
      " 'books uni. Sent2Vec books uni. + bi. DictRep BOW + emb C-PHRASE C-BOW '\n",
      " 'FastSent SkipThought',\n",
      " 'Table 3: Best unsupervised and semi-supervised methods ranked by macro '\n",
      " 'average along with their training times. ** indicates trained on GPU. * '\n",
      " 'indicates trained on a single node using 30 cores. Training times for '\n",
      " 'non-Sent2Vec models are due to (Hill et al., 2016a) Dataset STS 2014 SICK '\n",
      " '2014',\n",
      " 'Table 4: Comparison of the performance of the unsupervised and '\n",
      " 'semi-supervised sentence embeddings by (Arora et al., 2017) with our models, '\n",
      " 'in terms of Pearsons correlation. to unsupervised evaluations but gives a '\n",
      " 'significant boost-up in accuracy on supervised tasks.',\n",
      " 'Figure 1: Left figure: the profile of the word vector L2 norms as a function '\n",
      " 'of log(fw ) for each vocabulary word w, as learnt by our unigram model '\n",
      " 'trained on Toronto books. Right figure: down-weighting scheme proposed by a '\n",
      " '. Arora et al. (2017): weight(w) = a+f w',\n",
      " 'ilarity tasks, and we show that applying a simple blacklist already yields a '\n",
      " 'noticeable amelioration. It is important to note that the scores obtained '\n",
      " 'from supervised task-specific PSL embeddings trained for the purpose of '\n",
      " 'semantic similarity outperform our method on both SICK and average STS 2014, '\n",
      " 'which is expected as our model is trained purely unsupervised. The effect of '\n",
      " 'datasets and n-grams. Despite being trained on three very different '\n",
      " 'datasets, all of our models generalize well to sometimes very specific '\n",
      " 'domains. Models trained on Toronto Corpus are the state-of-the art on the '\n",
      " 'STS 2014 images dataset even beating the supervised CaptionRep model trained '\n",
      " 'on images. We also see that addition of bigrams to our models doesnt help '\n",
      " 'much when it comes',\n",
      " 'On learning the importance and the direction of the word vectors. Our model  '\n",
      " 'by learning how to generate and compose word vectors  has to learn both the '\n",
      " 'direction of the word embeddings as well as their norm. Considering the '\n",
      " 'norms of the used word vectors as by our averaging over the sentence, we '\n",
      " 'observe an interesting distribution of the importance of each word. In '\n",
      " 'Figure 1 we show the profile of the L2 -norm as a function of log(fw ) for '\n",
      " 'each w  V, and compare it to the static down-weighting mechanism of Arora et '\n",
      " 'al. (2017). We can observe that our model is learning to down-weight '\n",
      " 'frequent tokens by itself. It is also down-weighting rare tokens and the '\n",
      " 'norm profile seems to roughly follow Luhns hypothesis (Luhn, 1958), a well '\n",
      " 'known information retrieval paradigm, stating that mid-rank terms are the '\n",
      " 'most significant to discriminate content. Modifying the objective function '\n",
      " 'would change the weighting scheme learnt. From a more semantic oriented '\n",
      " 'objective, it should be possible to learn to attribute lower norms for very '\n",
      " 'frequent terms, to more specifically fit sentence similarity tasks.',\n",
      " '6. Conclusion In this paper, we introduced a novel unsupervised and '\n",
      " 'computationally efficient method to train and infer sentence embeddings. On '\n",
      " 'supervised evaluations, our method, on an average, achieves better '\n",
      " 'performance than all other unsupervised competitors except the SkipThought '\n",
      " 'vectors. However, SkipThought vectors show an extremely poor performance on '\n",
      " 'sentence similarity tasks while our model',\n",
      " 'is state-of-the-art for these evaluations on average. Future work could '\n",
      " 'focus on augmenting the model to exploit data with ordered sentences. '\n",
      " 'Furthermore, we would like to further investigate the models ability as '\n",
      " 'giving pre-trained embeddings to enable downstream transfer learning tasks. '\n",
      " 'Acknowledgments. We are indebted to Piotr Bojanowski and Armand Joulin for '\n",
      " 'helpful discussions.',\n",
      " 'References Agirre, Eneko, Banea, Carmen, Cardie, Claire, Cer, Daniel, Diab, '\n",
      " 'Mona, Gonzalez-Agirre, Aitor, Guo, Weiwei, Mihalcea, Rada, Rigau, German, '\n",
      " 'and Wiebe, Janyce. Semeval-2014 task 10: Multilingual semantic textual '\n",
      " 'similarity. In Proceedings of the 8th international workshop on semantic '\n",
      " 'evaluation (SemEval 2014), pp. 8191. Association for Computational '\n",
      " 'Linguistics Dublin, Ireland, 2014. Arora, Sanjeev, Li, Yuanzhi, Liang, '\n",
      " 'Yingyu, Ma, Tengyu, and Risteski, Andrej. A Latent Variable Model Approach '\n",
      " 'to PMIbased Word Embeddings. In Transactions of the Association for '\n",
      " 'Computational Linguistics, pp. 385399, July 2016. Arora, Sanjeev, Liang, '\n",
      " 'Yingyu, and Ma, Tengyu. A simple but tough-to-beat baseline for sentence '\n",
      " 'embeddings. In International Conference on Learning Representations (ICLR), '\n",
      " '2017. Bird, Steven, Klein, Ewan, and Loper, Edward. Natural language '\n",
      " 'processing with Python: analyzing text with the natural language toolkit.  '\n",
      " 'OReilly Media, Inc., 2009. Bojanowski, Piotr, Grave, Edouard, Joulin, '\n",
      " 'Armand, and Mikolov, Tomas. Enriching Word Vectors with Subword Information. '\n",
      " 'Transactions of the Association for Computational Linguistics, 5:135146, '\n",
      " '2017. Cer, Daniel, Diab, Mona, Agirre, Eneko, Lopez-Gazpio, Inigo, and '\n",
      " 'Specia, Lucia. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual '\n",
      " 'and Cross-lingual Focused Evaluation. In SemEval-2017 - Proceedings of the '\n",
      " '11th International Workshop on Semantic Evaluations, pp. 114, Vancouver, '\n",
      " 'Canada, August 2017. Association for Computational Linguistics. Dolan, Bill, '\n",
      " 'Quirk, Chris, and Brockett, Chris. Unsupervised construction of large '\n",
      " 'paraphrase corpora: Exploiting massively parallel news sources. In '\n",
      " 'Proceedings of the 20th international conference on Computational '\n",
      " 'Linguistics, pp. 350. Association for Computational Linguistics, 2004. '\n",
      " 'Ganitkevitch, Juri, Van Durme, Benjamin, and Callison-Burch, Chris. Ppdb: '\n",
      " 'The paraphrase database. In HLT-NAACL, pp. 758764, 2013. Goldberg, Yoav and '\n",
      " 'Levy, Omer. word2vec Explained: deriving Mikolov et al.s negative-sampling '\n",
      " 'word-embedding method. arXiv, February 2014.',\n",
      " 'Hu, Minqing and Liu, Bing. Mining and summarizing customer reviews. In '\n",
      " 'Proceedings of the tenth ACM SIGKDD international conference on Knowledge '\n",
      " 'discovery and data mining, pp. 168177. ACM, 2004. Huang, Furong and '\n",
      " 'Anandkumar, Animashree. Unsupervised Learning of Word-Sequence '\n",
      " 'Representations from Scratch via Convolutional Tensor Decomposition. arXiv, '\n",
      " '2016. Joulin, Armand, Grave, Edouard, Bojanowski, Piotr, and Mikolov, Tomas. '\n",
      " 'Bag of Tricks for Efficient Text Classification. In Proceedings of the 15th '\n",
      " 'Conference of the European Chapter of the Association for Computational '\n",
      " 'Linguistics, Short Papers, pp. 427431, Valencia, Spain, 2017. Kenter, Tom, '\n",
      " 'Borisov, Alexey, and de Rijke, Maarten. Siamese CBOW: Optimizing Word '\n",
      " 'Embeddings for Sentence Representations. In ACL - Proceedings of the 54th '\n",
      " 'Annual Meeting of the Association for Computational Linguistics, pp. 941951, '\n",
      " 'Berlin, Germany, 2016. Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan R, '\n",
      " 'Zemel, Richard, Urtasun, Raquel, Torralba, Antonio, and Fidler, Sanja. '\n",
      " 'Skip-Thought Vectors. In NIPS 2015 - Advances in Neural Information '\n",
      " 'Processing Systems 28, pp. 32943302, 2015. Le, Quoc V and Mikolov, Tomas. '\n",
      " 'Distributed Representations of Sentences and Documents. In ICML 2014 - '\n",
      " 'Proceedings of the 31st International Conference on Machine Learning, volume '\n",
      " '14, pp. 11881196, 2014. Levy, Omer, Goldberg, Yoav, and Dagan, Ido. '\n",
      " 'Improving distributional similarity with lessons learned from word '\n",
      " 'embeddings. Transactions of the Association for Computational Linguistics, '\n",
      " '3:211225, 2015. Luhn, Hans Peter. The automatic creation of literature '\n",
      " 'abstracts. IBM Journal of research and development, 2(2):159 165, 1958. '\n",
      " 'Manning, Christopher D, Surdeanu, Mihai, Bauer, John, Finkel, Jenny Rose, '\n",
      " 'Bethard, Steven, and McClosky, David. The stanford corenlp natural language '\n",
      " 'processing toolkit. In ACL (System Demonstrations), pp. 5560, 2014. Marelli, '\n",
      " 'Marco, Menini, Stefano, Baroni, Marco, Bentivogli, Luisa, Bernardi, '\n",
      " 'Raffaella, and Zamparelli, Roberto. A sick cure for the evaluation of '\n",
      " 'compositional distributional semantic models. In LREC, pp. 216223, 2014. '\n",
      " 'Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efficient '\n",
      " 'estimation of word representations in vector space. arXiv preprint '\n",
      " 'arXiv:1301.3781, 2013a. Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, '\n",
      " 'Greg S, and Dean, Jeff. Distributed Representations of Words and Phrases and '\n",
      " 'their Compositionality. In NIPS - Advances in Neural Information Processing '\n",
      " 'Systems 26, pp. 31113119, 2013b.',\n",
      " 'Hill, Felix, Cho, Kyunghyun, and Korhonen, Anna. Learning Distributed '\n",
      " 'Representations of Sentences from Unlabelled Data. In Proceedings of '\n",
      " 'NAACL-HLT, February 2016a.',\n",
      " 'Pang, Bo and Lee, Lillian. A sentimental education: Sentiment analysis using '\n",
      " 'subjectivity summarization based on minimum cuts. In Proceedings of the 42nd '\n",
      " 'annual meeting on Association for Computational Linguistics, pp. 271. '\n",
      " 'Association for Computational Linguistics, 2004.',\n",
      " 'Hill, Felix, Cho, KyungHyun, Korhonen, Anna, and Bengio, Yoshua. Learning to '\n",
      " 'understand phrases by embedding the dictionary. TACL, 4:1730, 2016b. URL '\n",
      " 'https://tacl2013.cs.columbia.edu/ojs/ index.php/tacl/article/view/711.',\n",
      " 'Pang, Bo and Lee, Lillian. Seeing stars: Exploiting class relationships for '\n",
      " 'sentiment categorization with respect to rating scales. In Proceedings of '\n",
      " 'the 43rd annual meeting on association for computational linguistics, pp. '\n",
      " '115124. Association for Computational Linguistics, 2005.',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features Pearson, Karl. Note on regression and inheritance in the case of '\n",
      " 'two parents. Proceedings of the Royal Society of London, 58: 240242, 1895. '\n",
      " 'Pennington, Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: '\n",
      " 'Global vectors for word representation. In EMNLP, volume 14, pp. 15321543, '\n",
      " '2014. Pham, NT, Kruszewski, G, Lazaridou, A, and Baroni, M. Jointly '\n",
      " 'optimizing word representations for lexical and sentential tasks with the '\n",
      " 'c-phrase model. ACL/IJCNLP, 2015. Rockafellar, R Tyrrell. Monotone operators '\n",
      " 'and the proximal point algorithm. SIAM journal on control and optimization, '\n",
      " '14(5):877898, 1976. Spearman, Charles. The proof and measurement of '\n",
      " 'association between two things. The American journal of psychology, 15 '\n",
      " '(1):72101, 1904. Voorhees, Ellen M. Overview of the trec 2001 question '\n",
      " 'answering track. In NIST special publication, pp. 4251, 2002. Wiebe, Janyce, '\n",
      " 'Wilson, Theresa, and Cardie, Claire. Annotating expressions of opinions and '\n",
      " 'emotions in language. Language resources and evaluation, 39(2):165210, 2005. '\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, Livescu, Karen, and Roth, Dan. '\n",
      " 'From paraphrase database to compositional paraphrase model and back. In TACL '\n",
      " '- Transactions of the Association for Computational Linguistics, 2015. '\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen. Towards '\n",
      " 'universal paraphrastic sentence embeddings. In International Conference on '\n",
      " 'Learning Representations (ICLR), 2016a. Wieting, John, Bansal, Mohit, '\n",
      " 'Gimpel, Kevin, and Livescu, Karen. Charagram: Embedding Words and Sentences '\n",
      " 'via Character n-grams. In EMNLP - Proceedings of the 2016 Conference on '\n",
      " 'Empirical Methods in Natural Language Processing, pp. 15041515, Stroudsburg, '\n",
      " 'PA, USA, 2016b. Association for Computational Linguistics.',\n",
      " 'Supplementary Material A. Parameters for training models Model Book corpus '\n",
      " 'Sent2Vec unigrams Book corpus Sent2Vec unigrams + bigrams Wiki Sent2Vec '\n",
      " 'unigrams Wiki Sent2Vec unigrams + bigrams Twitter Sent2Vec unigrams Twitter '\n",
      " 'Sent2Vec unigrams + bigrams',\n",
      " 'B. L1 regularization of models Optionally, our model can be additionally '\n",
      " 'improved by adding an L1 regularizer term in the objective function, leading '\n",
      " 'to slightly better generalization performance. Additionally, encouraging '\n",
      " 'sparsity in the embedding vectors is beneficial for memory reasons, allowing '\n",
      " 'higher embedding dimensions h. We propose to apply L1 regularization '\n",
      " 'individually to each word (and n-gram) vector (both source and target '\n",
      " 'vectors). Formally, the training objective function (3) then becomes',\n",
      " 'where  is the regularization parameter. Now, in order to minimize a function '\n",
      " 'of the form f (z) + g(z) where g(z) is not differentiable over the domain, '\n",
      " 'we can use the basic proximal-gradient scheme. In this iterative method, '\n",
      " 'after doing a gradient descent step on f (z) with learning rate , we update '\n",
      " 'z as zn+1 = prox,g (zn+ 12 )',\n",
      " '1 where prox,g (x) = arg miny {g(y) + 2 ky  xk22 } is called the proximal '\n",
      " 'function(Rockafellar, 1976) of g with  being the proximal parameter and zn+ '\n",
      " '21 is the value of z after a gradient (or SGD) step on zn .',\n",
      " 'In our case, g(z) = kzk1 and the corresponding proximal operator is given by '\n",
      " 'prox,g (x) = sign(x)  max(|xn |  , 0)',\n",
      " 'where  corresponds to element-wise product. Similar to the proximal-gradient '\n",
      " 'scheme, in our case we can optionally use the thresholding operator on the '\n",
      " 'updated word  lr 0 and n-gram vectors after an SGD step. The soft '\n",
      " 'thresholding parameter used for this update is |R(S\\\\\\\\{w and   lr0 for t '\n",
      " '})| 0 the source and target vectors respectively where lr is the current '\n",
      " 'learning rate,  is the L1 regularization parameter and S is the sentence on '\n",
      " 'which SGD is being run.',\n",
      " 'We observe that L1 regularization using the proximal step gives our models a '\n",
      " 'small boost in performance. Also, applying the thresholding operator takes '\n",
      " 'only |R(S\\\\\\\\{wt })|h floating point operations for the updating the word '\n",
      " 'vectors corresponding to the sentence and (|N | + 1)  h for updating the '\n",
      " 'target as well as the negative word vectors, where |N | is the number of '\n",
      " 'negatives sampled and h is the embedding dimension. Thus, performing L1 '\n",
      " 'regularization using soft-thresholding operator comes with a small '\n",
      " 'computational overhead. We set  to be 0.0005 for both the Wikipedia and the '\n",
      " 'Toronto Book Corpus unigrams + bigrams models.',\n",
      " 'C. Performance comparison with Sent2Vec models trained on different corpora '\n",
      " 'Data Unordered Sentences: (Toronto Books) Unordered sentences: Wikipedia (69 '\n",
      " 'million sentences; 1.7 B words) Unordered sentences: Twitter (1.2 billion '\n",
      " 'sentences; 19.7 B words)',\n",
      " 'Model Sent2Vec uni. Sent2Vec uni. + bi. Sent2Vec uni. + bi.L1reg Sent2Vec '\n",
      " 'uni. Sent2Vec uni. + bi. Sent2Vec uni. + bi.L1reg Sent2Vec uni. Sent2Vec '\n",
      " 'uni. + bi. CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW+embs '\n",
      " 'DictRep RNN DictRep RNN+embs.',\n",
      " 'MSRP (Acc / F1) 72.2 / 80.3 72.5 / 80.8 71.6 / 80.1 71.8 / 80.2 72.4 / 80.8 '\n",
      " '73.6 / 81.5 71.5 / 80.0 72.4 / 80.6 73.6 / 81.9 72.6 / 81.1 73.7 / 81.6 68.4 '\n",
      " '/ 76.8 73.2 / 81.6 66.8 / 76.0',\n",
      " 'Table 6: Comparison of the performance of different Sent2Vec models with '\n",
      " 'different semi-supervised/supervised models on different downstream '\n",
      " 'supervised evaluation tasks. An underline indicates the best performance for '\n",
      " 'the dataset and Sent2Vec model performances are bold if they perform as well '\n",
      " 'or better than all other non-Sent2Vec models, including those presented in '\n",
      " 'Table 1.',\n",
      " 'Model Sent2Vec book corpus uni. Sent2Vec book corpus uni. + bi. Sent2Vec '\n",
      " 'book corpus uni. + bi. L1 reg Sent2Vec wiki uni. Sent2Vec wiki uni. + bi. '\n",
      " 'Sent2Vec wiki uni. + bi. L1 reg Sent2Vec twitter uni. Sent2Vec twitter uni. '\n",
      " '+ bi. CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW + embs. DictRep '\n",
      " 'RNN DictRep RNN + embs.',\n",
      " 'News .62/.67 .62/.67 .62/.68 .66/.71 .68/.74 .69/.75 .67/.74 .68/.74 .26/.26 '\n",
      " '.05/.05 .62/.67 .65/.72 .40/.46 .51/.60',\n",
      " 'Forum .49/.49 .51/.51 .51/.52 .47/.47 .50/.50 .52/.52 .52/.53 .54/.54 '\n",
      " '.29/.22 .13/.09 .42/.40 .49/.47 .26/.23 .29/.27',\n",
      " 'STS 2014 WordNet Twitter .75/.72. .70/.75 .71/.68 .70/.75 .72/.70 .69/.75 '\n",
      " '.70/.68 .68/.72 .66/.64 .67/.72 .72/.69 .67/.72 .75/.72 .72/.78 .72/.69 '\n",
      " '.70/.77 .50/.35 .37/.31 .40/.33 .36/.30 .81/.81 .62/.66 .85/.86 .67/.72 '\n",
      " '.78/.78 .42/.42 .80/.81 .44/.47',\n",
      " 'Images .78/.82 .75/.79 .76/.81 .76/.79 .75/.79 .76/.80 .77/.81 .76/.79 '\n",
      " '.78/.81 .76/.82 .66/.68 .71/.74 .56/.56 .65/.70',\n",
      " 'Headlines .61/.63 .59/.62 .60/.63 .63/.67 .62/.67 .61/.66 .64/.68 .62/.67 '\n",
      " '.39/.36 .30/.28 .53/.58 .57/.61 .38/.40 .42/.46',\n",
      " 'SICK 2014 Test + Train .61/.70 .62/.70 .62/.71 .64/.71 .63/.71 .63/.72 '\n",
      " '.62/.71 .63/.72 .45/.44 .36/.35 .61/.63 .61/.70 .47/.49 .52/.56',\n",
      " 'Average .65/.68 .65/.67 .66/.68 .65/.68 .65/.68 .66/.69 .67/.71 .66/.70 '\n",
      " '.54/.62 .51/.59 .58/.66 .62/.70 .49/.55 .49/.59',\n",
      " 'Table 7: Unsupervised Evaluation: Comparison of the performance of different '\n",
      " 'Sent2Vec models with semisupervised/supervised models on Spearman/Pearson '\n",
      " 'correlation measures. An underline indicates the best performance for the '\n",
      " 'dataset and Sent2Vec model performances are bold if they perform as well or '\n",
      " 'better than all other non-Sent2Vec models, including those presented in '\n",
      " 'Table 2.']\n"
     ]
    }
   ],
   "source": [
    "# Remove reference number (eq. [0,1,2], [13])\n",
    "parahgraphs = [re.sub(' \\[([0-9]+, )*[0-9]+\\]','',p) for p in parahgraphs]\n",
    "pprint(parahgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abstract The recent tremendous success of unsupervised word embeddings in a '\n",
      " 'multitude of applications raises the obvious question if similar methods '\n",
      " 'could be derived to improve embeddings (i.e. semantic representations) of '\n",
      " 'word sequences as well. We present a simple but efficient unsupervised '\n",
      " 'objective to train distributed representations of sentences. Our method '\n",
      " 'outperforms the state-of-the-art unsupervised models on most benchmark '\n",
      " 'tasks, highlighting the robustness of the produced general-purpose sentence '\n",
      " 'embeddings.',\n",
      " '1. Introduction Improving unsupervised learning is of key importance for '\n",
      " 'advancing machine learning methods, as to unlock access to almost unlimited '\n",
      " 'amounts of data to be used as training resources. The majority of recent '\n",
      " 'success stories of deep learning does not fall into this category but '\n",
      " 'instead relied on supervised training (in particular in the vision domain). '\n",
      " 'A very notable exception comes from the text and natural language processing '\n",
      " 'domain, in the form of semantic word embeddings trained unsupervised '\n",
      " '(Mikolov et al., 2013b;a; Pennington et al., 2014). Within only a few years '\n",
      " 'from their invention, such word representations  which are based on a simple '\n",
      " 'matrix factorization model as we formalize below  are now routinely trained '\n",
      " 'on very large amounts of raw text data, and have become ubiquitous building '\n",
      " 'blocks of a majority of current state-of-the-art NLP applications.',\n",
      " 'learning for NLP leads towards increasingly powerful and complex models, '\n",
      " 'such as recurrent neural networks (RNNs), LSTMs, attention models and even '\n",
      " 'Neural Turing Machine architectures. While extremely strong in '\n",
      " 'expressiveness, the increased model complexity makes such models much slower '\n",
      " 'to train on larger datasets. On the other end of the spectrum, simpler '\n",
      " 'shallow models such as matrix factorizations (or bilinear models) can '\n",
      " 'benefit from training on much larger sets of data, which can be a key '\n",
      " 'advantage, especially in the unsupervised setting. Surprisingly, for '\n",
      " 'constructing sentence embeddings, naively using averaged word vectors was '\n",
      " 'recently shown to outperform LSTMs (see (Wieting et al., 2016a) for plain '\n",
      " 'averaging, and (Arora et al., 2017) for weighted averaging). This example '\n",
      " 'shows potential in exploiting the tradeoff between model complexity and '\n",
      " 'ability to process huge amounts of text using scalable algorithms, towards '\n",
      " 'the simpler side. In view of this trade-off, our work here further advances '\n",
      " 'unsupervised learning of sentence embeddings. Our proposed model can be seen '\n",
      " 'as an extension of the CBOW (Mikolov et al., 2013b;a) training objective to '\n",
      " 'train sentence instead of word embeddings. We demonstrate that the empirical '\n",
      " 'performance of our resulting general-purpose sentence embeddings very '\n",
      " 'significantly exceeds the state of the art, while keeping the model '\n",
      " 'simplicity as well as training and inference complexity exactly as low as in '\n",
      " 'averaging methods (Wieting et al., 2016a; Arora et al., 2017), thereby also '\n",
      " 'putting the title of (Arora et al., 2017) in perspective. Contributions. The '\n",
      " 'main contributions in this work can be summarized as follows:1',\n",
      " 'While very useful semantic representations are available for words, it '\n",
      " 'remains challenging to produce and learn such semantic embeddings for longer '\n",
      " 'pieces of text, such as sentences, paragraphs or entire documents. Even more '\n",
      " 'so, it remains a key goal to learn such general-purpose representations in '\n",
      " 'an unsupervised way.',\n",
      " ' Model. We propose Sent2Vec, a simple unsupervised model allowing to compose '\n",
      " 'sentence embeddings using the word vectors along with n-gram embeddings, '\n",
      " 'simultaneously training composition and the embedding vectors themselves.',\n",
      " 'Currently, two contrary research trends have emerged in text understanding: '\n",
      " 'On one hand, a strong trend in deep-',\n",
      " ' Scalability. The computational complexity of our embeddings is only O(1) '\n",
      " 'vector operations per word processed, both during training and inference of '\n",
      " 'the sen-',\n",
      " 'Equal contribution 1 Iprova SA, Switzerland 2 Computer and Communication '\n",
      " 'Sciences, EPFL, Switzerland. Correspondence to: Martin Jaggi '\n",
      " '<martin.jaggi@epfl.ch>.',\n",
      " 'tence embeddings. This strongly contrasts all neural network based '\n",
      " 'approaches, and allows our model to learn from extremely large datasets, '\n",
      " 'which is a crucial advantage in the unsupervised setting.  Performance. Our '\n",
      " 'method shows significant performance improvements compared to the current '\n",
      " 'stateof-the-art unsupervised and even semi-supervised models. The resulting '\n",
      " 'general-purpose embeddings show strong robustness when transferred to a wide '\n",
      " 'range of prediction benchmarks.',\n",
      " 'Formally, we learn source v w and target uw embeddings for each word w in '\n",
      " 'the vocabulary, with embedding dimension h and k = |V| as in (1). The '\n",
      " 'sentence embedding is defined as the average of the source word embeddings '\n",
      " 'of its constituent words, as in (2). We augment this model furthermore by '\n",
      " 'also learning source embeddings for not only unigrams but also n-grams '\n",
      " 'present in each sentence, and averaging the n-gram embeddings along with the '\n",
      " 'words, i.e., the sentence embedding v S for S is modeled as v S :=',\n",
      " '2. Model Our model is inspired by simple matrix factor models (bilinear '\n",
      " 'models) such as recently very successfully used in unsupervised learning of '\n",
      " 'word embeddings (Mikolov et al., 2013b;a; Pennington et al., 2014; '\n",
      " 'Bojanowski et al., 2017) as well as supervised of sentence classification '\n",
      " '(Joulin et al., 2017). More precisely, these models are formalized as an '\n",
      " 'optimization problem of the form min',\n",
      " 'where R(S) is the list of n-grams (including unigrams) present in sentence '\n",
      " 'S. In order to predict a missing word from the context, our objective models '\n",
      " 'the softmax output approximated by negative sampling following (Mikolov et '\n",
      " 'al., 2013b). For the large number of output classes |V| to be predicted, '\n",
      " 'negative sampling is known to significantly improve training efficiency, see '\n",
      " 'also (Goldberg & Levy, 2014). Given the binary logistic loss function ` : x '\n",
      " '7 log (1 + ex ) coupled with negative sampling, our unsupervised training '\n",
      " 'objective is formulated as follows:',\n",
      " 'for two parameter matrices U  Rkh and V  Rh|V| , where V denotes the '\n",
      " 'vocabulary. In all models studied, the columns of the matrix V will collect '\n",
      " 'the learned word vectors, having h dimensions. For a given sentence S, which '\n",
      " 'can be of arbitrary length, the indicator vector S  {0, 1}|V| is a binary '\n",
      " 'vector encoding S (bag of words encoding). Fixed-length context windows S '\n",
      " 'running over the corpus are used in word embedding methods as in C-BOW '\n",
      " '(Mikolov et al., 2013b;a) and GloVe (Pennington et al., 2014). Here we have '\n",
      " 'k = |V| and each cost function fS : Rk  R only depends on a single row of '\n",
      " 'its input, describing the observed target word for the given fixed-length '\n",
      " 'context S. In contrast, for sentence embeddings which are the focus of our '\n",
      " 'paper here, S will be entire sentences or documents (therefore variable '\n",
      " 'length). This property is shared with the supervised FastText classifier '\n",
      " '(Joulin et al., 2017), which however uses soft-max with k  |V| being the '\n",
      " 'number of class labels. 2.1. Proposed Unsupervised Model We propose a new '\n",
      " 'unsupervised model, Sent2Vec, for learning universal sentence embeddings. '\n",
      " 'Conceptually, the model can be interpreted as a natural extension of the '\n",
      " 'wordcontexts from C-BOW (Mikolov et al., 2013b;a) to a larger sentence '\n",
      " 'context, with the sentence words being specifically optimized towards '\n",
      " 'additive combination over the sentence, by means of the unsupervised '\n",
      " 'objective function.',\n",
      " 'where S corresponds to the current sentence and Nwt is the set of words '\n",
      " 'sampled negatively for the word wt  S. The negatives are sampled2 following '\n",
      " 'a multinomial distribution where with a probability  word',\n",
      " ' each P w ispassociated fwi , where fw is the norqn (w) := fw wi V malized '\n",
      " 'frequency of w in the corpus. To select the possible target unigrams '\n",
      " '(positives), we use subsampling as in (Joulin et al., 2017; Bojanowski et '\n",
      " 'al., 2017), each word w being discarded 1  p with probability \\\\t qp (w) '\n",
      " 'where qp (w) := min 1, t/fw + t/fw . Where t is the subsampling '\n",
      " 'hyper-parameter. Subsampling prevents very frequent words of having too much '\n",
      " 'influence in the learning as they would introduce strong biases in the '\n",
      " 'prediction task. With positives subsampling and respecting the negative '\n",
      " 'sampling distribution, the precise training objective function becomes X X',\n",
      " 'To efficiently sample negatives, a pre-processing table is constructed, '\n",
      " 'containing the words corresponding to the square root of their corpora '\n",
      " 'frequency. Then, the negatives Nwt are sampled uniformly at random from the '\n",
      " 'negatives table except the target wt itself, following (Joulin et al., 2017; '\n",
      " 'Bojanowski et al., 2017).',\n",
      " '2.2. Computational Efficiency In contrast to more complex neural network '\n",
      " 'based models, one of the core advantages of the proposed technique is the '\n",
      " 'low computational cost for both inference and training. Given a sentence S '\n",
      " 'and a trained model, computing the sentence representation v S only requires '\n",
      " '|S|  h floating point operations (or |R(S)|  h to be precise for the n-gram '\n",
      " 'case, see (2)), where h is the embedding dimension. The same holds for the '\n",
      " 'cost of training with SGD on the objective (3), per sentence seen in the '\n",
      " 'training corpus. Due to the simplicity of the model, parallel training is '\n",
      " 'straight-forward using parallelized or distributed SGD. 2.3. Comparison to '\n",
      " 'C-BOW C-BOW (Mikolov et al., 2013b;a) tries to predict a chosen target word '\n",
      " 'given its fixed-size context window, the context being defined by the '\n",
      " 'average of the vectors associated with the words at a distance less than the '\n",
      " 'window size hyperparameter ws. If our system, when restricted to unigram '\n",
      " 'features, can be seen as an extension of C-BOW where the context window '\n",
      " 'includes the entire sentence, in practice there are few important '\n",
      " 'differences as C-BOW uses important tricks to facilitate the learning of '\n",
      " 'word embeddings. C-BOW first uses frequent word subsampling on the '\n",
      " 'sentences, deciding to discard each token w with probability qp (w) or alike '\n",
      " '(small variations exist across implementations). Subsampling prevents the '\n",
      " 'generation of n-grams features, and deprives the sentence of an important '\n",
      " 'part of its syntactical features. It also shortens the distance between '\n",
      " 'subsampled words, implicitly increasing the span of the context window. A '\n",
      " 'second trick consists of using dynamic context windows: for each subsampled '\n",
      " 'word w, the size of its associated context window is sampled uniformly '\n",
      " 'between 1 and ws. Using dynamic context windows is equivalent to weighing by '\n",
      " 'the distance from the focus word w divided by the window size (Levy et al., '\n",
      " '2015). This makes the prediction task local, and go against our objective of '\n",
      " 'creating sentence embeddings as we want to learn how to compose all n-gram '\n",
      " 'features present in a sentence. In the results section, we report a '\n",
      " 'significant improvement of our method over C-BOW. 2.4. Model Training Three '\n",
      " 'different datasets have been used to train our models: the Toronto book '\n",
      " 'corpus3 , Wikipedia sentences and tweets. The Wikipedia and Toronto books '\n",
      " 'sentences have been tokenized using the Stanford NLP library (Manning et '\n",
      " 'al., 2014), while for tweets we used the NLTK tweets tokenizer (Bird et al., '\n",
      " '2009). For training, we select a sentence randomly from the dataset and then '\n",
      " 'proceed to select all the 3',\n",
      " 'possible target unigrams using subsampling. We update the weights using SGD '\n",
      " 'with a linearly decaying learning rate. Also, to prevent overfitting, for '\n",
      " 'each sentence we use dropout on its list of n-grams R(S) \\\\\\\\ {U (S)}, where '\n",
      " 'U (S) is the set of all unigrams contained in sentence S. After empirically '\n",
      " 'trying multiple dropout schemes, we find that dropping K n-grams (n > 1) for '\n",
      " 'each sentence is giving superior results compared to dropping each token '\n",
      " 'with some fixed probability. This dropout mechanism would negatively impact '\n",
      " 'shorter sentences. The regularization can be pushed further by applying L1 '\n",
      " 'regularization to the word vectors. Encouraging sparsity in the embedding '\n",
      " 'vectors is particularly beneficial for high dimension h. The additional soft '\n",
      " 'thresholding in every SGD step adds negligible computational cost. See also '\n",
      " 'Appendix B. We train two models on each dataset, one with unigrams only and '\n",
      " 'one with unigrams and bigrams. All training parameters for the models are '\n",
      " 'provided in Table 5 in the supplementary material. Our C++ implementation '\n",
      " 'builds upon the FastText library (Joulin et al., 2017; Bojanowski et al., '\n",
      " '2017). We will make our code and pre-trained models available open-source.',\n",
      " '3. Related Work We discuss existing models which have been proposed to '\n",
      " 'construct sentence embeddings. While there is a large body of works in this '\n",
      " 'direction  several among these using e.g. labelled datasets of paraphrase '\n",
      " 'pairs to obtain sentence embeddings in a supervised manner (Wieting et al., '\n",
      " '2016b;a)  we here focus on unsupervised, task-independent models. While some '\n",
      " 'methods require ordered raw text i.e., a coherent corpus where the next '\n",
      " 'sentence is a logical continuation of the previous sentence, others rely '\n",
      " 'only on raw text i.e., an unordered collection of sentences. Finally we also '\n",
      " 'discuss alternative models built from structured data sources. 3.1. '\n",
      " 'Unsupervised Models Independent of Sentence Ordering The ParagraphVector '\n",
      " 'DBOW model (Le & Mikolov, 2014) is a log-linear model which is trained to '\n",
      " 'learn sentence as well as word embeddings and then use a softmax '\n",
      " 'distribution to predict words contained in the sentence given the sentence '\n",
      " 'vector representation. They also propose a different model ParagraphVector '\n",
      " 'DM where they use n-grams of consecutive words along with the sentence '\n",
      " 'vector representation to predict the next word. (Hill et al., 2016a) propose '\n",
      " 'a Sequential (Denoising) Autoencoder, S(D)AE. This model first introduces '\n",
      " 'noise in the input data: Firstly each word is deleted with probability p0 , '\n",
      " 'then for each non-overlapping bigram, words',\n",
      " 'are swapped with probability px . The model then uses an LSTM-based '\n",
      " 'architecture to retrieve the original sentence from the corrupted version. '\n",
      " 'The model can then be used to encode new sentences into vector '\n",
      " 'representations. In the case of p0 = px = 0, the model simply becomes a '\n",
      " 'Sequential Autoencoder. (Hill et al., 2016a) also propose a variant (S(D)AE '\n",
      " '+ embs.) in which the words are represented by fixed pre-trained word vector '\n",
      " 'embeddings. (Arora et al., 2017) propose a model in which sentences are '\n",
      " 'represented as a weighted average of fixed (pre-trained) word vectors, '\n",
      " 'followed by post-processing step of subtracting the principal component. '\n",
      " 'Using the generative model of (Arora et al., 2016), words are generated '\n",
      " 'conditioned on a sentence discourse vector cs : P r[w | cs ] = fw + (1  )',\n",
      " 'where Zcs := and cs := c0 + (1  )cs and ,  are scalars. c0 is the common '\n",
      " 'discourse vector, representing a shared component among all discourses, '\n",
      " 'mainly related to syntax. It allows the model to better generate syntactical '\n",
      " 'features. The fw term is here to enable the model to generate some frequent '\n",
      " 'words even if their matching with the discourse vector cs is low. Therefore, '\n",
      " 'this model tries to generate sentences as a mixture of three type of words: '\n",
      " 'words matching the sentence discourse vector cs , syntactical words matching '\n",
      " 'c0 , and words with high fw . (Arora et al., 2017) demonstrated thatPfor '\n",
      " 'this model, the MLE of cs can be approximated by wS fwa+a v w , where a is a '\n",
      " 'scalar. The sentence discourse vector can hence be obtained by subtracting '\n",
      " 'c0 estimated by the first principal component of cs s on a set of sentences. '\n",
      " 'In other words, the sentence embeddings are obtained by a weighted average '\n",
      " 'of the word vectors stripping away the syntax by subtracting the common '\n",
      " 'discourse vector and down-weighting frequent tokens. They generate sentence '\n",
      " 'embeddings from diverse pre-trained word embeddings among which are '\n",
      " 'unsupervised word embeddings such as GloVe (Pennington et al., 2014) as well '\n",
      " 'as supervised word embeddings such as paragram-SL999 (PSL) (Wieting et al., '\n",
      " '2015) trained on the Paraphrase Database (Ganitkevitch et al., 2013). In a '\n",
      " 'very different line of work, C-PHRASE (Pham et al., 2015) relies on '\n",
      " 'additional information from the syntactic parse tree of each sentence, which '\n",
      " 'is incorporated into the C-BOW training objective. (Huang & Anandkumar, '\n",
      " '2016) show that single layer CNNs can be modeled using a tensor '\n",
      " 'decomposition approach. While building on an unsupervised objective, the '\n",
      " 'employed dictionary learning step for obtaining phrase templates is '\n",
      " 'task-specific (for each use-case), not resulting in general-purpose '\n",
      " 'embeddings.',\n",
      " '3.2. Unsupervised Models Depending on Sentence Ordering The SkipThought '\n",
      " 'model (Kiros et al., 2015) combines sentence level models with recurrent '\n",
      " 'neural networks. Given a sentence Si from an ordered corpus, the model is '\n",
      " 'trained to predict Si1 and Si+1 . FastSent (Hill et al., 2016a) is a '\n",
      " 'sentence-level log-linear bag-of-words model. Like SkipThought, it uses '\n",
      " 'adjacent sentences as the prediction target and is trained in an '\n",
      " 'unsupervised fashion. Using word sequences allows the model to improve over '\n",
      " 'the earlier work of paragraph2vec (Le & Mikolov, 2014). (Hill et al., 2016a) '\n",
      " 'augment FastSent further by training it to predict the constituent words of '\n",
      " 'the sentence as well. This model is named FastSent + AE in our comparisons. '\n",
      " 'Compared to our approach, Siamese C-BOW (Kenter et al., 2016) shares the '\n",
      " 'idea of learning to average word embeddings over a sentence. However, it '\n",
      " 'relies on a Siamese neural network architecture to predict surrounding '\n",
      " 'sentences, contrasting our simpler unsupervised objective. Note that on the '\n",
      " 'character sequence level instead of word sequences, FastText (Bojanowski et '\n",
      " 'al., 2017) uses the same conceptual model to obtain better word embeddings. '\n",
      " 'This is most similar to our proposed model, with two key differences: '\n",
      " 'Firstly, we predict from source word sequences to target words, as opposed '\n",
      " 'to character sequences to target words, and secondly, our model is averaging '\n",
      " 'the source embeddings instead of summing them. 3.3. Models requiring '\n",
      " 'structured data DictRep (Hill et al., 2016b) is trained to map dictionary '\n",
      " 'definitions of the words to the pre-trained word embeddings of these words. '\n",
      " 'They use two different architectures, namely BOW and RNN (LSTM) with the '\n",
      " 'choice of learning the input word embeddings or using them pre-trained. A '\n",
      " 'similar architecture is used by the CaptionRep variant, but here the task is '\n",
      " 'the mapping of given image captions to a pre-trained vector representation '\n",
      " 'of these images.',\n",
      " '4. Evaluation Tasks We use a standard set of supervised as well as '\n",
      " 'unsupervised benchmark tasks from the literature to evaluate our trained '\n",
      " 'models, following (Hill et al., 2016a). The breadth of tasks allows to '\n",
      " 'fairly measure generalization to a wide area of different domains, testing '\n",
      " 'the general-purpose quality (universality) of all competing sentence '\n",
      " 'embeddings. For downstream supervised evaluations, sentence embeddings are '\n",
      " 'combined with logistic regression to predict target labels. In the '\n",
      " 'unsupervised evaluation for sentence similarity, correlation of the cosine '\n",
      " 'similarity between two em-',\n",
      " 'beddings is compared to human annotators. Downstream Supervised Evaluation. '\n",
      " 'Sentence embeddings are evaluated for various supervised classification '\n",
      " 'tasks as follows. We evaluate paraphrase identification (MSRP) (Dolan et '\n",
      " 'al., 2004), classification of movie review sentiment (MR) (Pang & Lee, '\n",
      " '2005), product reviews (CR) (Hu & Liu, 2004), subjectivity classification '\n",
      " '(SUBJ)(Pang & Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and '\n",
      " 'question type classification (TREC) (Voorhees, 2002). To classify, we use '\n",
      " 'the code provided by (Kiros et al., 2015) in the same manner as in (Hill et '\n",
      " 'al., 2016a). For the MSRP dataset, containing pairs of sentences (S1 , S2 ) '\n",
      " 'with associated paraphrase label, we generate feature vectors by '\n",
      " 'concatenating their Sent2Vec representations |v S1  v S2 | with the '\n",
      " 'component-wise product v S1  v S2 . The predefined training split is used to '\n",
      " 'tune the L2 penalty parameter using cross-validation and the accuracy and F1 '\n",
      " 'scores are computed on the test set. For the remaining 5 datasets, Sent2Vec '\n",
      " 'embeddings are inferred from input sentences and directly fed to a logistic '\n",
      " 'regression classifier. Accuracy scores are obtained using 10-fold '\n",
      " 'cross-validation for the MR, CR, SUBJ and MPQA datasets. For those datasets '\n",
      " 'nested cross-validation is used to tune the L2 penalty. For the TREC '\n",
      " 'dataset, as for the MRSP dataset, the L2 penalty is tuned on the predefined '\n",
      " 'train split using 10-fold cross-validation, and the accuracy is computed on '\n",
      " 'the test set. Unsupervised Similarity Evaluation. We perform unsupervised '\n",
      " 'evaluation of the the learnt sentence embeddings using the sentence cosine '\n",
      " 'similarity, on the STS 2014 (Agirre et al., 2014) and SICK 2014 (Marelli et '\n",
      " 'al., 2014) datasets. These similarity scores are compared to the '\n",
      " 'gold-standard human judgements using Pearsons r (Pearson, 1895) and '\n",
      " 'Spearmans  (Spearman, 1904) correlation scores. The SICK dataset consists of '\n",
      " 'about 10,000 sentence pairs along with relatedness scores of the pairs. The '\n",
      " 'STS 2014 dataset contains 3,770 pairs, divided into six different categories '\n",
      " 'on the basis of origin of sentences/phrases namely Twitter, headlines, news, '\n",
      " 'forum, WordNet and images. See (Agirre et al., 2014) for more precise '\n",
      " 'information on how the pairs have been created.',\n",
      " '5. Results and Discussion In Tables 1 and 2, we compare our results with '\n",
      " 'those obtained by (Hill et al., 2016a) on different models. Along with the '\n",
      " 'models discussed in Section 3, this also includes the sentence embedding '\n",
      " 'baselines obtained by simple averaging of word embeddings over the sentence, '\n",
      " 'in both the C-BOW and skip-gram variants. TF-IDF BOW is a representation '\n",
      " 'consisting of the counts of the 200,000 most common feature-words, weighed '\n",
      " 'by their TF-IDF frequencies. To ensure coherence, we only include '\n",
      " 'unsupervised mod-',\n",
      " 'els in the main paper. Performance of supervised and semisupervised models '\n",
      " 'on these evaluations can be observed in Tables 6 and 7 in the supplementary '\n",
      " 'material. Downstream Supervised Evaluation Results. On running supervised '\n",
      " 'evaluations and observing the results in Table 1, we find that on an average '\n",
      " 'our models are second only to SkipThought vectors. Also, both our models '\n",
      " 'achieve state of the art results on the CR task. We also observe that on '\n",
      " 'half of the supervised tasks, our unigrams + bigram model is the the best '\n",
      " 'model after SkipThought. Our models are weaker on the MSRP task (which '\n",
      " 'consists of the identification of labelled paraphrases) compared to '\n",
      " 'stateof-the-art methods. However, we observe that the models which perform '\n",
      " 'extremely well on this task end up faring very poorly on the other tasks, '\n",
      " 'indicating a lack of generalizability. On rest of the tasks, our models '\n",
      " 'perform extremely well. The SkipThought model is able to outperform our '\n",
      " 'models on most of the tasks as it is trained to predict the previous and '\n",
      " 'next sentences and a lot of tasks are able to make use of this contextual '\n",
      " 'information missing in our Sent2Vec models. For example, the TREC task is a '\n",
      " 'poor measure of how one predicts the content of the sentence (the question) '\n",
      " 'but a good measure of how the next sentence in the sequence (the answer) is '\n",
      " 'predicted. Unsupervised Similarity Evaluation Results. In Table 2, we see '\n",
      " 'that our Sent2Vec models are state-of-the-art on the majority of tasks when '\n",
      " 'comparing to all the unsupervised models trained on the Toronto corpus, and '\n",
      " 'clearly achieve the best averaged performance. Our Sent2Vec models also on '\n",
      " 'average outperform or are at par with the C-PHRASE model, despite '\n",
      " 'significantly lagging behind on the STS 2014 WordNet and News subtasks. This '\n",
      " 'observation can be attributed to the fact that a big chunk of the data that '\n",
      " 'the C-PHRASE model is trained on comes from English Wikipedia, helping it to '\n",
      " 'perform well on datasets involving definition and news items. Also, C-PHRASE '\n",
      " 'uses data three times the size of the Toronto book corpus. Interestingly, '\n",
      " 'our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table '\n",
      " '3, despite the fact that we use no parse tree information. In the official '\n",
      " 'results of the more recent edition of the STS 2017 benchmark (Cer et al., '\n",
      " '2017), our model also significantly outperforms C-PHRASE, and delivers the '\n",
      " 'best unsupervised baseline method. Macro Average. To summarize our '\n",
      " 'contributions on both supervised and unsupervised tasks, in Table 3 we '\n",
      " 'present the results in terms of the macro average over the averages 4 For '\n",
      " 'the Siamese C-BOW model trained on the Toronto corpus, supervised evaluation '\n",
      " 'as well as similarity evaluation results on the SICK 2014 dataset are '\n",
      " 'unavailable.',\n",
      " 'Model SAE SAE + embs. SDAE SDAE + embs. ParagraphVec DBOW ParagraphVec DM '\n",
      " 'Skipgram C-BOW Unigram TFIDF Sent2Vec uni. Sent2Vec uni. + bi. SkipThought '\n",
      " 'FastSent FastSent+AE C-PHRASE',\n",
      " 'MSRP (Acc / F1) 74.3 / 81.7 70.6 / 77.9 76.4 / 83.4 73.7 / 80.7 72.9 / 81.1 '\n",
      " '73.6 / 81.9 69.3 / 77.2 67.6 / 76.1 73.6 / 81.7 72.2 / 80.3 72.5 / 80.8 73.0 '\n",
      " '/ 82.0 72.2 / 80.3 71.2 / 79.1 72.2 / 79.6',\n",
      " 'Table 1: Comparison of the performance of different models on different '\n",
      " 'supervised evaluation tasks. An underline indicates the best performance for '\n",
      " 'the dataset. Top 3 performances in each data category are shown in bold. The '\n",
      " 'average is calculated as the average of accuracy for each category (For '\n",
      " 'MSRP, we take the average of two entries.) Model SAE SAE + embs. SDAE SDAE + '\n",
      " 'embs. ParagraphVec DBOW ParagraphVec DM Skipgram C-BOW Unigram TF-IDF '\n",
      " 'Sent2Vec uni. Sent2Vec uni. + bi. SkipThought FastSent FastSent+AE Siamese '\n",
      " 'C-BOW4 C-PHRASE',\n",
      " 'News .17/.16 .52/.54 .07/.04 .51/.54 .31/.34 .42/.46 .56/.59 .57/.61 .48/.48 '\n",
      " '.62/.67 .62/.67 .44/.45 .58/.59 .56/.59 .58/.59 .69/.71',\n",
      " 'Forum .12/.12 .22/.23 .11/.13 .29/.29 .32/.32 .33/.34 .42/.42 .43/.44 '\n",
      " '.40/.38 .49/.49 .51/.51 .14/.15 .41/.36 .41/.40 .42/.41 .43/.41',\n",
      " 'STS 2014 WordNet Twitter .30/.23 .28/.22 .60/.55 .60/.60 .33/.24 .44/.42 '\n",
      " '.56/.50 .57/.58 .53/.50 .43/.46 .51/.48 .54/.57 .73/.70 .71/.74 .72/.69 '\n",
      " '.71/.75 .60/.59 .63/.65 .75/.72 .70/.75 .71/.68 .70/.75 .39/.34 .42/.43 '\n",
      " '.74/.70 .63/.66 .69/.64 .70/.74 .66/.61 .71/.73 .76/.73 .60/.65',\n",
      " 'Images .49/.46 .64/.64 .44/.38 .59/.59 .46/.44 .32/.30 .65/.67 .71/.73 '\n",
      " '.72/.74 .78/.82 .75/.79 .55/.60 .74/.78 .63/.65 .65/.65 .75/.79',\n",
      " 'Headlines .13/.11 .41/.41 .36/.36 .43/.44 .39/.41 .46/.47 .55/.58 .55/.59 '\n",
      " '.49/.49 .61/.63 .59/.62 .43/.44 .57/.59 .58/.60 .63/.64 .60/.65',\n",
      " 'SICK 2014 Test + Train .32/.31 .47/.49 .46/.46 .46/.46 .42/.46 .44/.40 '\n",
      " '.60/.69 .60/.69 .52/.58 .61/.70 .62/.70 .57/.60 .61/.72 .60/.65',\n",
      " 'Average .26/.23 .50/.49 .31/.29 .49/.49 .41/.42 .43/.43 .60/.63 .60/.65 '\n",
      " '.55/.56 .65/.68 .65/.67 .42/.43 .61/.63 .60/.61',\n",
      " 'Table 2: Unsupervised Evaluation Tasks: Comparison of the performance of '\n",
      " 'different models on Spearman/Pearson correlation measures. An underline '\n",
      " 'indicates the best performance for the dataset. Top 3 performances in each '\n",
      " 'data category are shown in bold. The average is calculated as the average of '\n",
      " 'entries for each correlation measure. of both supervised and unsupervised '\n",
      " 'tasks along with the training times of the models5 . For unsupervised tasks, '\n",
      " 'averages are taken over both Spearman and Pearson scores. The comparison '\n",
      " 'includes the best performing unsupervised and semi-supervised methods '\n",
      " 'described in Section 3. For models trained on the Toronto books dataset, we '\n",
      " 'report a 3.8 % points improvement over the state of the art. Considering all '\n",
      " 'supervised, semi-supervised methods and all datasets compared in (Hill et '\n",
      " 'al., 2016a), we report a 2.2 % points improvement. We also see a noticeable '\n",
      " 'improvement in accuracy as we use larger datasets like twitter and Wikipedia '\n",
      " 'dump. We can also see that the Sent2Vec models are also faster to train when '\n",
      " 'compared to methods like SkipThought and DictRep owing to the SGD step '\n",
      " 'allowing a high degree of parallelizability. We can clearly see Sent2Vec '\n",
      " 'outperforming other unsupervised and even semi-supervised methods. This can '\n",
      " 'be at5',\n",
      " 'tributed to the superior generalizability of our model across supervised and '\n",
      " 'unsupervised tasks. Comparison with Arora et al. (2017). In Table 4, we '\n",
      " 'report an experimental comparison to the model of Arora et al. (2017), which '\n",
      " 'is particularly tailored to sentence similarity tasks. In the table, the '\n",
      " 'suffix W indicates that their down-weighting scheme has been used, while the '\n",
      " 'suffix R indicates the removal of the first principal component. They report '\n",
      " 'values of a  [104 , 103 ] as giving the best results and used a = 103 for '\n",
      " 'all their experiments. Their down-weighting scheme hints us to reduce the '\n",
      " 'importance of syntactical features. To do so, we use a simple blacklist '\n",
      " 'containing the 25 most frequent tokens in the Twitter corpus and discard '\n",
      " 'them before averaging. Results are also reported in Table 4. We observe that '\n",
      " 'our results are competitive with the embeddings of Arora et al. (2017) for '\n",
      " 'purely unsupervised methods. We confirm their empirical finding that '\n",
      " 'reducing the influence of the syntax helps performance on semantic sim-',\n",
      " 'unsupervised unsupervised unsupervised unsupervised unsupervised '\n",
      " 'unsupervised semi-supervised unsupervised unsupervised unsupervised '\n",
      " 'unsupervised',\n",
      " 'twitter (19.7B words) twitter (19.7B words) Wikipedia (1.7B words) Wikipedia '\n",
      " '(1.7B words) Toronto books (0.9B words) Toronto books (0.9B words) '\n",
      " 'structured dictionary dataset 2.8B words + parse info. Toronto books (0.9B '\n",
      " 'words) Toronto books (0.9B words) Toronto books (0.9B words)',\n",
      " 'Sent2Vec uni. + bi. Sent2Vec uni. Sent2Vec uni. + bi. Sent2Vec uni. Sent2Vec '\n",
      " 'books uni. Sent2Vec books uni. + bi. DictRep BOW + emb C-PHRASE C-BOW '\n",
      " 'FastSent SkipThought',\n",
      " 'Table 3: Best unsupervised and semi-supervised methods ranked by macro '\n",
      " 'average along with their training times. ** indicates trained on GPU. * '\n",
      " 'indicates trained on a single node using 30 cores. Training times for '\n",
      " 'non-Sent2Vec models are due to (Hill et al., 2016a) Dataset STS 2014 SICK '\n",
      " '2014',\n",
      " 'Table 4: Comparison of the performance of the unsupervised and '\n",
      " 'semi-supervised sentence embeddings by (Arora et al., 2017) with our models, '\n",
      " 'in terms of Pearsons correlation. to unsupervised evaluations but gives a '\n",
      " 'significant boost-up in accuracy on supervised tasks.',\n",
      " 'Figure 1: Left figure: the profile of the word vector L2 norms as a function '\n",
      " 'of log(fw ) for each vocabulary word w, as learnt by our unigram model '\n",
      " 'trained on Toronto books. Right figure: down-weighting scheme proposed by a '\n",
      " '. Arora et al. (2017): weight(w) = a+f w',\n",
      " 'ilarity tasks, and we show that applying a simple blacklist already yields a '\n",
      " 'noticeable amelioration. It is important to note that the scores obtained '\n",
      " 'from supervised task-specific PSL embeddings trained for the purpose of '\n",
      " 'semantic similarity outperform our method on both SICK and average STS 2014, '\n",
      " 'which is expected as our model is trained purely unsupervised. The effect of '\n",
      " 'datasets and n-grams. Despite being trained on three very different '\n",
      " 'datasets, all of our models generalize well to sometimes very specific '\n",
      " 'domains. Models trained on Toronto Corpus are the state-of-the art on the '\n",
      " 'STS 2014 images dataset even beating the supervised CaptionRep model trained '\n",
      " 'on images. We also see that addition of bigrams to our models doesnt help '\n",
      " 'much when it comes',\n",
      " 'On learning the importance and the direction of the word vectors. Our model  '\n",
      " 'by learning how to generate and compose word vectors  has to learn both the '\n",
      " 'direction of the word embeddings as well as their norm. Considering the '\n",
      " 'norms of the used word vectors as by our averaging over the sentence, we '\n",
      " 'observe an interesting distribution of the importance of each word. In '\n",
      " 'Figure 1 we show the profile of the L2 -norm as a function of log(fw ) for '\n",
      " 'each w  V, and compare it to the static down-weighting mechanism of Arora et '\n",
      " 'al. (2017). We can observe that our model is learning to down-weight '\n",
      " 'frequent tokens by itself. It is also down-weighting rare tokens and the '\n",
      " 'norm profile seems to roughly follow Luhns hypothesis (Luhn, 1958), a well '\n",
      " 'known information retrieval paradigm, stating that mid-rank terms are the '\n",
      " 'most significant to discriminate content. Modifying the objective function '\n",
      " 'would change the weighting scheme learnt. From a more semantic oriented '\n",
      " 'objective, it should be possible to learn to attribute lower norms for very '\n",
      " 'frequent terms, to more specifically fit sentence similarity tasks.',\n",
      " '6. Conclusion In this paper, we introduced a novel unsupervised and '\n",
      " 'computationally efficient method to train and infer sentence embeddings. On '\n",
      " 'supervised evaluations, our method, on an average, achieves better '\n",
      " 'performance than all other unsupervised competitors except the SkipThought '\n",
      " 'vectors. However, SkipThought vectors show an extremely poor performance on '\n",
      " 'sentence similarity tasks while our model',\n",
      " 'is state-of-the-art for these evaluations on average. Future work could '\n",
      " 'focus on augmenting the model to exploit data with ordered sentences. '\n",
      " 'Furthermore, we would like to further investigate the models ability as '\n",
      " 'giving pre-trained embeddings to enable downstream transfer learning tasks. '\n",
      " 'Acknowledgments. We are indebted to Piotr Bojanowski and Armand Joulin for '\n",
      " 'helpful discussions.',\n",
      " 'References Agirre, Eneko, Banea, Carmen, Cardie, Claire, Cer, Daniel, Diab, '\n",
      " 'Mona, Gonzalez-Agirre, Aitor, Guo, Weiwei, Mihalcea, Rada, Rigau, German, '\n",
      " 'and Wiebe, Janyce. Semeval-2014 task 10: Multilingual semantic textual '\n",
      " 'similarity. In Proceedings of the 8th international workshop on semantic '\n",
      " 'evaluation (SemEval 2014), pp. 8191. Association for Computational '\n",
      " 'Linguistics Dublin, Ireland, 2014. Arora, Sanjeev, Li, Yuanzhi, Liang, '\n",
      " 'Yingyu, Ma, Tengyu, and Risteski, Andrej. A Latent Variable Model Approach '\n",
      " 'to PMIbased Word Embeddings. In Transactions of the Association for '\n",
      " 'Computational Linguistics, pp. 385399, July 2016. Arora, Sanjeev, Liang, '\n",
      " 'Yingyu, and Ma, Tengyu. A simple but tough-to-beat baseline for sentence '\n",
      " 'embeddings. In International Conference on Learning Representations (ICLR), '\n",
      " '2017. Bird, Steven, Klein, Ewan, and Loper, Edward. Natural language '\n",
      " 'processing with Python: analyzing text with the natural language toolkit.  '\n",
      " 'OReilly Media, Inc., 2009. Bojanowski, Piotr, Grave, Edouard, Joulin, '\n",
      " 'Armand, and Mikolov, Tomas. Enriching Word Vectors with Subword Information. '\n",
      " 'Transactions of the Association for Computational Linguistics, 5:135146, '\n",
      " '2017. Cer, Daniel, Diab, Mona, Agirre, Eneko, Lopez-Gazpio, Inigo, and '\n",
      " 'Specia, Lucia. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual '\n",
      " 'and Cross-lingual Focused Evaluation. In SemEval-2017 - Proceedings of the '\n",
      " '11th International Workshop on Semantic Evaluations, pp. 114, Vancouver, '\n",
      " 'Canada, August 2017. Association for Computational Linguistics. Dolan, Bill, '\n",
      " 'Quirk, Chris, and Brockett, Chris. Unsupervised construction of large '\n",
      " 'paraphrase corpora: Exploiting massively parallel news sources. In '\n",
      " 'Proceedings of the 20th international conference on Computational '\n",
      " 'Linguistics, pp. 350. Association for Computational Linguistics, 2004. '\n",
      " 'Ganitkevitch, Juri, Van Durme, Benjamin, and Callison-Burch, Chris. Ppdb: '\n",
      " 'The paraphrase database. In HLT-NAACL, pp. 758764, 2013. Goldberg, Yoav and '\n",
      " 'Levy, Omer. word2vec Explained: deriving Mikolov et al.s negative-sampling '\n",
      " 'word-embedding method. arXiv, February 2014.',\n",
      " 'Hu, Minqing and Liu, Bing. Mining and summarizing customer reviews. In '\n",
      " 'Proceedings of the tenth ACM SIGKDD international conference on Knowledge '\n",
      " 'discovery and data mining, pp. 168177. ACM, 2004. Huang, Furong and '\n",
      " 'Anandkumar, Animashree. Unsupervised Learning of Word-Sequence '\n",
      " 'Representations from Scratch via Convolutional Tensor Decomposition. arXiv, '\n",
      " '2016. Joulin, Armand, Grave, Edouard, Bojanowski, Piotr, and Mikolov, Tomas. '\n",
      " 'Bag of Tricks for Efficient Text Classification. In Proceedings of the 15th '\n",
      " 'Conference of the European Chapter of the Association for Computational '\n",
      " 'Linguistics, Short Papers, pp. 427431, Valencia, Spain, 2017. Kenter, Tom, '\n",
      " 'Borisov, Alexey, and de Rijke, Maarten. Siamese CBOW: Optimizing Word '\n",
      " 'Embeddings for Sentence Representations. In ACL - Proceedings of the 54th '\n",
      " 'Annual Meeting of the Association for Computational Linguistics, pp. 941951, '\n",
      " 'Berlin, Germany, 2016. Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan R, '\n",
      " 'Zemel, Richard, Urtasun, Raquel, Torralba, Antonio, and Fidler, Sanja. '\n",
      " 'Skip-Thought Vectors. In NIPS 2015 - Advances in Neural Information '\n",
      " 'Processing Systems 28, pp. 32943302, 2015. Le, Quoc V and Mikolov, Tomas. '\n",
      " 'Distributed Representations of Sentences and Documents. In ICML 2014 - '\n",
      " 'Proceedings of the 31st International Conference on Machine Learning, volume '\n",
      " '14, pp. 11881196, 2014. Levy, Omer, Goldberg, Yoav, and Dagan, Ido. '\n",
      " 'Improving distributional similarity with lessons learned from word '\n",
      " 'embeddings. Transactions of the Association for Computational Linguistics, '\n",
      " '3:211225, 2015. Luhn, Hans Peter. The automatic creation of literature '\n",
      " 'abstracts. IBM Journal of research and development, 2(2):159 165, 1958. '\n",
      " 'Manning, Christopher D, Surdeanu, Mihai, Bauer, John, Finkel, Jenny Rose, '\n",
      " 'Bethard, Steven, and McClosky, David. The stanford corenlp natural language '\n",
      " 'processing toolkit. In ACL (System Demonstrations), pp. 5560, 2014. Marelli, '\n",
      " 'Marco, Menini, Stefano, Baroni, Marco, Bentivogli, Luisa, Bernardi, '\n",
      " 'Raffaella, and Zamparelli, Roberto. A sick cure for the evaluation of '\n",
      " 'compositional distributional semantic models. In LREC, pp. 216223, 2014. '\n",
      " 'Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efficient '\n",
      " 'estimation of word representations in vector space. arXiv preprint '\n",
      " 'arXiv:1301.3781, 2013a. Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, '\n",
      " 'Greg S, and Dean, Jeff. Distributed Representations of Words and Phrases and '\n",
      " 'their Compositionality. In NIPS - Advances in Neural Information Processing '\n",
      " 'Systems 26, pp. 31113119, 2013b.',\n",
      " 'Hill, Felix, Cho, Kyunghyun, and Korhonen, Anna. Learning Distributed '\n",
      " 'Representations of Sentences from Unlabelled Data. In Proceedings of '\n",
      " 'NAACL-HLT, February 2016a.',\n",
      " 'Pang, Bo and Lee, Lillian. A sentimental education: Sentiment analysis using '\n",
      " 'subjectivity summarization based on minimum cuts. In Proceedings of the 42nd '\n",
      " 'annual meeting on Association for Computational Linguistics, pp. 271. '\n",
      " 'Association for Computational Linguistics, 2004.',\n",
      " 'Hill, Felix, Cho, KyungHyun, Korhonen, Anna, and Bengio, Yoshua. Learning to '\n",
      " 'understand phrases by embedding the dictionary. TACL, 4:1730, 2016b. URL '\n",
      " 'https://tacl2013.cs.columbia.edu/ojs/ index.php/tacl/article/view/711.',\n",
      " 'Pang, Bo and Lee, Lillian. Seeing stars: Exploiting class relationships for '\n",
      " 'sentiment categorization with respect to rating scales. In Proceedings of '\n",
      " 'the 43rd annual meeting on association for computational linguistics, pp. '\n",
      " '115124. Association for Computational Linguistics, 2005.',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features Pearson, Karl. Note on regression and inheritance in the case of '\n",
      " 'two parents. Proceedings of the Royal Society of London, 58: 240242, 1895. '\n",
      " 'Pennington, Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: '\n",
      " 'Global vectors for word representation. In EMNLP, volume 14, pp. 15321543, '\n",
      " '2014. Pham, NT, Kruszewski, G, Lazaridou, A, and Baroni, M. Jointly '\n",
      " 'optimizing word representations for lexical and sentential tasks with the '\n",
      " 'c-phrase model. ACL/IJCNLP, 2015. Rockafellar, R Tyrrell. Monotone operators '\n",
      " 'and the proximal point algorithm. SIAM journal on control and optimization, '\n",
      " '14(5):877898, 1976. Spearman, Charles. The proof and measurement of '\n",
      " 'association between two things. The American journal of psychology, 15 '\n",
      " '(1):72101, 1904. Voorhees, Ellen M. Overview of the trec 2001 question '\n",
      " 'answering track. In NIST special publication, pp. 4251, 2002. Wiebe, Janyce, '\n",
      " 'Wilson, Theresa, and Cardie, Claire. Annotating expressions of opinions and '\n",
      " 'emotions in language. Language resources and evaluation, 39(2):165210, 2005. '\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, Livescu, Karen, and Roth, Dan. '\n",
      " 'From paraphrase database to compositional paraphrase model and back. In TACL '\n",
      " '- Transactions of the Association for Computational Linguistics, 2015. '\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen. Towards '\n",
      " 'universal paraphrastic sentence embeddings. In International Conference on '\n",
      " 'Learning Representations (ICLR), 2016a. Wieting, John, Bansal, Mohit, '\n",
      " 'Gimpel, Kevin, and Livescu, Karen. Charagram: Embedding Words and Sentences '\n",
      " 'via Character n-grams. In EMNLP - Proceedings of the 2016 Conference on '\n",
      " 'Empirical Methods in Natural Language Processing, pp. 15041515, Stroudsburg, '\n",
      " 'PA, USA, 2016b. Association for Computational Linguistics.',\n",
      " 'Supplementary Material A. Parameters for training models Model Book corpus '\n",
      " 'Sent2Vec unigrams Book corpus Sent2Vec unigrams + bigrams Wiki Sent2Vec '\n",
      " 'unigrams Wiki Sent2Vec unigrams + bigrams Twitter Sent2Vec unigrams Twitter '\n",
      " 'Sent2Vec unigrams + bigrams',\n",
      " 'B. L1 regularization of models Optionally, our model can be additionally '\n",
      " 'improved by adding an L1 regularizer term in the objective function, leading '\n",
      " 'to slightly better generalization performance. Additionally, encouraging '\n",
      " 'sparsity in the embedding vectors is beneficial for memory reasons, allowing '\n",
      " 'higher embedding dimensions h. We propose to apply L1 regularization '\n",
      " 'individually to each word (and n-gram) vector (both source and target '\n",
      " 'vectors). Formally, the training objective function (3) then becomes',\n",
      " 'where  is the regularization parameter. Now, in order to minimize a function '\n",
      " 'of the form f (z) + g(z) where g(z) is not differentiable over the domain, '\n",
      " 'we can use the basic proximal-gradient scheme. In this iterative method, '\n",
      " 'after doing a gradient descent step on f (z) with learning rate , we update '\n",
      " 'z as zn+1 = prox,g (zn+ 12 )',\n",
      " '1 where prox,g (x) = arg miny {g(y) + 2 ky  xk22 } is called the proximal '\n",
      " 'function(Rockafellar, 1976) of g with  being the proximal parameter and zn+ '\n",
      " '21 is the value of z after a gradient (or SGD) step on zn .',\n",
      " 'In our case, g(z) = kzk1 and the corresponding proximal operator is given by '\n",
      " 'prox,g (x) = sign(x)  max(|xn |  , 0)',\n",
      " 'where  corresponds to element-wise product. Similar to the proximal-gradient '\n",
      " 'scheme, in our case we can optionally use the thresholding operator on the '\n",
      " 'updated word  lr 0 and n-gram vectors after an SGD step. The soft '\n",
      " 'thresholding parameter used for this update is |R(S\\\\\\\\{w and   lr0 for t '\n",
      " '})| 0 the source and target vectors respectively where lr is the current '\n",
      " 'learning rate,  is the L1 regularization parameter and S is the sentence on '\n",
      " 'which SGD is being run.',\n",
      " 'We observe that L1 regularization using the proximal step gives our models a '\n",
      " 'small boost in performance. Also, applying the thresholding operator takes '\n",
      " 'only |R(S\\\\\\\\{wt })|h floating point operations for the updating the word '\n",
      " 'vectors corresponding to the sentence and (|N | + 1)  h for updating the '\n",
      " 'target as well as the negative word vectors, where |N | is the number of '\n",
      " 'negatives sampled and h is the embedding dimension. Thus, performing L1 '\n",
      " 'regularization using soft-thresholding operator comes with a small '\n",
      " 'computational overhead. We set  to be 0.0005 for both the Wikipedia and the '\n",
      " 'Toronto Book Corpus unigrams + bigrams models.',\n",
      " 'C. Performance comparison with Sent2Vec models trained on different corpora '\n",
      " 'Data Unordered Sentences: (Toronto Books) Unordered sentences: Wikipedia (69 '\n",
      " 'million sentences; 1.7 B words) Unordered sentences: Twitter (1.2 billion '\n",
      " 'sentences; 19.7 B words)',\n",
      " 'Model Sent2Vec uni. Sent2Vec uni. + bi. Sent2Vec uni. + bi.L1reg Sent2Vec '\n",
      " 'uni. Sent2Vec uni. + bi. Sent2Vec uni. + bi.L1reg Sent2Vec uni. Sent2Vec '\n",
      " 'uni. + bi. CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW+embs '\n",
      " 'DictRep RNN DictRep RNN+embs.',\n",
      " 'MSRP (Acc / F1) 72.2 / 80.3 72.5 / 80.8 71.6 / 80.1 71.8 / 80.2 72.4 / 80.8 '\n",
      " '73.6 / 81.5 71.5 / 80.0 72.4 / 80.6 73.6 / 81.9 72.6 / 81.1 73.7 / 81.6 68.4 '\n",
      " '/ 76.8 73.2 / 81.6 66.8 / 76.0',\n",
      " 'Table 6: Comparison of the performance of different Sent2Vec models with '\n",
      " 'different semi-supervised/supervised models on different downstream '\n",
      " 'supervised evaluation tasks. An underline indicates the best performance for '\n",
      " 'the dataset and Sent2Vec model performances are bold if they perform as well '\n",
      " 'or better than all other non-Sent2Vec models, including those presented in '\n",
      " 'Table 1.',\n",
      " 'Model Sent2Vec book corpus uni. Sent2Vec book corpus uni. + bi. Sent2Vec '\n",
      " 'book corpus uni. + bi. L1 reg Sent2Vec wiki uni. Sent2Vec wiki uni. + bi. '\n",
      " 'Sent2Vec wiki uni. + bi. L1 reg Sent2Vec twitter uni. Sent2Vec twitter uni. '\n",
      " '+ bi. CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW + embs. DictRep '\n",
      " 'RNN DictRep RNN + embs.',\n",
      " 'News .62/.67 .62/.67 .62/.68 .66/.71 .68/.74 .69/.75 .67/.74 .68/.74 .26/.26 '\n",
      " '.05/.05 .62/.67 .65/.72 .40/.46 .51/.60',\n",
      " 'Forum .49/.49 .51/.51 .51/.52 .47/.47 .50/.50 .52/.52 .52/.53 .54/.54 '\n",
      " '.29/.22 .13/.09 .42/.40 .49/.47 .26/.23 .29/.27',\n",
      " 'STS 2014 WordNet Twitter .75/.72. .70/.75 .71/.68 .70/.75 .72/.70 .69/.75 '\n",
      " '.70/.68 .68/.72 .66/.64 .67/.72 .72/.69 .67/.72 .75/.72 .72/.78 .72/.69 '\n",
      " '.70/.77 .50/.35 .37/.31 .40/.33 .36/.30 .81/.81 .62/.66 .85/.86 .67/.72 '\n",
      " '.78/.78 .42/.42 .80/.81 .44/.47',\n",
      " 'Images .78/.82 .75/.79 .76/.81 .76/.79 .75/.79 .76/.80 .77/.81 .76/.79 '\n",
      " '.78/.81 .76/.82 .66/.68 .71/.74 .56/.56 .65/.70',\n",
      " 'Headlines .61/.63 .59/.62 .60/.63 .63/.67 .62/.67 .61/.66 .64/.68 .62/.67 '\n",
      " '.39/.36 .30/.28 .53/.58 .57/.61 .38/.40 .42/.46',\n",
      " 'SICK 2014 Test + Train .61/.70 .62/.70 .62/.71 .64/.71 .63/.71 .63/.72 '\n",
      " '.62/.71 .63/.72 .45/.44 .36/.35 .61/.63 .61/.70 .47/.49 .52/.56',\n",
      " 'Average .65/.68 .65/.67 .66/.68 .65/.68 .65/.68 .66/.69 .67/.71 .66/.70 '\n",
      " '.54/.62 .51/.59 .58/.66 .62/.70 .49/.55 .49/.59',\n",
      " 'Table 7: Unsupervised Evaluation: Comparison of the performance of different '\n",
      " 'Sent2Vec models with semisupervised/supervised models on Spearman/Pearson '\n",
      " 'correlation measures. An underline indicates the best performance for the '\n",
      " 'dataset and Sent2Vec model performances are bold if they perform as well or '\n",
      " 'better than all other non-Sent2Vec models, including those presented in '\n",
      " 'Table 2.']\n"
     ]
    }
   ],
   "source": [
    "# Remove references\n",
    "parahgraphs = [p for p in parahgraphs if not re.match(r'^\\[[0-9]+\\]', p)]\n",
    "pprint(parahgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abstract The recent tremendous success of unsupervised word embeddings in a '\n",
      " 'multitude of applications raises the obvious question if similar methods '\n",
      " 'could be derived to improve embeddings (i.e. semantic representations) of '\n",
      " 'word sequences as well. We present a simple but efficient unsupervised '\n",
      " 'objective to train distributed representations of sentences. Our method '\n",
      " 'outperforms the state-of-the-art unsupervised models on most benchmark '\n",
      " 'tasks, highlighting the robustness of the produced general-purpose sentence '\n",
      " 'embeddings.',\n",
      " '0. Introduction Improving unsupervised learning is of key importance for '\n",
      " 'advancing machine learning methods, as to unlock access to almost unlimited '\n",
      " 'amounts of data to be used as training resources. The majority of recent '\n",
      " 'success stories of deep learning does not fall into this category but '\n",
      " 'instead relied on supervised training (in particular in the vision domain). '\n",
      " 'A very notable exception comes from the text and natural language processing '\n",
      " 'domain, in the form of semantic word embeddings trained unsupervised '\n",
      " '(Mikolov et al., 0000b;a; Pennington et al., 0000). Within only a few years '\n",
      " 'from their invention, such word representations  which are based on a simple '\n",
      " 'matrix factorization model as we formalize below  are now routinely trained '\n",
      " 'on very large amounts of raw text data, and have become ubiquitous building '\n",
      " 'blocks of a majority of current state-of-the-art NLP applications.',\n",
      " 'learning for NLP leads towards increasingly powerful and complex models, '\n",
      " 'such as recurrent neural networks (RNNs), LSTMs, attention models and even '\n",
      " 'Neural Turing Machine architectures. While extremely strong in '\n",
      " 'expressiveness, the increased model complexity makes such models much slower '\n",
      " 'to train on larger datasets. On the other end of the spectrum, simpler '\n",
      " 'shallow models such as matrix factorizations (or bilinear models) can '\n",
      " 'benefit from training on much larger sets of data, which can be a key '\n",
      " 'advantage, especially in the unsupervised setting. Surprisingly, for '\n",
      " 'constructing sentence embeddings, naively using averaged word vectors was '\n",
      " 'recently shown to outperform LSTMs (see (Wieting et al., 0000a) for plain '\n",
      " 'averaging, and (Arora et al., 0000) for weighted averaging). This example '\n",
      " 'shows potential in exploiting the tradeoff between model complexity and '\n",
      " 'ability to process huge amounts of text using scalable algorithms, towards '\n",
      " 'the simpler side. In view of this trade-off, our work here further advances '\n",
      " 'unsupervised learning of sentence embeddings. Our proposed model can be seen '\n",
      " 'as an extension of the CBOW (Mikolov et al., 0000b;a) training objective to '\n",
      " 'train sentence instead of word embeddings. We demonstrate that the empirical '\n",
      " 'performance of our resulting general-purpose sentence embeddings very '\n",
      " 'significantly exceeds the state of the art, while keeping the model '\n",
      " 'simplicity as well as training and inference complexity exactly as low as in '\n",
      " 'averaging methods (Wieting et al., 0000a; Arora et al., 0000), thereby also '\n",
      " 'putting the title of (Arora et al., 0000) in perspective. Contributions. The '\n",
      " 'main contributions in this work can be summarized as follows:0',\n",
      " 'While very useful semantic representations are available for words, it '\n",
      " 'remains challenging to produce and learn such semantic embeddings for longer '\n",
      " 'pieces of text, such as sentences, paragraphs or entire documents. Even more '\n",
      " 'so, it remains a key goal to learn such general-purpose representations in '\n",
      " 'an unsupervised way.',\n",
      " ' Model. We propose Sent0Vec, a simple unsupervised model allowing to compose '\n",
      " 'sentence embeddings using the word vectors along with n-gram embeddings, '\n",
      " 'simultaneously training composition and the embedding vectors themselves.',\n",
      " 'Currently, two contrary research trends have emerged in text understanding: '\n",
      " 'On one hand, a strong trend in deep-',\n",
      " ' Scalability. The computational complexity of our embeddings is only O(0) '\n",
      " 'vector operations per word processed, both during training and inference of '\n",
      " 'the sen-',\n",
      " 'Equal contribution 0 Iprova SA, Switzerland 0 Computer and Communication '\n",
      " 'Sciences, EPFL, Switzerland. Correspondence to: Martin Jaggi '\n",
      " '<martin.jaggi@epfl.ch>.',\n",
      " 'tence embeddings. This strongly contrasts all neural network based '\n",
      " 'approaches, and allows our model to learn from extremely large datasets, '\n",
      " 'which is a crucial advantage in the unsupervised setting.  Performance. Our '\n",
      " 'method shows significant performance improvements compared to the current '\n",
      " 'stateof-the-art unsupervised and even semi-supervised models. The resulting '\n",
      " 'general-purpose embeddings show strong robustness when transferred to a wide '\n",
      " 'range of prediction benchmarks.',\n",
      " 'Formally, we learn source v w and target uw embeddings for each word w in '\n",
      " 'the vocabulary, with embedding dimension h and k = |V| as in (0). The '\n",
      " 'sentence embedding is defined as the average of the source word embeddings '\n",
      " 'of its constituent words, as in (0). We augment this model furthermore by '\n",
      " 'also learning source embeddings for not only unigrams but also n-grams '\n",
      " 'present in each sentence, and averaging the n-gram embeddings along with the '\n",
      " 'words, i.e., the sentence embedding v S for S is modeled as v S :=',\n",
      " '0. Model Our model is inspired by simple matrix factor models (bilinear '\n",
      " 'models) such as recently very successfully used in unsupervised learning of '\n",
      " 'word embeddings (Mikolov et al., 0000b;a; Pennington et al., 0000; '\n",
      " 'Bojanowski et al., 0000) as well as supervised of sentence classification '\n",
      " '(Joulin et al., 0000). More precisely, these models are formalized as an '\n",
      " 'optimization problem of the form min',\n",
      " 'where R(S) is the list of n-grams (including unigrams) present in sentence '\n",
      " 'S. In order to predict a missing word from the context, our objective models '\n",
      " 'the softmax output approximated by negative sampling following (Mikolov et '\n",
      " 'al., 0000b). For the large number of output classes |V| to be predicted, '\n",
      " 'negative sampling is known to significantly improve training efficiency, see '\n",
      " 'also (Goldberg & Levy, 0000). Given the binary logistic loss function ` : x '\n",
      " '0 log (0 + ex ) coupled with negative sampling, our unsupervised training '\n",
      " 'objective is formulated as follows:',\n",
      " 'for two parameter matrices U  Rkh and V  Rh|V| , where V denotes the '\n",
      " 'vocabulary. In all models studied, the columns of the matrix V will collect '\n",
      " 'the learned word vectors, having h dimensions. For a given sentence S, which '\n",
      " 'can be of arbitrary length, the indicator vector S  {0, 0}|V| is a binary '\n",
      " 'vector encoding S (bag of words encoding). Fixed-length context windows S '\n",
      " 'running over the corpus are used in word embedding methods as in C-BOW '\n",
      " '(Mikolov et al., 0000b;a) and GloVe (Pennington et al., 0000). Here we have '\n",
      " 'k = |V| and each cost function fS : Rk  R only depends on a single row of '\n",
      " 'its input, describing the observed target word for the given fixed-length '\n",
      " 'context S. In contrast, for sentence embeddings which are the focus of our '\n",
      " 'paper here, S will be entire sentences or documents (therefore variable '\n",
      " 'length). This property is shared with the supervised FastText classifier '\n",
      " '(Joulin et al., 0000), which however uses soft-max with k  |V| being the '\n",
      " 'number of class labels. 0.0. Proposed Unsupervised Model We propose a new '\n",
      " 'unsupervised model, Sent0Vec, for learning universal sentence embeddings. '\n",
      " 'Conceptually, the model can be interpreted as a natural extension of the '\n",
      " 'wordcontexts from C-BOW (Mikolov et al., 0000b;a) to a larger sentence '\n",
      " 'context, with the sentence words being specifically optimized towards '\n",
      " 'additive combination over the sentence, by means of the unsupervised '\n",
      " 'objective function.',\n",
      " 'where S corresponds to the current sentence and Nwt is the set of words '\n",
      " 'sampled negatively for the word wt  S. The negatives are sampled0 following '\n",
      " 'a multinomial distribution where with a probability  word',\n",
      " ' each P w ispassociated fwi , where fw is the norqn (w) := fw wi V malized '\n",
      " 'frequency of w in the corpus. To select the possible target unigrams '\n",
      " '(positives), we use subsampling as in (Joulin et al., 0000; Bojanowski et '\n",
      " 'al., 0000), each word w being discarded 0  p with probability \\\\t qp (w) '\n",
      " 'where qp (w) := min 0, t/fw + t/fw . Where t is the subsampling '\n",
      " 'hyper-parameter. Subsampling prevents very frequent words of having too much '\n",
      " 'influence in the learning as they would introduce strong biases in the '\n",
      " 'prediction task. With positives subsampling and respecting the negative '\n",
      " 'sampling distribution, the precise training objective function becomes X X',\n",
      " 'To efficiently sample negatives, a pre-processing table is constructed, '\n",
      " 'containing the words corresponding to the square root of their corpora '\n",
      " 'frequency. Then, the negatives Nwt are sampled uniformly at random from the '\n",
      " 'negatives table except the target wt itself, following (Joulin et al., 0000; '\n",
      " 'Bojanowski et al., 0000).',\n",
      " '0.0. Computational Efficiency In contrast to more complex neural network '\n",
      " 'based models, one of the core advantages of the proposed technique is the '\n",
      " 'low computational cost for both inference and training. Given a sentence S '\n",
      " 'and a trained model, computing the sentence representation v S only requires '\n",
      " '|S|  h floating point operations (or |R(S)|  h to be precise for the n-gram '\n",
      " 'case, see (0)), where h is the embedding dimension. The same holds for the '\n",
      " 'cost of training with SGD on the objective (0), per sentence seen in the '\n",
      " 'training corpus. Due to the simplicity of the model, parallel training is '\n",
      " 'straight-forward using parallelized or distributed SGD. 0.0. Comparison to '\n",
      " 'C-BOW C-BOW (Mikolov et al., 0000b;a) tries to predict a chosen target word '\n",
      " 'given its fixed-size context window, the context being defined by the '\n",
      " 'average of the vectors associated with the words at a distance less than the '\n",
      " 'window size hyperparameter ws. If our system, when restricted to unigram '\n",
      " 'features, can be seen as an extension of C-BOW where the context window '\n",
      " 'includes the entire sentence, in practice there are few important '\n",
      " 'differences as C-BOW uses important tricks to facilitate the learning of '\n",
      " 'word embeddings. C-BOW first uses frequent word subsampling on the '\n",
      " 'sentences, deciding to discard each token w with probability qp (w) or alike '\n",
      " '(small variations exist across implementations). Subsampling prevents the '\n",
      " 'generation of n-grams features, and deprives the sentence of an important '\n",
      " 'part of its syntactical features. It also shortens the distance between '\n",
      " 'subsampled words, implicitly increasing the span of the context window. A '\n",
      " 'second trick consists of using dynamic context windows: for each subsampled '\n",
      " 'word w, the size of its associated context window is sampled uniformly '\n",
      " 'between 0 and ws. Using dynamic context windows is equivalent to weighing by '\n",
      " 'the distance from the focus word w divided by the window size (Levy et al., '\n",
      " '0000). This makes the prediction task local, and go against our objective of '\n",
      " 'creating sentence embeddings as we want to learn how to compose all n-gram '\n",
      " 'features present in a sentence. In the results section, we report a '\n",
      " 'significant improvement of our method over C-BOW. 0.0. Model Training Three '\n",
      " 'different datasets have been used to train our models: the Toronto book '\n",
      " 'corpus0 , Wikipedia sentences and tweets. The Wikipedia and Toronto books '\n",
      " 'sentences have been tokenized using the Stanford NLP library (Manning et '\n",
      " 'al., 0000), while for tweets we used the NLTK tweets tokenizer (Bird et al., '\n",
      " '0000). For training, we select a sentence randomly from the dataset and then '\n",
      " 'proceed to select all the 0',\n",
      " 'possible target unigrams using subsampling. We update the weights using SGD '\n",
      " 'with a linearly decaying learning rate. Also, to prevent overfitting, for '\n",
      " 'each sentence we use dropout on its list of n-grams R(S) \\\\\\\\ {U (S)}, where '\n",
      " 'U (S) is the set of all unigrams contained in sentence S. After empirically '\n",
      " 'trying multiple dropout schemes, we find that dropping K n-grams (n > 0) for '\n",
      " 'each sentence is giving superior results compared to dropping each token '\n",
      " 'with some fixed probability. This dropout mechanism would negatively impact '\n",
      " 'shorter sentences. The regularization can be pushed further by applying L0 '\n",
      " 'regularization to the word vectors. Encouraging sparsity in the embedding '\n",
      " 'vectors is particularly beneficial for high dimension h. The additional soft '\n",
      " 'thresholding in every SGD step adds negligible computational cost. See also '\n",
      " 'Appendix B. We train two models on each dataset, one with unigrams only and '\n",
      " 'one with unigrams and bigrams. All training parameters for the models are '\n",
      " 'provided in Table 0 in the supplementary material. Our C++ implementation '\n",
      " 'builds upon the FastText library (Joulin et al., 0000; Bojanowski et al., '\n",
      " '0000). We will make our code and pre-trained models available open-source.',\n",
      " '0. Related Work We discuss existing models which have been proposed to '\n",
      " 'construct sentence embeddings. While there is a large body of works in this '\n",
      " 'direction  several among these using e.g. labelled datasets of paraphrase '\n",
      " 'pairs to obtain sentence embeddings in a supervised manner (Wieting et al., '\n",
      " '0000b;a)  we here focus on unsupervised, task-independent models. While some '\n",
      " 'methods require ordered raw text i.e., a coherent corpus where the next '\n",
      " 'sentence is a logical continuation of the previous sentence, others rely '\n",
      " 'only on raw text i.e., an unordered collection of sentences. Finally we also '\n",
      " 'discuss alternative models built from structured data sources. 0.0. '\n",
      " 'Unsupervised Models Independent of Sentence Ordering The ParagraphVector '\n",
      " 'DBOW model (Le & Mikolov, 0000) is a log-linear model which is trained to '\n",
      " 'learn sentence as well as word embeddings and then use a softmax '\n",
      " 'distribution to predict words contained in the sentence given the sentence '\n",
      " 'vector representation. They also propose a different model ParagraphVector '\n",
      " 'DM where they use n-grams of consecutive words along with the sentence '\n",
      " 'vector representation to predict the next word. (Hill et al., 0000a) propose '\n",
      " 'a Sequential (Denoising) Autoencoder, S(D)AE. This model first introduces '\n",
      " 'noise in the input data: Firstly each word is deleted with probability p0 , '\n",
      " 'then for each non-overlapping bigram, words',\n",
      " 'are swapped with probability px . The model then uses an LSTM-based '\n",
      " 'architecture to retrieve the original sentence from the corrupted version. '\n",
      " 'The model can then be used to encode new sentences into vector '\n",
      " 'representations. In the case of p0 = px = 0, the model simply becomes a '\n",
      " 'Sequential Autoencoder. (Hill et al., 0000a) also propose a variant (S(D)AE '\n",
      " '+ embs.) in which the words are represented by fixed pre-trained word vector '\n",
      " 'embeddings. (Arora et al., 0000) propose a model in which sentences are '\n",
      " 'represented as a weighted average of fixed (pre-trained) word vectors, '\n",
      " 'followed by post-processing step of subtracting the principal component. '\n",
      " 'Using the generative model of (Arora et al., 0000), words are generated '\n",
      " 'conditioned on a sentence discourse vector cs : P r[w | cs ] = fw + (0  )',\n",
      " 'where Zcs := and cs := c0 + (0  )cs and ,  are scalars. c0 is the common '\n",
      " 'discourse vector, representing a shared component among all discourses, '\n",
      " 'mainly related to syntax. It allows the model to better generate syntactical '\n",
      " 'features. The fw term is here to enable the model to generate some frequent '\n",
      " 'words even if their matching with the discourse vector cs is low. Therefore, '\n",
      " 'this model tries to generate sentences as a mixture of three type of words: '\n",
      " 'words matching the sentence discourse vector cs , syntactical words matching '\n",
      " 'c0 , and words with high fw . (Arora et al., 0000) demonstrated thatPfor '\n",
      " 'this model, the MLE of cs can be approximated by wS fwa+a v w , where a is a '\n",
      " 'scalar. The sentence discourse vector can hence be obtained by subtracting '\n",
      " 'c0 estimated by the first principal component of cs s on a set of sentences. '\n",
      " 'In other words, the sentence embeddings are obtained by a weighted average '\n",
      " 'of the word vectors stripping away the syntax by subtracting the common '\n",
      " 'discourse vector and down-weighting frequent tokens. They generate sentence '\n",
      " 'embeddings from diverse pre-trained word embeddings among which are '\n",
      " 'unsupervised word embeddings such as GloVe (Pennington et al., 0000) as well '\n",
      " 'as supervised word embeddings such as paragram-SL000 (PSL) (Wieting et al., '\n",
      " '0000) trained on the Paraphrase Database (Ganitkevitch et al., 0000). In a '\n",
      " 'very different line of work, C-PHRASE (Pham et al., 0000) relies on '\n",
      " 'additional information from the syntactic parse tree of each sentence, which '\n",
      " 'is incorporated into the C-BOW training objective. (Huang & Anandkumar, '\n",
      " '0000) show that single layer CNNs can be modeled using a tensor '\n",
      " 'decomposition approach. While building on an unsupervised objective, the '\n",
      " 'employed dictionary learning step for obtaining phrase templates is '\n",
      " 'task-specific (for each use-case), not resulting in general-purpose '\n",
      " 'embeddings.',\n",
      " '0.0. Unsupervised Models Depending on Sentence Ordering The SkipThought '\n",
      " 'model (Kiros et al., 0000) combines sentence level models with recurrent '\n",
      " 'neural networks. Given a sentence Si from an ordered corpus, the model is '\n",
      " 'trained to predict Si0 and Si+0 . FastSent (Hill et al., 0000a) is a '\n",
      " 'sentence-level log-linear bag-of-words model. Like SkipThought, it uses '\n",
      " 'adjacent sentences as the prediction target and is trained in an '\n",
      " 'unsupervised fashion. Using word sequences allows the model to improve over '\n",
      " 'the earlier work of paragraph0vec (Le & Mikolov, 0000). (Hill et al., 0000a) '\n",
      " 'augment FastSent further by training it to predict the constituent words of '\n",
      " 'the sentence as well. This model is named FastSent + AE in our comparisons. '\n",
      " 'Compared to our approach, Siamese C-BOW (Kenter et al., 0000) shares the '\n",
      " 'idea of learning to average word embeddings over a sentence. However, it '\n",
      " 'relies on a Siamese neural network architecture to predict surrounding '\n",
      " 'sentences, contrasting our simpler unsupervised objective. Note that on the '\n",
      " 'character sequence level instead of word sequences, FastText (Bojanowski et '\n",
      " 'al., 0000) uses the same conceptual model to obtain better word embeddings. '\n",
      " 'This is most similar to our proposed model, with two key differences: '\n",
      " 'Firstly, we predict from source word sequences to target words, as opposed '\n",
      " 'to character sequences to target words, and secondly, our model is averaging '\n",
      " 'the source embeddings instead of summing them. 0.0. Models requiring '\n",
      " 'structured data DictRep (Hill et al., 0000b) is trained to map dictionary '\n",
      " 'definitions of the words to the pre-trained word embeddings of these words. '\n",
      " 'They use two different architectures, namely BOW and RNN (LSTM) with the '\n",
      " 'choice of learning the input word embeddings or using them pre-trained. A '\n",
      " 'similar architecture is used by the CaptionRep variant, but here the task is '\n",
      " 'the mapping of given image captions to a pre-trained vector representation '\n",
      " 'of these images.',\n",
      " '0. Evaluation Tasks We use a standard set of supervised as well as '\n",
      " 'unsupervised benchmark tasks from the literature to evaluate our trained '\n",
      " 'models, following (Hill et al., 0000a). The breadth of tasks allows to '\n",
      " 'fairly measure generalization to a wide area of different domains, testing '\n",
      " 'the general-purpose quality (universality) of all competing sentence '\n",
      " 'embeddings. For downstream supervised evaluations, sentence embeddings are '\n",
      " 'combined with logistic regression to predict target labels. In the '\n",
      " 'unsupervised evaluation for sentence similarity, correlation of the cosine '\n",
      " 'similarity between two em-',\n",
      " 'beddings is compared to human annotators. Downstream Supervised Evaluation. '\n",
      " 'Sentence embeddings are evaluated for various supervised classification '\n",
      " 'tasks as follows. We evaluate paraphrase identification (MSRP) (Dolan et '\n",
      " 'al., 0000), classification of movie review sentiment (MR) (Pang & Lee, '\n",
      " '0000), product reviews (CR) (Hu & Liu, 0000), subjectivity classification '\n",
      " '(SUBJ)(Pang & Lee, 0000), opinion polarity (MPQA) (Wiebe et al., 0000) and '\n",
      " 'question type classification (TREC) (Voorhees, 0000). To classify, we use '\n",
      " 'the code provided by (Kiros et al., 0000) in the same manner as in (Hill et '\n",
      " 'al., 0000a). For the MSRP dataset, containing pairs of sentences (S0 , S0 ) '\n",
      " 'with associated paraphrase label, we generate feature vectors by '\n",
      " 'concatenating their Sent0Vec representations |v S0  v S0 | with the '\n",
      " 'component-wise product v S0  v S0 . The predefined training split is used to '\n",
      " 'tune the L0 penalty parameter using cross-validation and the accuracy and F0 '\n",
      " 'scores are computed on the test set. For the remaining 0 datasets, Sent0Vec '\n",
      " 'embeddings are inferred from input sentences and directly fed to a logistic '\n",
      " 'regression classifier. Accuracy scores are obtained using 00-fold '\n",
      " 'cross-validation for the MR, CR, SUBJ and MPQA datasets. For those datasets '\n",
      " 'nested cross-validation is used to tune the L0 penalty. For the TREC '\n",
      " 'dataset, as for the MRSP dataset, the L0 penalty is tuned on the predefined '\n",
      " 'train split using 00-fold cross-validation, and the accuracy is computed on '\n",
      " 'the test set. Unsupervised Similarity Evaluation. We perform unsupervised '\n",
      " 'evaluation of the the learnt sentence embeddings using the sentence cosine '\n",
      " 'similarity, on the STS 0000 (Agirre et al., 0000) and SICK 0000 (Marelli et '\n",
      " 'al., 0000) datasets. These similarity scores are compared to the '\n",
      " 'gold-standard human judgements using Pearsons r (Pearson, 0000) and '\n",
      " 'Spearmans  (Spearman, 0000) correlation scores. The SICK dataset consists of '\n",
      " 'about 00,000 sentence pairs along with relatedness scores of the pairs. The '\n",
      " 'STS 0000 dataset contains 0,000 pairs, divided into six different categories '\n",
      " 'on the basis of origin of sentences/phrases namely Twitter, headlines, news, '\n",
      " 'forum, WordNet and images. See (Agirre et al., 0000) for more precise '\n",
      " 'information on how the pairs have been created.',\n",
      " '0. Results and Discussion In Tables 0 and 0, we compare our results with '\n",
      " 'those obtained by (Hill et al., 0000a) on different models. Along with the '\n",
      " 'models discussed in Section 0, this also includes the sentence embedding '\n",
      " 'baselines obtained by simple averaging of word embeddings over the sentence, '\n",
      " 'in both the C-BOW and skip-gram variants. TF-IDF BOW is a representation '\n",
      " 'consisting of the counts of the 000,000 most common feature-words, weighed '\n",
      " 'by their TF-IDF frequencies. To ensure coherence, we only include '\n",
      " 'unsupervised mod-',\n",
      " 'els in the main paper. Performance of supervised and semisupervised models '\n",
      " 'on these evaluations can be observed in Tables 0 and 0 in the supplementary '\n",
      " 'material. Downstream Supervised Evaluation Results. On running supervised '\n",
      " 'evaluations and observing the results in Table 0, we find that on an average '\n",
      " 'our models are second only to SkipThought vectors. Also, both our models '\n",
      " 'achieve state of the art results on the CR task. We also observe that on '\n",
      " 'half of the supervised tasks, our unigrams + bigram model is the the best '\n",
      " 'model after SkipThought. Our models are weaker on the MSRP task (which '\n",
      " 'consists of the identification of labelled paraphrases) compared to '\n",
      " 'stateof-the-art methods. However, we observe that the models which perform '\n",
      " 'extremely well on this task end up faring very poorly on the other tasks, '\n",
      " 'indicating a lack of generalizability. On rest of the tasks, our models '\n",
      " 'perform extremely well. The SkipThought model is able to outperform our '\n",
      " 'models on most of the tasks as it is trained to predict the previous and '\n",
      " 'next sentences and a lot of tasks are able to make use of this contextual '\n",
      " 'information missing in our Sent0Vec models. For example, the TREC task is a '\n",
      " 'poor measure of how one predicts the content of the sentence (the question) '\n",
      " 'but a good measure of how the next sentence in the sequence (the answer) is '\n",
      " 'predicted. Unsupervised Similarity Evaluation Results. In Table 0, we see '\n",
      " 'that our Sent0Vec models are state-of-the-art on the majority of tasks when '\n",
      " 'comparing to all the unsupervised models trained on the Toronto corpus, and '\n",
      " 'clearly achieve the best averaged performance. Our Sent0Vec models also on '\n",
      " 'average outperform or are at par with the C-PHRASE model, despite '\n",
      " 'significantly lagging behind on the STS 0000 WordNet and News subtasks. This '\n",
      " 'observation can be attributed to the fact that a big chunk of the data that '\n",
      " 'the C-PHRASE model is trained on comes from English Wikipedia, helping it to '\n",
      " 'perform well on datasets involving definition and news items. Also, C-PHRASE '\n",
      " 'uses data three times the size of the Toronto book corpus. Interestingly, '\n",
      " 'our model outperforms C-PHRASE when trained on Wikipedia, as shown in Table '\n",
      " '0, despite the fact that we use no parse tree information. In the official '\n",
      " 'results of the more recent edition of the STS 0000 benchmark (Cer et al., '\n",
      " '0000), our model also significantly outperforms C-PHRASE, and delivers the '\n",
      " 'best unsupervised baseline method. Macro Average. To summarize our '\n",
      " 'contributions on both supervised and unsupervised tasks, in Table 0 we '\n",
      " 'present the results in terms of the macro average over the averages 0 For '\n",
      " 'the Siamese C-BOW model trained on the Toronto corpus, supervised evaluation '\n",
      " 'as well as similarity evaluation results on the SICK 0000 dataset are '\n",
      " 'unavailable.',\n",
      " 'Model SAE SAE + embs. SDAE SDAE + embs. ParagraphVec DBOW ParagraphVec DM '\n",
      " 'Skipgram C-BOW Unigram TFIDF Sent0Vec uni. Sent0Vec uni. + bi. SkipThought '\n",
      " 'FastSent FastSent+AE C-PHRASE',\n",
      " 'MSRP (Acc / F0) 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 '\n",
      " '00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 '\n",
      " '/ 00.0 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0',\n",
      " 'Table 0: Comparison of the performance of different models on different '\n",
      " 'supervised evaluation tasks. An underline indicates the best performance for '\n",
      " 'the dataset. Top 0 performances in each data category are shown in bold. The '\n",
      " 'average is calculated as the average of accuracy for each category (For '\n",
      " 'MSRP, we take the average of two entries.) Model SAE SAE + embs. SDAE SDAE + '\n",
      " 'embs. ParagraphVec DBOW ParagraphVec DM Skipgram C-BOW Unigram TF-IDF '\n",
      " 'Sent0Vec uni. Sent0Vec uni. + bi. SkipThought FastSent FastSent+AE Siamese '\n",
      " 'C-BOW0 C-PHRASE',\n",
      " 'News .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'Forum .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'STS 0000 WordNet Twitter .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'Images .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'Headlines .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'SICK 0000 Test + Train .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'Average .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'Table 0: Unsupervised Evaluation Tasks: Comparison of the performance of '\n",
      " 'different models on Spearman/Pearson correlation measures. An underline '\n",
      " 'indicates the best performance for the dataset. Top 0 performances in each '\n",
      " 'data category are shown in bold. The average is calculated as the average of '\n",
      " 'entries for each correlation measure. of both supervised and unsupervised '\n",
      " 'tasks along with the training times of the models0 . For unsupervised tasks, '\n",
      " 'averages are taken over both Spearman and Pearson scores. The comparison '\n",
      " 'includes the best performing unsupervised and semi-supervised methods '\n",
      " 'described in Section 0. For models trained on the Toronto books dataset, we '\n",
      " 'report a 0.0 % points improvement over the state of the art. Considering all '\n",
      " 'supervised, semi-supervised methods and all datasets compared in (Hill et '\n",
      " 'al., 0000a), we report a 0.0 % points improvement. We also see a noticeable '\n",
      " 'improvement in accuracy as we use larger datasets like twitter and Wikipedia '\n",
      " 'dump. We can also see that the Sent0Vec models are also faster to train when '\n",
      " 'compared to methods like SkipThought and DictRep owing to the SGD step '\n",
      " 'allowing a high degree of parallelizability. We can clearly see Sent0Vec '\n",
      " 'outperforming other unsupervised and even semi-supervised methods. This can '\n",
      " 'be at0',\n",
      " 'tributed to the superior generalizability of our model across supervised and '\n",
      " 'unsupervised tasks. Comparison with Arora et al. (0000). In Table 0, we '\n",
      " 'report an experimental comparison to the model of Arora et al. (0000), which '\n",
      " 'is particularly tailored to sentence similarity tasks. In the table, the '\n",
      " 'suffix W indicates that their down-weighting scheme has been used, while the '\n",
      " 'suffix R indicates the removal of the first principal component. They report '\n",
      " 'values of a  [000 , 000 ] as giving the best results and used a = 000 for '\n",
      " 'all their experiments. Their down-weighting scheme hints us to reduce the '\n",
      " 'importance of syntactical features. To do so, we use a simple blacklist '\n",
      " 'containing the 00 most frequent tokens in the Twitter corpus and discard '\n",
      " 'them before averaging. Results are also reported in Table 0. We observe that '\n",
      " 'our results are competitive with the embeddings of Arora et al. (0000) for '\n",
      " 'purely unsupervised methods. We confirm their empirical finding that '\n",
      " 'reducing the influence of the syntax helps performance on semantic sim-',\n",
      " 'unsupervised unsupervised unsupervised unsupervised unsupervised '\n",
      " 'unsupervised semi-supervised unsupervised unsupervised unsupervised '\n",
      " 'unsupervised',\n",
      " 'twitter (00.0B words) twitter (00.0B words) Wikipedia (0.0B words) Wikipedia '\n",
      " '(0.0B words) Toronto books (0.0B words) Toronto books (0.0B words) '\n",
      " 'structured dictionary dataset 0.0B words + parse info. Toronto books (0.0B '\n",
      " 'words) Toronto books (0.0B words) Toronto books (0.0B words)',\n",
      " 'Sent0Vec uni. + bi. Sent0Vec uni. Sent0Vec uni. + bi. Sent0Vec uni. Sent0Vec '\n",
      " 'books uni. Sent0Vec books uni. + bi. DictRep BOW + emb C-PHRASE C-BOW '\n",
      " 'FastSent SkipThought',\n",
      " 'Table 0: Best unsupervised and semi-supervised methods ranked by macro '\n",
      " 'average along with their training times. ** indicates trained on GPU. * '\n",
      " 'indicates trained on a single node using 00 cores. Training times for '\n",
      " 'non-Sent0Vec models are due to (Hill et al., 0000a) Dataset STS 0000 SICK '\n",
      " '0000',\n",
      " 'Table 0: Comparison of the performance of the unsupervised and '\n",
      " 'semi-supervised sentence embeddings by (Arora et al., 0000) with our models, '\n",
      " 'in terms of Pearsons correlation. to unsupervised evaluations but gives a '\n",
      " 'significant boost-up in accuracy on supervised tasks.',\n",
      " 'Figure 0: Left figure: the profile of the word vector L0 norms as a function '\n",
      " 'of log(fw ) for each vocabulary word w, as learnt by our unigram model '\n",
      " 'trained on Toronto books. Right figure: down-weighting scheme proposed by a '\n",
      " '. Arora et al. (0000): weight(w) = a+f w',\n",
      " 'ilarity tasks, and we show that applying a simple blacklist already yields a '\n",
      " 'noticeable amelioration. It is important to note that the scores obtained '\n",
      " 'from supervised task-specific PSL embeddings trained for the purpose of '\n",
      " 'semantic similarity outperform our method on both SICK and average STS 0000, '\n",
      " 'which is expected as our model is trained purely unsupervised. The effect of '\n",
      " 'datasets and n-grams. Despite being trained on three very different '\n",
      " 'datasets, all of our models generalize well to sometimes very specific '\n",
      " 'domains. Models trained on Toronto Corpus are the state-of-the art on the '\n",
      " 'STS 0000 images dataset even beating the supervised CaptionRep model trained '\n",
      " 'on images. We also see that addition of bigrams to our models doesnt help '\n",
      " 'much when it comes',\n",
      " 'On learning the importance and the direction of the word vectors. Our model  '\n",
      " 'by learning how to generate and compose word vectors  has to learn both the '\n",
      " 'direction of the word embeddings as well as their norm. Considering the '\n",
      " 'norms of the used word vectors as by our averaging over the sentence, we '\n",
      " 'observe an interesting distribution of the importance of each word. In '\n",
      " 'Figure 0 we show the profile of the L0 -norm as a function of log(fw ) for '\n",
      " 'each w  V, and compare it to the static down-weighting mechanism of Arora et '\n",
      " 'al. (0000). We can observe that our model is learning to down-weight '\n",
      " 'frequent tokens by itself. It is also down-weighting rare tokens and the '\n",
      " 'norm profile seems to roughly follow Luhns hypothesis (Luhn, 0000), a well '\n",
      " 'known information retrieval paradigm, stating that mid-rank terms are the '\n",
      " 'most significant to discriminate content. Modifying the objective function '\n",
      " 'would change the weighting scheme learnt. From a more semantic oriented '\n",
      " 'objective, it should be possible to learn to attribute lower norms for very '\n",
      " 'frequent terms, to more specifically fit sentence similarity tasks.',\n",
      " '0. Conclusion In this paper, we introduced a novel unsupervised and '\n",
      " 'computationally efficient method to train and infer sentence embeddings. On '\n",
      " 'supervised evaluations, our method, on an average, achieves better '\n",
      " 'performance than all other unsupervised competitors except the SkipThought '\n",
      " 'vectors. However, SkipThought vectors show an extremely poor performance on '\n",
      " 'sentence similarity tasks while our model',\n",
      " 'is state-of-the-art for these evaluations on average. Future work could '\n",
      " 'focus on augmenting the model to exploit data with ordered sentences. '\n",
      " 'Furthermore, we would like to further investigate the models ability as '\n",
      " 'giving pre-trained embeddings to enable downstream transfer learning tasks. '\n",
      " 'Acknowledgments. We are indebted to Piotr Bojanowski and Armand Joulin for '\n",
      " 'helpful discussions.',\n",
      " 'References Agirre, Eneko, Banea, Carmen, Cardie, Claire, Cer, Daniel, Diab, '\n",
      " 'Mona, Gonzalez-Agirre, Aitor, Guo, Weiwei, Mihalcea, Rada, Rigau, German, '\n",
      " 'and Wiebe, Janyce. Semeval-0000 task 00: Multilingual semantic textual '\n",
      " 'similarity. In Proceedings of the 0th international workshop on semantic '\n",
      " 'evaluation (SemEval 0000), pp. 0000. Association for Computational '\n",
      " 'Linguistics Dublin, Ireland, 0000. Arora, Sanjeev, Li, Yuanzhi, Liang, '\n",
      " 'Yingyu, Ma, Tengyu, and Risteski, Andrej. A Latent Variable Model Approach '\n",
      " 'to PMIbased Word Embeddings. In Transactions of the Association for '\n",
      " 'Computational Linguistics, pp. 000000, July 0000. Arora, Sanjeev, Liang, '\n",
      " 'Yingyu, and Ma, Tengyu. A simple but tough-to-beat baseline for sentence '\n",
      " 'embeddings. In International Conference on Learning Representations (ICLR), '\n",
      " '0000. Bird, Steven, Klein, Ewan, and Loper, Edward. Natural language '\n",
      " 'processing with Python: analyzing text with the natural language toolkit.  '\n",
      " 'OReilly Media, Inc., 0000. Bojanowski, Piotr, Grave, Edouard, Joulin, '\n",
      " 'Armand, and Mikolov, Tomas. Enriching Word Vectors with Subword Information. '\n",
      " 'Transactions of the Association for Computational Linguistics, 0:000000, '\n",
      " '0000. Cer, Daniel, Diab, Mona, Agirre, Eneko, Lopez-Gazpio, Inigo, and '\n",
      " 'Specia, Lucia. SemEval-0000 Task 0: Semantic Textual Similarity Multilingual '\n",
      " 'and Cross-lingual Focused Evaluation. In SemEval-0000 - Proceedings of the '\n",
      " '00th International Workshop on Semantic Evaluations, pp. 000, Vancouver, '\n",
      " 'Canada, August 0000. Association for Computational Linguistics. Dolan, Bill, '\n",
      " 'Quirk, Chris, and Brockett, Chris. Unsupervised construction of large '\n",
      " 'paraphrase corpora: Exploiting massively parallel news sources. In '\n",
      " 'Proceedings of the 00th international conference on Computational '\n",
      " 'Linguistics, pp. 000. Association for Computational Linguistics, 0000. '\n",
      " 'Ganitkevitch, Juri, Van Durme, Benjamin, and Callison-Burch, Chris. Ppdb: '\n",
      " 'The paraphrase database. In HLT-NAACL, pp. 000000, 0000. Goldberg, Yoav and '\n",
      " 'Levy, Omer. word0vec Explained: deriving Mikolov et al.s negative-sampling '\n",
      " 'word-embedding method. arXiv, February 0000.',\n",
      " 'Hu, Minqing and Liu, Bing. Mining and summarizing customer reviews. In '\n",
      " 'Proceedings of the tenth ACM SIGKDD international conference on Knowledge '\n",
      " 'discovery and data mining, pp. 000000. ACM, 0000. Huang, Furong and '\n",
      " 'Anandkumar, Animashree. Unsupervised Learning of Word-Sequence '\n",
      " 'Representations from Scratch via Convolutional Tensor Decomposition. arXiv, '\n",
      " '0000. Joulin, Armand, Grave, Edouard, Bojanowski, Piotr, and Mikolov, Tomas. '\n",
      " 'Bag of Tricks for Efficient Text Classification. In Proceedings of the 00th '\n",
      " 'Conference of the European Chapter of the Association for Computational '\n",
      " 'Linguistics, Short Papers, pp. 000000, Valencia, Spain, 0000. Kenter, Tom, '\n",
      " 'Borisov, Alexey, and de Rijke, Maarten. Siamese CBOW: Optimizing Word '\n",
      " 'Embeddings for Sentence Representations. In ACL - Proceedings of the 00th '\n",
      " 'Annual Meeting of the Association for Computational Linguistics, pp. 000000, '\n",
      " 'Berlin, Germany, 0000. Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan R, '\n",
      " 'Zemel, Richard, Urtasun, Raquel, Torralba, Antonio, and Fidler, Sanja. '\n",
      " 'Skip-Thought Vectors. In NIPS 0000 - Advances in Neural Information '\n",
      " 'Processing Systems 00, pp. 00000000, 0000. Le, Quoc V and Mikolov, Tomas. '\n",
      " 'Distributed Representations of Sentences and Documents. In ICML 0000 - '\n",
      " 'Proceedings of the 00st International Conference on Machine Learning, volume '\n",
      " '00, pp. 00000000, 0000. Levy, Omer, Goldberg, Yoav, and Dagan, Ido. '\n",
      " 'Improving distributional similarity with lessons learned from word '\n",
      " 'embeddings. Transactions of the Association for Computational Linguistics, '\n",
      " '0:000000, 0000. Luhn, Hans Peter. The automatic creation of literature '\n",
      " 'abstracts. IBM Journal of research and development, 0(0):000 000, 0000. '\n",
      " 'Manning, Christopher D, Surdeanu, Mihai, Bauer, John, Finkel, Jenny Rose, '\n",
      " 'Bethard, Steven, and McClosky, David. The stanford corenlp natural language '\n",
      " 'processing toolkit. In ACL (System Demonstrations), pp. 0000, 0000. Marelli, '\n",
      " 'Marco, Menini, Stefano, Baroni, Marco, Bentivogli, Luisa, Bernardi, '\n",
      " 'Raffaella, and Zamparelli, Roberto. A sick cure for the evaluation of '\n",
      " 'compositional distributional semantic models. In LREC, pp. 000000, 0000. '\n",
      " 'Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Efficient '\n",
      " 'estimation of word representations in vector space. arXiv preprint '\n",
      " 'arXiv:0000.0000, 0000a. Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, '\n",
      " 'Greg S, and Dean, Jeff. Distributed Representations of Words and Phrases and '\n",
      " 'their Compositionality. In NIPS - Advances in Neural Information Processing '\n",
      " 'Systems 00, pp. 00000000, 0000b.',\n",
      " 'Hill, Felix, Cho, Kyunghyun, and Korhonen, Anna. Learning Distributed '\n",
      " 'Representations of Sentences from Unlabelled Data. In Proceedings of '\n",
      " 'NAACL-HLT, February 0000a.',\n",
      " 'Pang, Bo and Lee, Lillian. A sentimental education: Sentiment analysis using '\n",
      " 'subjectivity summarization based on minimum cuts. In Proceedings of the 00nd '\n",
      " 'annual meeting on Association for Computational Linguistics, pp. 000. '\n",
      " 'Association for Computational Linguistics, 0000.',\n",
      " 'Hill, Felix, Cho, KyungHyun, Korhonen, Anna, and Bengio, Yoshua. Learning to '\n",
      " 'understand phrases by embedding the dictionary. TACL, 0:0000, 0000b. URL '\n",
      " 'https://tacl0000.cs.columbia.edu/ojs/ index.php/tacl/article/view/000.',\n",
      " 'Pang, Bo and Lee, Lillian. Seeing stars: Exploiting class relationships for '\n",
      " 'sentiment categorization with respect to rating scales. In Proceedings of '\n",
      " 'the 00rd annual meeting on association for computational linguistics, pp. '\n",
      " '000000. Association for Computational Linguistics, 0000.',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features Pearson, Karl. Note on regression and inheritance in the case of '\n",
      " 'two parents. Proceedings of the Royal Society of London, 00: 000000, 0000. '\n",
      " 'Pennington, Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: '\n",
      " 'Global vectors for word representation. In EMNLP, volume 00, pp. 00000000, '\n",
      " '0000. Pham, NT, Kruszewski, G, Lazaridou, A, and Baroni, M. Jointly '\n",
      " 'optimizing word representations for lexical and sentential tasks with the '\n",
      " 'c-phrase model. ACL/IJCNLP, 0000. Rockafellar, R Tyrrell. Monotone operators '\n",
      " 'and the proximal point algorithm. SIAM journal on control and optimization, '\n",
      " '00(0):000000, 0000. Spearman, Charles. The proof and measurement of '\n",
      " 'association between two things. The American journal of psychology, 00 '\n",
      " '(0):00000, 0000. Voorhees, Ellen M. Overview of the trec 0000 question '\n",
      " 'answering track. In NIST special publication, pp. 0000, 0000. Wiebe, Janyce, '\n",
      " 'Wilson, Theresa, and Cardie, Claire. Annotating expressions of opinions and '\n",
      " 'emotions in language. Language resources and evaluation, 00(0):000000, 0000. '\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, Livescu, Karen, and Roth, Dan. '\n",
      " 'From paraphrase database to compositional paraphrase model and back. In TACL '\n",
      " '- Transactions of the Association for Computational Linguistics, 0000. '\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen. Towards '\n",
      " 'universal paraphrastic sentence embeddings. In International Conference on '\n",
      " 'Learning Representations (ICLR), 0000a. Wieting, John, Bansal, Mohit, '\n",
      " 'Gimpel, Kevin, and Livescu, Karen. Charagram: Embedding Words and Sentences '\n",
      " 'via Character n-grams. In EMNLP - Proceedings of the 0000 Conference on '\n",
      " 'Empirical Methods in Natural Language Processing, pp. 00000000, Stroudsburg, '\n",
      " 'PA, USA, 0000b. Association for Computational Linguistics.',\n",
      " 'Supplementary Material A. Parameters for training models Model Book corpus '\n",
      " 'Sent0Vec unigrams Book corpus Sent0Vec unigrams + bigrams Wiki Sent0Vec '\n",
      " 'unigrams Wiki Sent0Vec unigrams + bigrams Twitter Sent0Vec unigrams Twitter '\n",
      " 'Sent0Vec unigrams + bigrams',\n",
      " 'B. L0 regularization of models Optionally, our model can be additionally '\n",
      " 'improved by adding an L0 regularizer term in the objective function, leading '\n",
      " 'to slightly better generalization performance. Additionally, encouraging '\n",
      " 'sparsity in the embedding vectors is beneficial for memory reasons, allowing '\n",
      " 'higher embedding dimensions h. We propose to apply L0 regularization '\n",
      " 'individually to each word (and n-gram) vector (both source and target '\n",
      " 'vectors). Formally, the training objective function (0) then becomes',\n",
      " 'where  is the regularization parameter. Now, in order to minimize a function '\n",
      " 'of the form f (z) + g(z) where g(z) is not differentiable over the domain, '\n",
      " 'we can use the basic proximal-gradient scheme. In this iterative method, '\n",
      " 'after doing a gradient descent step on f (z) with learning rate , we update '\n",
      " 'z as zn+0 = prox,g (zn+ 00 )',\n",
      " '0 where prox,g (x) = arg miny {g(y) + 0 ky  xk00 } is called the proximal '\n",
      " 'function(Rockafellar, 0000) of g with  being the proximal parameter and zn+ '\n",
      " '00 is the value of z after a gradient (or SGD) step on zn .',\n",
      " 'In our case, g(z) = kzk0 and the corresponding proximal operator is given by '\n",
      " 'prox,g (x) = sign(x)  max(|xn |  , 0)',\n",
      " 'where  corresponds to element-wise product. Similar to the proximal-gradient '\n",
      " 'scheme, in our case we can optionally use the thresholding operator on the '\n",
      " 'updated word  lr 0 and n-gram vectors after an SGD step. The soft '\n",
      " 'thresholding parameter used for this update is |R(S\\\\\\\\{w and   lr0 for t '\n",
      " '})| 0 the source and target vectors respectively where lr is the current '\n",
      " 'learning rate,  is the L0 regularization parameter and S is the sentence on '\n",
      " 'which SGD is being run.',\n",
      " 'We observe that L0 regularization using the proximal step gives our models a '\n",
      " 'small boost in performance. Also, applying the thresholding operator takes '\n",
      " 'only |R(S\\\\\\\\{wt })|h floating point operations for the updating the word '\n",
      " 'vectors corresponding to the sentence and (|N | + 0)  h for updating the '\n",
      " 'target as well as the negative word vectors, where |N | is the number of '\n",
      " 'negatives sampled and h is the embedding dimension. Thus, performing L0 '\n",
      " 'regularization using soft-thresholding operator comes with a small '\n",
      " 'computational overhead. We set  to be 0.0000 for both the Wikipedia and the '\n",
      " 'Toronto Book Corpus unigrams + bigrams models.',\n",
      " 'C. Performance comparison with Sent0Vec models trained on different corpora '\n",
      " 'Data Unordered Sentences: (Toronto Books) Unordered sentences: Wikipedia (00 '\n",
      " 'million sentences; 0.0 B words) Unordered sentences: Twitter (0.0 billion '\n",
      " 'sentences; 00.0 B words)',\n",
      " 'Model Sent0Vec uni. Sent0Vec uni. + bi. Sent0Vec uni. + bi.L0reg Sent0Vec '\n",
      " 'uni. Sent0Vec uni. + bi. Sent0Vec uni. + bi.L0reg Sent0Vec uni. Sent0Vec '\n",
      " 'uni. + bi. CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW+embs '\n",
      " 'DictRep RNN DictRep RNN+embs.',\n",
      " 'MSRP (Acc / F0) 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 '\n",
      " '00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 / 00.0 00.0 '\n",
      " '/ 00.0 00.0 / 00.0 00.0 / 00.0',\n",
      " 'Table 0: Comparison of the performance of different Sent0Vec models with '\n",
      " 'different semi-supervised/supervised models on different downstream '\n",
      " 'supervised evaluation tasks. An underline indicates the best performance for '\n",
      " 'the dataset and Sent0Vec model performances are bold if they perform as well '\n",
      " 'or better than all other non-Sent0Vec models, including those presented in '\n",
      " 'Table 0.',\n",
      " 'Model Sent0Vec book corpus uni. Sent0Vec book corpus uni. + bi. Sent0Vec '\n",
      " 'book corpus uni. + bi. L0 reg Sent0Vec wiki uni. Sent0Vec wiki uni. + bi. '\n",
      " 'Sent0Vec wiki uni. + bi. L0 reg Sent0Vec twitter uni. Sent0Vec twitter uni. '\n",
      " '+ bi. CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW + embs. DictRep '\n",
      " 'RNN DictRep RNN + embs.',\n",
      " 'News .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'Forum .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'STS 0000 WordNet Twitter .00/.00. .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'Images .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'Headlines .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'SICK 0000 Test + Train .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'Average .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00 '\n",
      " '.00/.00 .00/.00 .00/.00 .00/.00 .00/.00 .00/.00',\n",
      " 'Table 0: Unsupervised Evaluation: Comparison of the performance of different '\n",
      " 'Sent0Vec models with semisupervised/supervised models on Spearman/Pearson '\n",
      " 'correlation measures. An underline indicates the best performance for the '\n",
      " 'dataset and Sent0Vec model performances are bold if they perform as well or '\n",
      " 'better than all other non-Sent0Vec models, including those presented in '\n",
      " 'Table 0.']\n"
     ]
    }
   ],
   "source": [
    "# Replace all digits to '0'\n",
    "parahgraphs = [re.sub('[0-9]','0',p) for p in parahgraphs]\n",
    "pprint(parahgraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3. Sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abstract The recent tremendous success of unsupervised word embeddings in a '\n",
      " 'multitude of applications raises the obvious question if similar methods '\n",
      " 'could be derived to improve embeddings (i.e.',\n",
      " 'We present a simple but efficient unsupervised objective to train '\n",
      " 'distributed representations of sentences.',\n",
      " 'Our method outperforms the state-of-the-art unsupervised models on most '\n",
      " 'benchmark tasks, highlighting the robustness of the produced general-purpose '\n",
      " 'sentence embeddings.',\n",
      " 'Introduction Improving unsupervised learning is of key importance for '\n",
      " 'advancing machine learning methods, as to unlock access to almost unlimited '\n",
      " 'amounts of data to be used as training resources.',\n",
      " 'The majority of recent success stories of deep learning does not fall into '\n",
      " 'this category but instead relied on supervised training (in particular in '\n",
      " 'the vision domain).',\n",
      " 'A very notable exception comes from the text and natural language processing '\n",
      " 'domain, in the form of semantic word embeddings trained unsupervised '\n",
      " '(Mikolov et al., 0000b;a; Pennington et al., 0000).',\n",
      " 'Within only a few years from their invention, such word representations  '\n",
      " 'which are based on a simple matrix factorization model as we formalize '\n",
      " 'below  are now routinely trained on very large amounts of raw text data, and '\n",
      " 'have become ubiquitous building blocks of a majority of current '\n",
      " 'state-of-the-art NLP applications.',\n",
      " 'While extremely strong in expressiveness, the increased model complexity '\n",
      " 'makes such models much slower to train on larger datasets.',\n",
      " 'On the other end of the spectrum, simpler shallow models such as matrix '\n",
      " 'factorizations (or bilinear models) can benefit from training on much larger '\n",
      " 'sets of data, which can be a key advantage, especially in the unsupervised '\n",
      " 'setting.',\n",
      " 'Surprisingly, for constructing sentence embeddings, naively using averaged '\n",
      " 'word vectors was recently shown to outperform LSTMs (see (Wieting et al., '\n",
      " '0000a) for plain averaging, and (Arora et al., 0000) for weighted '\n",
      " 'averaging).',\n",
      " 'This example shows potential in exploiting the tradeoff between model '\n",
      " 'complexity and ability to process huge amounts of text using scalable '\n",
      " 'algorithms, towards the simpler side.',\n",
      " 'In view of this trade-off, our work here further advances unsupervised '\n",
      " 'learning of sentence embeddings.',\n",
      " 'Our proposed model can be seen as an extension of the CBOW (Mikolov et al., '\n",
      " '0000b;a) training objective to train sentence instead of word embeddings.',\n",
      " 'We demonstrate that the empirical performance of our resulting '\n",
      " 'general-purpose sentence embeddings very significantly exceeds the state of '\n",
      " 'the art, while keeping the model simplicity as well as training and '\n",
      " 'inference complexity exactly as low as in averaging methods (Wieting et al., '\n",
      " '0000a; Arora et al., 0000), thereby also putting the title of (Arora et al., '\n",
      " '0000) in perspective.',\n",
      " 'While very useful semantic representations are available for words, it '\n",
      " 'remains challenging to produce and learn such semantic embeddings for longer '\n",
      " 'pieces of text, such as sentences, paragraphs or entire documents.',\n",
      " 'Even more so, it remains a key goal to learn such general-purpose '\n",
      " 'representations in an unsupervised way.',\n",
      " 'We propose Sent0Vec, a simple unsupervised model allowing to compose '\n",
      " 'sentence embeddings using the word vectors along with n-gram embeddings, '\n",
      " 'simultaneously training composition and the embedding vectors themselves.',\n",
      " 'Equal contribution 0 Iprova SA, Switzerland 0 Computer and Communication '\n",
      " 'Sciences, EPFL, Switzerland.',\n",
      " 'Correspondence to: Martin Jaggi <martin.jaggi@epfl.ch>.',\n",
      " 'This strongly contrasts all neural network based approaches, and allows our '\n",
      " 'model to learn from extremely large datasets, which is a crucial advantage '\n",
      " 'in the unsupervised setting.',\n",
      " 'Our method shows significant performance improvements compared to the '\n",
      " 'current stateof-the-art unsupervised and even semi-supervised models.',\n",
      " 'The resulting general-purpose embeddings show strong robustness when '\n",
      " 'transferred to a wide range of prediction benchmarks.',\n",
      " 'Formally, we learn source v w and target uw embeddings for each word w in '\n",
      " 'the vocabulary, with embedding dimension h and k = |V| as in (0).',\n",
      " 'The sentence embedding is defined as the average of the source word '\n",
      " 'embeddings of its constituent words, as in (0).',\n",
      " 'Model Our model is inspired by simple matrix factor models (bilinear models) '\n",
      " 'such as recently very successfully used in unsupervised learning of word '\n",
      " 'embeddings (Mikolov et al., 0000b;a; Pennington et al., 0000; Bojanowski et '\n",
      " 'al., 0000) as well as supervised of sentence classification (Joulin et al., '\n",
      " '0000).',\n",
      " 'For the large number of output classes |V| to be predicted, negative '\n",
      " 'sampling is known to significantly improve training efficiency, see also '\n",
      " '(Goldberg & Levy, 0000).',\n",
      " 'In all models studied, the columns of the matrix V will collect the learned '\n",
      " 'word vectors, having h dimensions.',\n",
      " 'For a given sentence S, which can be of arbitrary length, the indicator '\n",
      " 'vector S  {0, 0}|V| is a binary vector encoding S (bag of words encoding).',\n",
      " 'Fixed-length context windows S running over the corpus are used in word '\n",
      " 'embedding methods as in C-BOW (Mikolov et al., 0000b;a) and GloVe '\n",
      " '(Pennington et al., 0000).',\n",
      " 'Here we have k = |V| and each cost function fS : Rk  R only depends on a '\n",
      " 'single row of its input, describing the observed target word for the given '\n",
      " 'fixed-length context S. In contrast, for sentence embeddings which are the '\n",
      " 'focus of our paper here, S will be entire sentences or documents (therefore '\n",
      " 'variable length).',\n",
      " 'This property is shared with the supervised FastText classifier (Joulin et '\n",
      " 'al., 0000), which however uses soft-max with k  |V| being the number of '\n",
      " 'class labels.',\n",
      " 'Proposed Unsupervised Model We propose a new unsupervised model, Sent0Vec, '\n",
      " 'for learning universal sentence embeddings.',\n",
      " 'Conceptually, the model can be interpreted as a natural extension of the '\n",
      " 'wordcontexts from C-BOW (Mikolov et al., 0000b;a) to a larger sentence '\n",
      " 'context, with the sentence words being specifically optimized towards '\n",
      " 'additive combination over the sentence, by means of the unsupervised '\n",
      " 'objective function.',\n",
      " 'To select the possible target unigrams (positives), we use subsampling as in '\n",
      " '(Joulin et al., 0000; Bojanowski et al., 0000), each word w being discarded '\n",
      " '0  p with probability \\\\t qp (w) where qp (w) := min 0, t/fw + t/fw .',\n",
      " 'Where t is the subsampling hyper-parameter.',\n",
      " 'Subsampling prevents very frequent words of having too much influence in the '\n",
      " 'learning as they would introduce strong biases in the prediction task.',\n",
      " 'To efficiently sample negatives, a pre-processing table is constructed, '\n",
      " 'containing the words corresponding to the square root of their corpora '\n",
      " 'frequency.',\n",
      " 'Then, the negatives Nwt are sampled uniformly at random from the negatives '\n",
      " 'table except the target wt itself, following (Joulin et al., 0000; '\n",
      " 'Bojanowski et al., 0000).',\n",
      " 'Computational Efficiency In contrast to more complex neural network based '\n",
      " 'models, one of the core advantages of the proposed technique is the low '\n",
      " 'computational cost for both inference and training.',\n",
      " 'Given a sentence S and a trained model, computing the sentence '\n",
      " 'representation v S only requires |S|  h floating point operations (or '\n",
      " '|R(S)|  h to be precise for the n-gram case, see (0)), where h is the '\n",
      " 'embedding dimension.',\n",
      " 'The same holds for the cost of training with SGD on the objective (0), per '\n",
      " 'sentence seen in the training corpus.',\n",
      " 'Due to the simplicity of the model, parallel training is straight-forward '\n",
      " 'using parallelized or distributed SGD.',\n",
      " 'Comparison to C-BOW C-BOW (Mikolov et al., 0000b;a) tries to predict a '\n",
      " 'chosen target word given its fixed-size context window, the context being '\n",
      " 'defined by the average of the vectors associated with the words at a '\n",
      " 'distance less than the window size hyperparameter ws.',\n",
      " 'If our system, when restricted to unigram features, can be seen as an '\n",
      " 'extension of C-BOW where the context window includes the entire sentence, in '\n",
      " 'practice there are few important differences as C-BOW uses important tricks '\n",
      " 'to facilitate the learning of word embeddings.',\n",
      " 'C-BOW first uses frequent word subsampling on the sentences, deciding to '\n",
      " 'discard each token w with probability qp (w) or alike (small variations '\n",
      " 'exist across implementations).',\n",
      " 'Subsampling prevents the generation of n-grams features, and deprives the '\n",
      " 'sentence of an important part of its syntactical features.',\n",
      " 'It also shortens the distance between subsampled words, implicitly '\n",
      " 'increasing the span of the context window.',\n",
      " 'A second trick consists of using dynamic context windows: for each '\n",
      " 'subsampled word w, the size of its associated context window is sampled '\n",
      " 'uniformly between 0 and ws.',\n",
      " 'Using dynamic context windows is equivalent to weighing by the distance from '\n",
      " 'the focus word w divided by the window size (Levy et al., 0000).',\n",
      " 'This makes the prediction task local, and go against our objective of '\n",
      " 'creating sentence embeddings as we want to learn how to compose all n-gram '\n",
      " 'features present in a sentence.',\n",
      " 'In the results section, we report a significant improvement of our method '\n",
      " 'over C-BOW.',\n",
      " 'Model Training Three different datasets have been used to train our models: '\n",
      " 'the Toronto book corpus0 , Wikipedia sentences and tweets.',\n",
      " 'The Wikipedia and Toronto books sentences have been tokenized using the '\n",
      " 'Stanford NLP library (Manning et al., 0000), while for tweets we used the '\n",
      " 'NLTK tweets tokenizer (Bird et al., 0000).',\n",
      " 'We update the weights using SGD with a linearly decaying learning rate.',\n",
      " 'Also, to prevent overfitting, for each sentence we use dropout on its list '\n",
      " 'of n-grams R(S) \\\\\\\\ {U (S)}, where U (S) is the set of all unigrams '\n",
      " 'contained in sentence S. After empirically trying multiple dropout schemes, '\n",
      " 'we find that dropping K n-grams (n > 0) for each sentence is giving superior '\n",
      " 'results compared to dropping each token with some fixed probability.',\n",
      " 'This dropout mechanism would negatively impact shorter sentences.',\n",
      " 'The regularization can be pushed further by applying L0 regularization to '\n",
      " 'the word vectors.',\n",
      " 'Encouraging sparsity in the embedding vectors is particularly beneficial for '\n",
      " 'high dimension h. The additional soft thresholding in every SGD step adds '\n",
      " 'negligible computational cost.',\n",
      " 'We train two models on each dataset, one with unigrams only and one with '\n",
      " 'unigrams and bigrams.',\n",
      " 'All training parameters for the models are provided in Table 0 in the '\n",
      " 'supplementary material.',\n",
      " 'Our C++ implementation builds upon the FastText library (Joulin et al., '\n",
      " '0000; Bojanowski et al., 0000).',\n",
      " 'We will make our code and pre-trained models available open-source.',\n",
      " 'Related Work We discuss existing models which have been proposed to '\n",
      " 'construct sentence embeddings.',\n",
      " 'While there is a large body of works in this direction  several among these '\n",
      " 'using e.g.',\n",
      " 'While some methods require ordered raw text i.e., a coherent corpus where '\n",
      " 'the next sentence is a logical continuation of the previous sentence, others '\n",
      " 'rely only on raw text i.e., an unordered collection of sentences.',\n",
      " 'Finally we also discuss alternative models built from structured data '\n",
      " 'sources.',\n",
      " 'Unsupervised Models Independent of Sentence Ordering The ParagraphVector '\n",
      " 'DBOW model (Le & Mikolov, 0000) is a log-linear model which is trained to '\n",
      " 'learn sentence as well as word embeddings and then use a softmax '\n",
      " 'distribution to predict words contained in the sentence given the sentence '\n",
      " 'vector representation.',\n",
      " 'They also propose a different model ParagraphVector DM where they use '\n",
      " 'n-grams of consecutive words along with the sentence vector representation '\n",
      " 'to predict the next word.',\n",
      " 'The model then uses an LSTM-based architecture to retrieve the original '\n",
      " 'sentence from the corrupted version.',\n",
      " 'The model can then be used to encode new sentences into vector '\n",
      " 'representations.',\n",
      " 'In the case of p0 = px = 0, the model simply becomes a Sequential '\n",
      " 'Autoencoder.',\n",
      " 'It allows the model to better generate syntactical features.',\n",
      " 'The fw term is here to enable the model to generate some frequent words even '\n",
      " 'if their matching with the discourse vector cs is low.',\n",
      " 'Therefore, this model tries to generate sentences as a mixture of three type '\n",
      " 'of words: words matching the sentence discourse vector cs , syntactical '\n",
      " 'words matching c0 , and words with high fw .',\n",
      " 'The sentence discourse vector can hence be obtained by subtracting c0 '\n",
      " 'estimated by the first principal component of cs s on a set of sentences.',\n",
      " 'In other words, the sentence embeddings are obtained by a weighted average '\n",
      " 'of the word vectors stripping away the syntax by subtracting the common '\n",
      " 'discourse vector and down-weighting frequent tokens.',\n",
      " 'They generate sentence embeddings from diverse pre-trained word embeddings '\n",
      " 'among which are unsupervised word embeddings such as GloVe (Pennington et '\n",
      " 'al., 0000) as well as supervised word embeddings such as paragram-SL000 '\n",
      " '(PSL) (Wieting et al., 0000) trained on the Paraphrase Database '\n",
      " '(Ganitkevitch et al., 0000).',\n",
      " 'In a very different line of work, C-PHRASE (Pham et al., 0000) relies on '\n",
      " 'additional information from the syntactic parse tree of each sentence, which '\n",
      " 'is incorporated into the C-BOW training objective.',\n",
      " 'While building on an unsupervised objective, the employed dictionary '\n",
      " 'learning step for obtaining phrase templates is task-specific (for each '\n",
      " 'use-case), not resulting in general-purpose embeddings.',\n",
      " 'Unsupervised Models Depending on Sentence Ordering The SkipThought model '\n",
      " '(Kiros et al., 0000) combines sentence level models with recurrent neural '\n",
      " 'networks.',\n",
      " 'Given a sentence Si from an ordered corpus, the model is trained to predict '\n",
      " 'Si0 and Si+0 .',\n",
      " 'FastSent (Hill et al., 0000a) is a sentence-level log-linear bag-of-words '\n",
      " 'model.',\n",
      " 'Like SkipThought, it uses adjacent sentences as the prediction target and is '\n",
      " 'trained in an unsupervised fashion.',\n",
      " 'Using word sequences allows the model to improve over the earlier work of '\n",
      " 'paragraph0vec (Le & Mikolov, 0000).',\n",
      " 'This model is named FastSent + AE in our comparisons.',\n",
      " 'Compared to our approach, Siamese C-BOW (Kenter et al., 0000) shares the '\n",
      " 'idea of learning to average word embeddings over a sentence.',\n",
      " 'However, it relies on a Siamese neural network architecture to predict '\n",
      " 'surrounding sentences, contrasting our simpler unsupervised objective.',\n",
      " 'Note that on the character sequence level instead of word sequences, '\n",
      " 'FastText (Bojanowski et al., 0000) uses the same conceptual model to obtain '\n",
      " 'better word embeddings.',\n",
      " 'This is most similar to our proposed model, with two key differences: '\n",
      " 'Firstly, we predict from source word sequences to target words, as opposed '\n",
      " 'to character sequences to target words, and secondly, our model is averaging '\n",
      " 'the source embeddings instead of summing them.',\n",
      " 'Models requiring structured data DictRep (Hill et al., 0000b) is trained to '\n",
      " 'map dictionary definitions of the words to the pre-trained word embeddings '\n",
      " 'of these words.',\n",
      " 'They use two different architectures, namely BOW and RNN (LSTM) with the '\n",
      " 'choice of learning the input word embeddings or using them pre-trained.',\n",
      " 'A similar architecture is used by the CaptionRep variant, but here the task '\n",
      " 'is the mapping of given image captions to a pre-trained vector '\n",
      " 'representation of these images.',\n",
      " 'Evaluation Tasks We use a standard set of supervised as well as unsupervised '\n",
      " 'benchmark tasks from the literature to evaluate our trained models, '\n",
      " 'following (Hill et al., 0000a).',\n",
      " 'The breadth of tasks allows to fairly measure generalization to a wide area '\n",
      " 'of different domains, testing the general-purpose quality (universality) of '\n",
      " 'all competing sentence embeddings.',\n",
      " 'For downstream supervised evaluations, sentence embeddings are combined with '\n",
      " 'logistic regression to predict target labels.',\n",
      " 'Downstream Supervised Evaluation.',\n",
      " 'Sentence embeddings are evaluated for various supervised classification '\n",
      " 'tasks as follows.',\n",
      " 'We evaluate paraphrase identification (MSRP) (Dolan et al., 0000), '\n",
      " 'classification of movie review sentiment (MR) (Pang & Lee, 0000), product '\n",
      " 'reviews (CR) (Hu & Liu, 0000), subjectivity classification (SUBJ)(Pang & '\n",
      " 'Lee, 0000), opinion polarity (MPQA) (Wiebe et al., 0000) and question type '\n",
      " 'classification (TREC) (Voorhees, 0000).',\n",
      " 'To classify, we use the code provided by (Kiros et al., 0000) in the same '\n",
      " 'manner as in (Hill et al., 0000a).',\n",
      " 'For the MSRP dataset, containing pairs of sentences (S0 , S0 ) with '\n",
      " 'associated paraphrase label, we generate feature vectors by concatenating '\n",
      " 'their Sent0Vec representations |v S0  v S0 | with the component-wise product '\n",
      " 'v S0  v S0 .',\n",
      " 'The predefined training split is used to tune the L0 penalty parameter using '\n",
      " 'cross-validation and the accuracy and F0 scores are computed on the test '\n",
      " 'set.',\n",
      " 'For the remaining 0 datasets, Sent0Vec embeddings are inferred from input '\n",
      " 'sentences and directly fed to a logistic regression classifier.',\n",
      " 'Accuracy scores are obtained using 00-fold cross-validation for the MR, CR, '\n",
      " 'SUBJ and MPQA datasets.',\n",
      " 'For those datasets nested cross-validation is used to tune the L0 penalty.',\n",
      " 'For the TREC dataset, as for the MRSP dataset, the L0 penalty is tuned on '\n",
      " 'the predefined train split using 00-fold cross-validation, and the accuracy '\n",
      " 'is computed on the test set.',\n",
      " 'Unsupervised Similarity Evaluation.',\n",
      " 'We perform unsupervised evaluation of the the learnt sentence embeddings '\n",
      " 'using the sentence cosine similarity, on the STS 0000 (Agirre et al., 0000) '\n",
      " 'and SICK 0000 (Marelli et al., 0000) datasets.',\n",
      " 'These similarity scores are compared to the gold-standard human judgements '\n",
      " 'using Pearsons r (Pearson, 0000) and Spearmans  (Spearman, 0000) correlation '\n",
      " 'scores.',\n",
      " 'The SICK dataset consists of about 00,000 sentence pairs along with '\n",
      " 'relatedness scores of the pairs.',\n",
      " 'The STS 0000 dataset contains 0,000 pairs, divided into six different '\n",
      " 'categories on the basis of origin of sentences/phrases namely Twitter, '\n",
      " 'headlines, news, forum, WordNet and images.',\n",
      " 'See (Agirre et al., 0000) for more precise information on how the pairs have '\n",
      " 'been created.',\n",
      " 'Results and Discussion In Tables 0 and 0, we compare our results with those '\n",
      " 'obtained by (Hill et al., 0000a) on different models.',\n",
      " 'Along with the models discussed in Section 0, this also includes the '\n",
      " 'sentence embedding baselines obtained by simple averaging of word embeddings '\n",
      " 'over the sentence, in both the C-BOW and skip-gram variants.',\n",
      " 'TF-IDF BOW is a representation consisting of the counts of the 000,000 most '\n",
      " 'common feature-words, weighed by their TF-IDF frequencies.',\n",
      " 'Performance of supervised and semisupervised models on these evaluations can '\n",
      " 'be observed in Tables 0 and 0 in the supplementary material.',\n",
      " 'Downstream Supervised Evaluation Results.',\n",
      " 'On running supervised evaluations and observing the results in Table 0, we '\n",
      " 'find that on an average our models are second only to SkipThought vectors.',\n",
      " 'Also, both our models achieve state of the art results on the CR task.',\n",
      " 'We also observe that on half of the supervised tasks, our unigrams + bigram '\n",
      " 'model is the the best model after SkipThought.',\n",
      " 'Our models are weaker on the MSRP task (which consists of the identification '\n",
      " 'of labelled paraphrases) compared to stateof-the-art methods.',\n",
      " 'However, we observe that the models which perform extremely well on this '\n",
      " 'task end up faring very poorly on the other tasks, indicating a lack of '\n",
      " 'generalizability.',\n",
      " 'On rest of the tasks, our models perform extremely well.',\n",
      " 'The SkipThought model is able to outperform our models on most of the tasks '\n",
      " 'as it is trained to predict the previous and next sentences and a lot of '\n",
      " 'tasks are able to make use of this contextual information missing in our '\n",
      " 'Sent0Vec models.',\n",
      " 'For example, the TREC task is a poor measure of how one predicts the content '\n",
      " 'of the sentence (the question) but a good measure of how the next sentence '\n",
      " 'in the sequence (the answer) is predicted.',\n",
      " 'Unsupervised Similarity Evaluation Results.',\n",
      " 'In Table 0, we see that our Sent0Vec models are state-of-the-art on the '\n",
      " 'majority of tasks when comparing to all the unsupervised models trained on '\n",
      " 'the Toronto corpus, and clearly achieve the best averaged performance.',\n",
      " 'Our Sent0Vec models also on average outperform or are at par with the '\n",
      " 'C-PHRASE model, despite significantly lagging behind on the STS 0000 WordNet '\n",
      " 'and News subtasks.',\n",
      " 'This observation can be attributed to the fact that a big chunk of the data '\n",
      " 'that the C-PHRASE model is trained on comes from English Wikipedia, helping '\n",
      " 'it to perform well on datasets involving definition and news items.',\n",
      " 'Also, C-PHRASE uses data three times the size of the Toronto book corpus.',\n",
      " 'Interestingly, our model outperforms C-PHRASE when trained on Wikipedia, as '\n",
      " 'shown in Table 0, despite the fact that we use no parse tree information.',\n",
      " 'In the official results of the more recent edition of the STS 0000 benchmark '\n",
      " '(Cer et al., 0000), our model also significantly outperforms C-PHRASE, and '\n",
      " 'delivers the best unsupervised baseline method.',\n",
      " 'To summarize our contributions on both supervised and unsupervised tasks, in '\n",
      " 'Table 0 we present the results in terms of the macro average over the '\n",
      " 'averages 0 For the Siamese C-BOW model trained on the Toronto corpus, '\n",
      " 'supervised evaluation as well as similarity evaluation results on the SICK '\n",
      " '0000 dataset are unavailable.',\n",
      " 'Model SAE SAE + embs.',\n",
      " 'ParagraphVec DBOW ParagraphVec DM Skipgram C-BOW Unigram TFIDF Sent0Vec uni.',\n",
      " 'Table 0: Comparison of the performance of different models on different '\n",
      " 'supervised evaluation tasks.',\n",
      " 'An underline indicates the best performance for the dataset.',\n",
      " 'Top 0 performances in each data category are shown in bold.',\n",
      " 'Model SAE SAE + embs.',\n",
      " 'ParagraphVec DBOW ParagraphVec DM Skipgram C-BOW Unigram TF-IDF Sent0Vec '\n",
      " 'uni.',\n",
      " 'Table 0: Unsupervised Evaluation Tasks: Comparison of the performance of '\n",
      " 'different models on Spearman/Pearson correlation measures.',\n",
      " 'An underline indicates the best performance for the dataset.',\n",
      " 'Top 0 performances in each data category are shown in bold.',\n",
      " 'The average is calculated as the average of entries for each correlation '\n",
      " 'measure.',\n",
      " 'For unsupervised tasks, averages are taken over both Spearman and Pearson '\n",
      " 'scores.',\n",
      " 'The comparison includes the best performing unsupervised and semi-supervised '\n",
      " 'methods described in Section 0.',\n",
      " 'For models trained on the Toronto books dataset, we report a 0.0 % points '\n",
      " 'improvement over the state of the art.',\n",
      " 'Considering all supervised, semi-supervised methods and all datasets '\n",
      " 'compared in (Hill et al., 0000a), we report a 0.0 % points improvement.',\n",
      " 'We also see a noticeable improvement in accuracy as we use larger datasets '\n",
      " 'like twitter and Wikipedia dump.',\n",
      " 'We can also see that the Sent0Vec models are also faster to train when '\n",
      " 'compared to methods like SkipThought and DictRep owing to the SGD step '\n",
      " 'allowing a high degree of parallelizability.',\n",
      " 'We can clearly see Sent0Vec outperforming other unsupervised and even '\n",
      " 'semi-supervised methods.',\n",
      " 'Comparison with Arora et al.',\n",
      " 'In Table 0, we report an experimental comparison to the model of Arora et '\n",
      " 'al.',\n",
      " 'In the table, the suffix W indicates that their down-weighting scheme has '\n",
      " 'been used, while the suffix R indicates the removal of the first principal '\n",
      " 'component.',\n",
      " 'They report values of a  [000 , 000 ] as giving the best results and used a '\n",
      " '= 000 for all their experiments.',\n",
      " 'Their down-weighting scheme hints us to reduce the importance of syntactical '\n",
      " 'features.',\n",
      " 'To do so, we use a simple blacklist containing the 00 most frequent tokens '\n",
      " 'in the Twitter corpus and discard them before averaging.',\n",
      " 'Results are also reported in Table 0.',\n",
      " 'We observe that our results are competitive with the embeddings of Arora et '\n",
      " 'al.',\n",
      " 'Table 0: Best unsupervised and semi-supervised methods ranked by macro '\n",
      " 'average along with their training times.',\n",
      " 'Table 0: Comparison of the performance of the unsupervised and '\n",
      " 'semi-supervised sentence embeddings by (Arora et al., 0000) with our models, '\n",
      " 'in terms of Pearsons correlation.',\n",
      " 'Figure 0: Left figure: the profile of the word vector L0 norms as a function '\n",
      " 'of log(fw ) for each vocabulary word w, as learnt by our unigram model '\n",
      " 'trained on Toronto books.',\n",
      " 'Right figure: down-weighting scheme proposed by a .',\n",
      " 'It is important to note that the scores obtained from supervised '\n",
      " 'task-specific PSL embeddings trained for the purpose of semantic similarity '\n",
      " 'outperform our method on both SICK and average STS 0000, which is expected '\n",
      " 'as our model is trained purely unsupervised.',\n",
      " 'The effect of datasets and n-grams.',\n",
      " 'Despite being trained on three very different datasets, all of our models '\n",
      " 'generalize well to sometimes very specific domains.',\n",
      " 'Models trained on Toronto Corpus are the state-of-the art on the STS 0000 '\n",
      " 'images dataset even beating the supervised CaptionRep model trained on '\n",
      " 'images.',\n",
      " 'On learning the importance and the direction of the word vectors.',\n",
      " 'Our model  by learning how to generate and compose word vectors  has to '\n",
      " 'learn both the direction of the word embeddings as well as their norm.',\n",
      " 'Considering the norms of the used word vectors as by our averaging over the '\n",
      " 'sentence, we observe an interesting distribution of the importance of each '\n",
      " 'word.',\n",
      " 'In Figure 0 we show the profile of the L0 -norm as a function of log(fw ) '\n",
      " 'for each w  V, and compare it to the static down-weighting mechanism of '\n",
      " 'Arora et al.',\n",
      " 'We can observe that our model is learning to down-weight frequent tokens by '\n",
      " 'itself.',\n",
      " 'It is also down-weighting rare tokens and the norm profile seems to roughly '\n",
      " 'follow Luhns hypothesis (Luhn, 0000), a well known information retrieval '\n",
      " 'paradigm, stating that mid-rank terms are the most significant to '\n",
      " 'discriminate content.',\n",
      " 'Modifying the objective function would change the weighting scheme learnt.',\n",
      " 'From a more semantic oriented objective, it should be possible to learn to '\n",
      " 'attribute lower norms for very frequent terms, to more specifically fit '\n",
      " 'sentence similarity tasks.',\n",
      " 'Conclusion In this paper, we introduced a novel unsupervised and '\n",
      " 'computationally efficient method to train and infer sentence embeddings.',\n",
      " 'On supervised evaluations, our method, on an average, achieves better '\n",
      " 'performance than all other unsupervised competitors except the SkipThought '\n",
      " 'vectors.',\n",
      " 'Future work could focus on augmenting the model to exploit data with ordered '\n",
      " 'sentences.',\n",
      " 'Furthermore, we would like to further investigate the models ability as '\n",
      " 'giving pre-trained embeddings to enable downstream transfer learning tasks.',\n",
      " 'We are indebted to Piotr Bojanowski and Armand Joulin for helpful '\n",
      " 'discussions.',\n",
      " 'References Agirre, Eneko, Banea, Carmen, Cardie, Claire, Cer, Daniel, Diab, '\n",
      " 'Mona, Gonzalez-Agirre, Aitor, Guo, Weiwei, Mihalcea, Rada, Rigau, German, '\n",
      " 'and Wiebe, Janyce.',\n",
      " 'Semeval-0000 task 00: Multilingual semantic textual similarity.',\n",
      " 'In Proceedings of the 0th international workshop on semantic evaluation '\n",
      " '(SemEval 0000), pp.',\n",
      " 'Association for Computational Linguistics Dublin, Ireland, 0000.',\n",
      " 'Arora, Sanjeev, Li, Yuanzhi, Liang, Yingyu, Ma, Tengyu, and Risteski, '\n",
      " 'Andrej.',\n",
      " 'A Latent Variable Model Approach to PMIbased Word Embeddings.',\n",
      " 'In Transactions of the Association for Computational Linguistics, pp.',\n",
      " 'Arora, Sanjeev, Liang, Yingyu, and Ma, Tengyu.',\n",
      " 'A simple but tough-to-beat baseline for sentence embeddings.',\n",
      " 'In International Conference on Learning Representations (ICLR), 0000.',\n",
      " 'Bird, Steven, Klein, Ewan, and Loper, Edward.',\n",
      " 'Natural language processing with Python: analyzing text with the natural '\n",
      " 'language toolkit.',\n",
      " 'OReilly Media, Inc., 0000.',\n",
      " 'Bojanowski, Piotr, Grave, Edouard, Joulin, Armand, and Mikolov, Tomas.',\n",
      " 'Enriching Word Vectors with Subword Information.',\n",
      " 'Transactions of the Association for Computational Linguistics, 0:000000, '\n",
      " '0000.',\n",
      " 'Cer, Daniel, Diab, Mona, Agirre, Eneko, Lopez-Gazpio, Inigo, and Specia, '\n",
      " 'Lucia.',\n",
      " 'SemEval-0000 Task 0: Semantic Textual Similarity Multilingual and '\n",
      " 'Cross-lingual Focused Evaluation.',\n",
      " 'In SemEval-0000 - Proceedings of the 00th International Workshop on Semantic '\n",
      " 'Evaluations, pp.',\n",
      " 'Association for Computational Linguistics.',\n",
      " 'Dolan, Bill, Quirk, Chris, and Brockett, Chris.',\n",
      " 'Unsupervised construction of large paraphrase corpora: Exploiting massively '\n",
      " 'parallel news sources.',\n",
      " 'In Proceedings of the 00th international conference on Computational '\n",
      " 'Linguistics, pp.',\n",
      " 'Association for Computational Linguistics, 0000.',\n",
      " 'Ganitkevitch, Juri, Van Durme, Benjamin, and Callison-Burch, Chris.',\n",
      " 'Ppdb: The paraphrase database.',\n",
      " 'Goldberg, Yoav and Levy, Omer.',\n",
      " 'Hu, Minqing and Liu, Bing.',\n",
      " 'Mining and summarizing customer reviews.',\n",
      " 'In Proceedings of the tenth ACM SIGKDD international conference on Knowledge '\n",
      " 'discovery and data mining, pp.',\n",
      " 'Huang, Furong and Anandkumar, Animashree.',\n",
      " 'Unsupervised Learning of Word-Sequence Representations from Scratch via '\n",
      " 'Convolutional Tensor Decomposition.',\n",
      " 'Joulin, Armand, Grave, Edouard, Bojanowski, Piotr, and Mikolov, Tomas.',\n",
      " 'Bag of Tricks for Efficient Text Classification.',\n",
      " 'In Proceedings of the 00th Conference of the European Chapter of the '\n",
      " 'Association for Computational Linguistics, Short Papers, pp.',\n",
      " 'Kenter, Tom, Borisov, Alexey, and de Rijke, Maarten.',\n",
      " 'Siamese CBOW: Optimizing Word Embeddings for Sentence Representations.',\n",
      " 'In ACL - Proceedings of the 00th Annual Meeting of the Association for '\n",
      " 'Computational Linguistics, pp.',\n",
      " 'Kiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan R, Zemel, Richard, Urtasun, '\n",
      " 'Raquel, Torralba, Antonio, and Fidler, Sanja.',\n",
      " 'Skip-Thought Vectors.',\n",
      " 'In NIPS 0000 - Advances in Neural Information Processing Systems 00, pp.',\n",
      " 'Le, Quoc V and Mikolov, Tomas.',\n",
      " 'Distributed Representations of Sentences and Documents.',\n",
      " 'In ICML 0000 - Proceedings of the 00st International Conference on Machine '\n",
      " 'Learning, volume 00, pp.',\n",
      " 'Levy, Omer, Goldberg, Yoav, and Dagan, Ido.',\n",
      " 'Improving distributional similarity with lessons learned from word '\n",
      " 'embeddings.',\n",
      " 'Transactions of the Association for Computational Linguistics, 0:000000, '\n",
      " '0000.',\n",
      " 'The automatic creation of literature abstracts.',\n",
      " 'IBM Journal of research and development, 0(0):000 000, 0000.',\n",
      " 'Manning, Christopher D, Surdeanu, Mihai, Bauer, John, Finkel, Jenny Rose, '\n",
      " 'Bethard, Steven, and McClosky, David.',\n",
      " 'The stanford corenlp natural language processing toolkit.',\n",
      " 'In ACL (System Demonstrations), pp.',\n",
      " 'Marelli, Marco, Menini, Stefano, Baroni, Marco, Bentivogli, Luisa, Bernardi, '\n",
      " 'Raffaella, and Zamparelli, Roberto.',\n",
      " 'A sick cure for the evaluation of compositional distributional semantic '\n",
      " 'models.',\n",
      " 'Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jeffrey.',\n",
      " 'Efficient estimation of word representations in vector space.',\n",
      " 'Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff.',\n",
      " 'Distributed Representations of Words and Phrases and their Compositionality.',\n",
      " 'In NIPS - Advances in Neural Information Processing Systems 00, pp.',\n",
      " 'Hill, Felix, Cho, Kyunghyun, and Korhonen, Anna.',\n",
      " 'Learning Distributed Representations of Sentences from Unlabelled Data.',\n",
      " 'In Proceedings of NAACL-HLT, February 0000a.',\n",
      " 'Pang, Bo and Lee, Lillian.',\n",
      " 'A sentimental education: Sentiment analysis using subjectivity summarization '\n",
      " 'based on minimum cuts.',\n",
      " 'In Proceedings of the 00nd annual meeting on Association for Computational '\n",
      " 'Linguistics, pp.',\n",
      " 'Association for Computational Linguistics, 0000.',\n",
      " 'Hill, Felix, Cho, KyungHyun, Korhonen, Anna, and Bengio, Yoshua.',\n",
      " 'Learning to understand phrases by embedding the dictionary.',\n",
      " 'URL https://tacl0000.cs.columbia.edu/ojs/ index.php/tacl/article/view/000.',\n",
      " 'Pang, Bo and Lee, Lillian.',\n",
      " 'Seeing stars: Exploiting class relationships for sentiment categorization '\n",
      " 'with respect to rating scales.',\n",
      " 'In Proceedings of the 00rd annual meeting on association for computational '\n",
      " 'linguistics, pp.',\n",
      " 'Association for Computational Linguistics, 0000.',\n",
      " 'Unsupervised Learning of Sentence Embeddings using Compositional n-Gram '\n",
      " 'Features Pearson, Karl.',\n",
      " 'Note on regression and inheritance in the case of two parents.',\n",
      " 'Proceedings of the Royal Society of London, 00: 000000, 0000.',\n",
      " 'Pennington, Jeffrey, Socher, Richard, and Manning, Christopher D. Glove: '\n",
      " 'Global vectors for word representation.',\n",
      " 'In EMNLP, volume 00, pp.',\n",
      " 'Pham, NT, Kruszewski, G, Lazaridou, A, and Baroni, M. Jointly optimizing '\n",
      " 'word representations for lexical and sentential tasks with the c-phrase '\n",
      " 'model.',\n",
      " 'Rockafellar, R Tyrrell.',\n",
      " 'Monotone operators and the proximal point algorithm.',\n",
      " 'SIAM journal on control and optimization, 00(0):000000, 0000.',\n",
      " 'The proof and measurement of association between two things.',\n",
      " 'The American journal of psychology, 00 (0):00000, 0000.',\n",
      " 'Voorhees, Ellen M. Overview of the trec 0000 question answering track.',\n",
      " 'In NIST special publication, pp.',\n",
      " 'Wiebe, Janyce, Wilson, Theresa, and Cardie, Claire.',\n",
      " 'Annotating expressions of opinions and emotions in language.',\n",
      " 'Language resources and evaluation, 00(0):000000, 0000.',\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, Livescu, Karen, and Roth, Dan.',\n",
      " 'From paraphrase database to compositional paraphrase model and back.',\n",
      " 'In TACL - Transactions of the Association for Computational Linguistics, '\n",
      " '0000.',\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen.',\n",
      " 'Towards universal paraphrastic sentence embeddings.',\n",
      " 'In International Conference on Learning Representations (ICLR), 0000a.',\n",
      " 'Wieting, John, Bansal, Mohit, Gimpel, Kevin, and Livescu, Karen.',\n",
      " 'Charagram: Embedding Words and Sentences via Character n-grams.',\n",
      " 'In EMNLP - Proceedings of the 0000 Conference on Empirical Methods in '\n",
      " 'Natural Language Processing, pp.',\n",
      " 'Association for Computational Linguistics.',\n",
      " 'B. L0 regularization of models Optionally, our model can be additionally '\n",
      " 'improved by adding an L0 regularizer term in the objective function, leading '\n",
      " 'to slightly better generalization performance.',\n",
      " 'Additionally, encouraging sparsity in the embedding vectors is beneficial '\n",
      " 'for memory reasons, allowing higher embedding dimensions h. We propose to '\n",
      " 'apply L0 regularization individually to each word (and n-gram) vector (both '\n",
      " 'source and target vectors).',\n",
      " 'Now, in order to minimize a function of the form f (z) + g(z) where g(z) is '\n",
      " 'not differentiable over the domain, we can use the basic proximal-gradient '\n",
      " 'scheme.',\n",
      " 'Similar to the proximal-gradient scheme, in our case we can optionally use '\n",
      " 'the thresholding operator on the updated word  lr 0 and n-gram vectors after '\n",
      " 'an SGD step.',\n",
      " 'The soft thresholding parameter used for this update is |R(S\\\\\\\\{w and   lr0 '\n",
      " 'for t })| 0 the source and target vectors respectively where lr is the '\n",
      " 'current learning rate,  is the L0 regularization parameter and S is the '\n",
      " 'sentence on which SGD is being run.',\n",
      " 'We observe that L0 regularization using the proximal step gives our models a '\n",
      " 'small boost in performance.',\n",
      " 'Also, applying the thresholding operator takes only |R(S\\\\\\\\{wt })|h '\n",
      " 'floating point operations for the updating the word vectors corresponding to '\n",
      " 'the sentence and (|N | + 0)  h for updating the target as well as the '\n",
      " 'negative word vectors, where |N | is the number of negatives sampled and h '\n",
      " 'is the embedding dimension.',\n",
      " 'Thus, performing L0 regularization using soft-thresholding operator comes '\n",
      " 'with a small computational overhead.',\n",
      " 'We set  to be 0.0000 for both the Wikipedia and the Toronto Book Corpus '\n",
      " 'unigrams + bigrams models.',\n",
      " 'CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW+embs DictRep RNN '\n",
      " 'DictRep RNN+embs.',\n",
      " 'Table 0: Comparison of the performance of different Sent0Vec models with '\n",
      " 'different semi-supervised/supervised models on different downstream '\n",
      " 'supervised evaluation tasks.',\n",
      " 'An underline indicates the best performance for the dataset and Sent0Vec '\n",
      " 'model performances are bold if they perform as well or better than all other '\n",
      " 'non-Sent0Vec models, including those presented in Table 0.',\n",
      " 'Model Sent0Vec book corpus uni.',\n",
      " 'Sent0Vec book corpus uni.',\n",
      " 'Sent0Vec book corpus uni.',\n",
      " 'L0 reg Sent0Vec wiki uni.',\n",
      " 'L0 reg Sent0Vec twitter uni.',\n",
      " 'Sent0Vec twitter uni.',\n",
      " 'CaptionRep BOW CaptionRep RNN DictRep BOW DictRep BOW + embs.',\n",
      " 'DictRep RNN DictRep RNN + embs.',\n",
      " 'STS 0000 WordNet Twitter .00/.00.',\n",
      " 'Table 0: Unsupervised Evaluation: Comparison of the performance of different '\n",
      " 'Sent0Vec models with semisupervised/supervised models on Spearman/Pearson '\n",
      " 'correlation measures.',\n",
      " 'An underline indicates the best performance for the dataset and Sent0Vec '\n",
      " 'model performances are bold if they perform as well or better than all other '\n",
      " 'non-Sent0Vec models, including those presented in Table 0.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence tokenizing and filtering invalid sentences\n",
    "import nltk\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "results = []\n",
    "for p in parahgraphs:\n",
    "    sentences = tokenizer.tokenize(p)\n",
    "    sentences = [s for s in sentences if len(s) > 20 and s[0].isupper() and s[len(s)-1] == '.']\n",
    "    results.extend(sentences)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n"
     ]
    }
   ],
   "source": [
    "# Count final results\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4. Word tokenizing & lemmatizing & remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import package and download resources\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('model', 107),\n",
      " ('sentence', 84),\n",
      " ('word', 76),\n",
      " ('embeddings', 50),\n",
      " ('unsupervised', 41),\n",
      " ('vector', 36),\n",
      " ('task', 30),\n",
      " ('supervised', 29),\n",
      " ('trained', 27),\n",
      " ('learning', 24),\n",
      " ('bow', 22),\n",
      " ('representation', 21),\n",
      " ('evaluation', 21),\n",
      " ('using', 20),\n",
      " ('performance', 20),\n",
      " ('table', 20),\n",
      " ('method', 19),\n",
      " ('computational', 19),\n",
      " ('also', 18),\n",
      " ('training', 16),\n",
      " ('well', 15),\n",
      " ('corpus', 15),\n",
      " ('different', 15),\n",
      " ('association', 15),\n",
      " ('linguistics', 15),\n",
      " ('average', 14),\n",
      " ('use', 14),\n",
      " ('result', 14),\n",
      " ('used', 13),\n",
      " ('mikolov', 13),\n",
      " ('gram', 13),\n",
      " ('dataset', 13),\n",
      " ('data', 12),\n",
      " ('datasets', 12),\n",
      " ('embedding', 12),\n",
      " ('target', 12),\n",
      " ('proceeding', 12),\n",
      " ('objective', 11),\n",
      " ('phrase', 11),\n",
      " ('semantic', 10),\n",
      " ('arora', 10),\n",
      " ('context', 10),\n",
      " ('comparison', 10),\n",
      " ('dictrep', 10),\n",
      " ('similarity', 10),\n",
      " ('best', 10),\n",
      " ('art', 9),\n",
      " ('source', 9),\n",
      " ('window', 9),\n",
      " ('feature', 9),\n",
      " ('toronto', 9),\n",
      " ('book', 9),\n",
      " ('train', 8),\n",
      " ('text', 8),\n",
      " ('see', 8),\n",
      " ('learn', 8),\n",
      " ('bojanowski', 8),\n",
      " ('joulin', 8),\n",
      " ('predict', 8),\n",
      " ('paraphrase', 8),\n",
      " ('information', 8),\n",
      " ('hill', 8),\n",
      " ('uni', 8),\n",
      " ('simple', 7),\n",
      " ('state', 7),\n",
      " ('purpose', 7),\n",
      " ('language', 7),\n",
      " ('processing', 7),\n",
      " ('set', 7),\n",
      " ('averaging', 7),\n",
      " ('compared', 7),\n",
      " ('semi', 7),\n",
      " ('given', 7),\n",
      " ('function', 7),\n",
      " ('frequent', 7),\n",
      " ('pre', 7),\n",
      " ('sgd', 7),\n",
      " ('scheme', 7),\n",
      " ('regularization', 7),\n",
      " ('weighting', 7),\n",
      " ('skipthought', 7),\n",
      " ('sequence', 7),\n",
      " ('rnn', 7),\n",
      " ('score', 7),\n",
      " ('sts', 7),\n",
      " ('international', 7),\n",
      " ('conference', 7),\n",
      " ('general', 6),\n",
      " ('natural', 6),\n",
      " ('wieting', 6),\n",
      " ('work', 6),\n",
      " ('proposed', 6),\n",
      " ('neural', 6),\n",
      " ('dimension', 6),\n",
      " ('classification', 6),\n",
      " ('negative', 6),\n",
      " ('unigrams', 6),\n",
      " ('token', 6),\n",
      " ('wikipedia', 6),\n",
      " ('better', 6),\n",
      " ('generate', 6),\n",
      " ('term', 6),\n",
      " ('obtained', 6),\n",
      " ('captionrep', 6),\n",
      " ('measure', 6),\n",
      " ('perform', 6),\n",
      " ('twitter', 6),\n",
      " ('observe', 6),\n",
      " ('embs', 6),\n",
      " ('indicates', 6),\n",
      " ('norm', 6),\n",
      " ('distributed', 5),\n",
      " ('domain', 5),\n",
      " ('pennington', 5),\n",
      " ('based', 5),\n",
      " ('large', 5),\n",
      " ('even', 5),\n",
      " ('along', 5),\n",
      " ('improvement', 5),\n",
      " ('subsampling', 5),\n",
      " ('parameter', 5),\n",
      " ('point', 5),\n",
      " ('case', 5),\n",
      " ('size', 5),\n",
      " ('report', 5),\n",
      " ('thresholding', 5),\n",
      " ('step', 5),\n",
      " ('two', 5),\n",
      " ('image', 5),\n",
      " ('downstream', 5),\n",
      " ('pair', 5),\n",
      " ('cross', 5),\n",
      " ('agirre', 5),\n",
      " ('sick', 5),\n",
      " ('pearson', 5),\n",
      " ('correlation', 5),\n",
      " ('tomas', 5),\n",
      " ('question', 4),\n",
      " ('similar', 4),\n",
      " ('efficient', 4),\n",
      " ('benchmark', 4),\n",
      " ('key', 4),\n",
      " ('importance', 4),\n",
      " ('category', 4),\n",
      " ('instead', 4),\n",
      " ('matrix', 4),\n",
      " ('extremely', 4),\n",
      " ('make', 4),\n",
      " ('larger', 4),\n",
      " ('shown', 4),\n",
      " ('outperform', 4),\n",
      " ('show', 4),\n",
      " ('significantly', 4),\n",
      " ('propose', 4),\n",
      " ('network', 4),\n",
      " ('allows', 4),\n",
      " ('prediction', 4),\n",
      " ('levy', 4),\n",
      " ('length', 4),\n",
      " ('fixed', 4),\n",
      " ('cost', 4),\n",
      " ('soft', 4),\n",
      " ('would', 4),\n",
      " ('one', 4),\n",
      " ('system', 4),\n",
      " ('unigram', 4),\n",
      " ('important', 4),\n",
      " ('syntactical', 4),\n",
      " ('three', 4),\n",
      " ('next', 4),\n",
      " ('log', 4),\n",
      " ('architecture', 4),\n",
      " ('discourse', 4),\n",
      " ('like', 4),\n",
      " ('siamese', 4),\n",
      " ('pang', 4),\n",
      " ('lee', 4),\n",
      " ('trec', 4),\n",
      " ('validation', 4),\n",
      " ('accuracy', 4),\n",
      " ('spearman', 4),\n",
      " ('news', 4),\n",
      " ('sae', 4),\n",
      " ('paragraphvec', 4),\n",
      " ('underline', 4),\n",
      " ('bold', 4),\n",
      " ('figure', 4),\n",
      " ('semeval', 4),\n",
      " ('transaction', 4),\n",
      " ('john', 4),\n",
      " ('operator', 4),\n",
      " ('proximal', 4),\n",
      " ('recent', 3),\n",
      " ('improve', 3),\n",
      " ('present', 3),\n",
      " ('outperforms', 3),\n",
      " ('amount', 3),\n",
      " ('majority', 3),\n",
      " ('come', 3),\n",
      " ('raw', 3),\n",
      " ('current', 3),\n",
      " ('strong', 3),\n",
      " ('complexity', 3),\n",
      " ('much', 3),\n",
      " ('simpler', 3),\n",
      " ('advantage', 3),\n",
      " ('exploiting', 3),\n",
      " ('towards', 3),\n",
      " ('advance', 3),\n",
      " ('seen', 3),\n",
      " ('extension', 3),\n",
      " ('resulting', 3),\n",
      " ('low', 3),\n",
      " ('entire', 3),\n",
      " ('document', 3),\n",
      " ('allowing', 3),\n",
      " ('compose', 3),\n",
      " ('contrast', 3),\n",
      " ('approach', 3),\n",
      " ('significant', 3),\n",
      " ('number', 3),\n",
      " ('class', 3),\n",
      " ('goldberg', 3),\n",
      " ('bag', 3),\n",
      " ('glove', 3),\n",
      " ('input', 3),\n",
      " ('focus', 3),\n",
      " ('paper', 3),\n",
      " ('fasttext', 3),\n",
      " ('however', 3),\n",
      " ('label', 3),\n",
      " ('probability', 3),\n",
      " ('containing', 3),\n",
      " ('sampled', 3),\n",
      " ('associated', 3),\n",
      " ('distance', 3),\n",
      " ('includes', 3),\n",
      " ('trick', 3),\n",
      " ('first', 3),\n",
      " ('small', 3),\n",
      " ('consists', 3),\n",
      " ('section', 3),\n",
      " ('tweet', 3),\n",
      " ('manning', 3),\n",
      " ('dropout', 3),\n",
      " ('giving', 3),\n",
      " ('high', 3),\n",
      " ('bigram', 3),\n",
      " ('direction', 3),\n",
      " ('ordered', 3),\n",
      " ('dbow', 3),\n",
      " ('matching', 3),\n",
      " ('component', 3),\n",
      " ('database', 3),\n",
      " ('dictionary', 3),\n",
      " ('specific', 3),\n",
      " ('kiros', 3),\n",
      " ('level', 3),\n",
      " ('note', 3),\n",
      " ('character', 3),\n",
      " ('regression', 3),\n",
      " ('msrp', 3),\n",
      " ('review', 3),\n",
      " ('sentiment', 3),\n",
      " ('wiebe', 3),\n",
      " ('penalty', 3),\n",
      " ('learnt', 3),\n",
      " ('wordnet', 3),\n",
      " ('baseline', 3),\n",
      " ('idf', 3),\n",
      " ('despite', 3),\n",
      " ('cer', 3),\n",
      " ('profile', 3),\n",
      " ('piotr', 3),\n",
      " ('armand', 3),\n",
      " ('chris', 3),\n",
      " ('annual', 3),\n",
      " ('meeting', 3),\n",
      " ('journal', 3),\n",
      " ('compositional', 3),\n",
      " ('bansal', 3),\n",
      " ('mohit', 3),\n",
      " ('gimpel', 3),\n",
      " ('kevin', 3),\n",
      " ('livescu', 3),\n",
      " ('karen', 3),\n",
      " ('abstract', 2),\n",
      " ('success', 2),\n",
      " ('application', 2),\n",
      " ('could', 2),\n",
      " ('robustness', 2),\n",
      " ('improving', 2),\n",
      " ('machine', 2),\n",
      " ('resource', 2),\n",
      " ('form', 2),\n",
      " ('factorization', 2),\n",
      " ('building', 2),\n",
      " ('nlp', 2),\n",
      " ('end', 2),\n",
      " ('bilinear', 2),\n",
      " ('setting', 2),\n",
      " ('averaged', 2),\n",
      " ('recently', 2),\n",
      " ('weighted', 2),\n",
      " ('example', 2),\n",
      " ('ability', 2),\n",
      " ('algorithm', 2),\n",
      " ('view', 2),\n",
      " ('cbow', 2),\n",
      " ('empirical', 2),\n",
      " ('simplicity', 2),\n",
      " ('inference', 2),\n",
      " ('available', 2),\n",
      " ('remains', 2),\n",
      " ('contribution', 2),\n",
      " ('switzerland', 2),\n",
      " ('epfl', 2),\n",
      " ('martin', 2),\n",
      " ('jaggi', 2),\n",
      " ('stateof', 2),\n",
      " ('wide', 2),\n",
      " ('vocabulary', 2),\n",
      " ('defined', 2),\n",
      " ('predicted', 2),\n",
      " ('known', 2),\n",
      " ('efficiency', 2),\n",
      " ('learned', 2),\n",
      " ('encoding', 2),\n",
      " ('running', 2),\n",
      " ('observed', 2),\n",
      " ('therefore', 2),\n",
      " ('variable', 2),\n",
      " ('classifier', 2),\n",
      " ('new', 2),\n",
      " ('universal', 2),\n",
      " ('specifically', 2),\n",
      " ('possible', 2),\n",
      " ('prevents', 2),\n",
      " ('corresponding', 2),\n",
      " ('frequency', 2),\n",
      " ('uniformly', 2),\n",
      " ('except', 2),\n",
      " ('following', 2),\n",
      " ('floating', 2),\n",
      " ('operation', 2),\n",
      " ('precise', 2),\n",
      " ('parallel', 2),\n",
      " ('try', 2),\n",
      " ('difference', 2),\n",
      " ('discard', 2),\n",
      " ('implementation', 2),\n",
      " ('subsampled', 2),\n",
      " ('second', 2),\n",
      " ('dynamic', 2),\n",
      " ('divided', 2),\n",
      " ('stanford', 2),\n",
      " ('library', 2),\n",
      " ('bird', 2),\n",
      " ('update', 2),\n",
      " ('weight', 2),\n",
      " ('rate', 2),\n",
      " ('contained', 2),\n",
      " ('find', 2),\n",
      " ('dropping', 2),\n",
      " ('mechanism', 2),\n",
      " ('applying', 2),\n",
      " ('encouraging', 2),\n",
      " ('sparsity', 2),\n",
      " ('beneficial', 2),\n",
      " ('additional', 2),\n",
      " ('provided', 2),\n",
      " ('supplementary', 2),\n",
      " ('material', 2),\n",
      " ('code', 2),\n",
      " ('discus', 2),\n",
      " ('among', 2),\n",
      " ('previous', 2),\n",
      " ('structured', 2),\n",
      " ('ordering', 2),\n",
      " ('paragraphvector', 2),\n",
      " ('linear', 2),\n",
      " ('distribution', 2),\n",
      " ('lstm', 2),\n",
      " ('enable', 2),\n",
      " ('type', 2),\n",
      " ('subtracting', 2),\n",
      " ('principal', 2),\n",
      " ('common', 2),\n",
      " ('psl', 2),\n",
      " ('ganitkevitch', 2),\n",
      " ('pham', 2),\n",
      " ('relies', 2),\n",
      " ('parse', 2),\n",
      " ('tree', 2),\n",
      " ('fastsent', 2),\n",
      " ('kenter', 2),\n",
      " ('definition', 2),\n",
      " ('namely', 2),\n",
      " ('variant', 2),\n",
      " ('standard', 2),\n",
      " ('literature', 2),\n",
      " ('evaluate', 2),\n",
      " ('generalization', 2),\n",
      " ('logistic', 2),\n",
      " ('identification', 2),\n",
      " ('dolan', 2),\n",
      " ('product', 2),\n",
      " ('liu', 2),\n",
      " ('subjectivity', 2),\n",
      " ('subj', 2),\n",
      " ('opinion', 2),\n",
      " ('mpqa', 2),\n",
      " ('voorhees', 2),\n",
      " ('predefined', 2),\n",
      " ('split', 2),\n",
      " ('tune', 2),\n",
      " ('computed', 2),\n",
      " ('test', 2),\n",
      " ('fold', 2),\n",
      " ('marelli', 2),\n",
      " ('pearsons', 2),\n",
      " ('discussion', 2),\n",
      " ('compare', 2),\n",
      " ('skip', 2),\n",
      " ('semisupervised', 2),\n",
      " ('achieve', 2),\n",
      " ('able', 2),\n",
      " ('content', 2),\n",
      " ('clearly', 2),\n",
      " ('fact', 2),\n",
      " ('time', 2),\n",
      " ('macro', 2),\n",
      " ('skipgram', 2),\n",
      " ('top', 2),\n",
      " ('performing', 2),\n",
      " ('considering', 2),\n",
      " ('suffix', 2),\n",
      " ('eneko', 2),\n",
      " ('cardie', 2),\n",
      " ('claire', 2),\n",
      " ('daniel', 2),\n",
      " ('diab', 2),\n",
      " ('mona', 2),\n",
      " ('janyce', 2),\n",
      " ('multilingual', 2),\n",
      " ('textual', 2),\n",
      " ('workshop', 2),\n",
      " ('sanjeev', 2),\n",
      " ('liang', 2),\n",
      " ('yingyu', 2),\n",
      " ('tengyu', 2),\n",
      " ('iclr', 2),\n",
      " ('steven', 2),\n",
      " ('toolkit', 2),\n",
      " ('grave', 2),\n",
      " ('edouard', 2),\n",
      " ('yoav', 2),\n",
      " ('omer', 2),\n",
      " ('mining', 2),\n",
      " ('via', 2),\n",
      " ('optimizing', 2),\n",
      " ('acl', 2),\n",
      " ('richard', 2),\n",
      " ('nip', 2),\n",
      " ('volume', 2),\n",
      " ('distributional', 2),\n",
      " ('christopher', 2),\n",
      " ('marco', 2),\n",
      " ('baroni', 2),\n",
      " ('chen', 2),\n",
      " ('kai', 2),\n",
      " ('corrado', 2),\n",
      " ('greg', 2),\n",
      " ('dean', 2),\n",
      " ('jeffrey', 2),\n",
      " ('felix', 2),\n",
      " ('cho', 2),\n",
      " ('kyunghyun', 2),\n",
      " ('korhonen', 2),\n",
      " ('anna', 2),\n",
      " ('lillian', 2),\n",
      " ('tacl', 2),\n",
      " ('emnlp', 2),\n",
      " ('optionally', 2),\n",
      " ('additionally', 2),\n",
      " ('gradient', 2),\n",
      " ('updating', 2),\n",
      " ('non', 2),\n",
      " ('including', 2),\n",
      " ('presented', 2),\n",
      " ('reg', 2),\n",
      " ('tremendous', 1),\n",
      " ('multitude', 1),\n",
      " ('raise', 1),\n",
      " ('obvious', 1),\n",
      " ('derived', 1),\n",
      " ('highlighting', 1),\n",
      " ('produced', 1),\n",
      " ('introduction', 1),\n",
      " ('advancing', 1),\n",
      " ('unlock', 1),\n",
      " ('access', 1),\n",
      " ('almost', 1),\n",
      " ('unlimited', 1),\n",
      " ('story', 1),\n",
      " ('deep', 1),\n",
      " ('doe', 1),\n",
      " ('fall', 1),\n",
      " ('relied', 1),\n",
      " ('particular', 1),\n",
      " ('vision', 1),\n",
      " ('notable', 1),\n",
      " ('exception', 1),\n",
      " ('within', 1),\n",
      " ('year', 1),\n",
      " ('invention', 1),\n",
      " ('formalize', 1),\n",
      " ('routinely', 1),\n",
      " ('become', 1),\n",
      " ('ubiquitous', 1),\n",
      " ('block', 1),\n",
      " ('expressiveness', 1),\n",
      " ('increased', 1),\n",
      " ('slower', 1),\n",
      " ('spectrum', 1),\n",
      " ('shallow', 1),\n",
      " ('benefit', 1),\n",
      " ('especially', 1),\n",
      " ('surprisingly', 1),\n",
      " ('constructing', 1),\n",
      " ('naively', 1),\n",
      " ('lstms', 1),\n",
      " ('plain', 1),\n",
      " ('potential', 1),\n",
      " ('tradeoff', 1),\n",
      " ('process', 1),\n",
      " ('huge', 1),\n",
      " ('scalable', 1),\n",
      " ('side', 1),\n",
      " ('trade', 1),\n",
      " ('demonstrate', 1),\n",
      " ('exceeds', 1),\n",
      " ('keeping', 1),\n",
      " ('exactly', 1),\n",
      " ('thereby', 1),\n",
      " ('putting', 1),\n",
      " ('title', 1),\n",
      " ('perspective', 1),\n",
      " ('useful', 1),\n",
      " ('challenging', 1),\n",
      " ('produce', 1),\n",
      " ('longer', 1),\n",
      " ('piece', 1),\n",
      " ('paragraph', 1),\n",
      " ('goal', 1),\n",
      " ('way', 1),\n",
      " ('simultaneously', 1),\n",
      " ('composition', 1),\n",
      " ('equal', 1),\n",
      " ('iprova', 1),\n",
      " ('computer', 1),\n",
      " ('communication', 1),\n",
      " ('science', 1),\n",
      " ('correspondence', 1),\n",
      " ('strongly', 1),\n",
      " ('crucial', 1),\n",
      " ('transferred', 1),\n",
      " ('range', 1),\n",
      " ('formally', 1),\n",
      " ('constituent', 1),\n",
      " ('inspired', 1),\n",
      " ('factor', 1),\n",
      " ('successfully', 1),\n",
      " ('output', 1),\n",
      " ('sampling', 1),\n",
      " ('studied', 1),\n",
      " ('column', 1),\n",
      " ('collect', 1),\n",
      " ('arbitrary', 1),\n",
      " ('indicator', 1),\n",
      " ('binary', 1),\n",
      " ('depends', 1),\n",
      " ('single', 1),\n",
      " ('row', 1),\n",
      " ('describing', 1),\n",
      " ('property', 1),\n",
      " ('shared', 1),\n",
      " ('max', 1),\n",
      " ('conceptually', 1),\n",
      " ('interpreted', 1),\n",
      " ('wordcontexts', 1),\n",
      " ('optimized', 1),\n",
      " ('additive', 1),\n",
      " ('combination', 1),\n",
      " ('mean', 1),\n",
      " ('select', 1),\n",
      " ('positive', 1),\n",
      " ('discarded', 1),\n",
      " ('min', 1),\n",
      " ('hyper', 1),\n",
      " ('influence', 1),\n",
      " ('introduce', 1),\n",
      " ('bias', 1),\n",
      " ('efficiently', 1),\n",
      " ('sample', 1),\n",
      " ('constructed', 1),\n",
      " ('square', 1),\n",
      " ('root', 1),\n",
      " ('nwt', 1),\n",
      " ('random', 1),\n",
      " ('complex', 1),\n",
      " ('core', 1),\n",
      " ('technique', 1),\n",
      " ('computing', 1),\n",
      " ('requires', 1),\n",
      " ('hold', 1),\n",
      " ('per', 1),\n",
      " ('due', 1),\n",
      " ('straight', 1),\n",
      " ('forward', 1),\n",
      " ('parallelized', 1),\n",
      " ('chosen', 1),\n",
      " ('hyperparameter', 1),\n",
      " ('restricted', 1),\n",
      " ('practice', 1),\n",
      " ('facilitate', 1),\n",
      " ('deciding', 1),\n",
      " ('alike', 1),\n",
      " ('variation', 1),\n",
      " ('exist', 1),\n",
      " ('across', 1),\n",
      " ('generation', 1),\n",
      " ('deprives', 1),\n",
      " ('part', 1),\n",
      " ('shortens', 1),\n",
      " ('implicitly', 1),\n",
      " ('increasing', 1),\n",
      " ('span', 1),\n",
      " ('equivalent', 1),\n",
      " ('weighing', 1),\n",
      " ('local', 1),\n",
      " ('creating', 1),\n",
      " ('want', 1),\n",
      " ('tokenized', 1),\n",
      " ('nltk', 1),\n",
      " ('tokenizer', 1),\n",
      " ('linearly', 1),\n",
      " ('decaying', 1),\n",
      " ('prevent', 1),\n",
      " ('overfitting', 1),\n",
      " ('list', 1),\n",
      " ('empirically', 1),\n",
      " ('trying', 1),\n",
      " ('multiple', 1),\n",
      " ('superior', 1),\n",
      " ('negatively', 1),\n",
      " ('impact', 1),\n",
      " ('shorter', 1),\n",
      " ('pushed', 1),\n",
      " ('particularly', 1),\n",
      " ('every', 1),\n",
      " ('add', 1),\n",
      " ('negligible', 1),\n",
      " ('build', 1),\n",
      " ('upon', 1),\n",
      " ('open', 1),\n",
      " ('related', 1),\n",
      " ('existing', 1),\n",
      " ('construct', 1),\n",
      " ('body', 1),\n",
      " ('several', 1),\n",
      " ('require', 1),\n",
      " ('coherent', 1),\n",
      " ('logical', 1),\n",
      " ('continuation', 1),\n",
      " ('others', 1),\n",
      " ('rely', 1),\n",
      " ('unordered', 1),\n",
      " ('collection', 1),\n",
      " ('finally', 1),\n",
      " ('alternative', 1),\n",
      " ('built', 1),\n",
      " ('independent', 1),\n",
      " ('softmax', 1),\n",
      " ('consecutive', 1),\n",
      " ('retrieve', 1),\n",
      " ('original', 1),\n",
      " ('corrupted', 1),\n",
      " ('version', 1),\n",
      " ('encode', 1),\n",
      " ('simply', 1),\n",
      " ('becomes', 1),\n",
      " ('sequential', 1),\n",
      " ('autoencoder', 1),\n",
      " ('mixture', 1),\n",
      " ('hence', 1),\n",
      " ('estimated', 1),\n",
      " ('stripping', 1),\n",
      " ('away', 1),\n",
      " ('syntax', 1),\n",
      " ('diverse', 1),\n",
      " ('paragram', 1),\n",
      " ('line', 1),\n",
      " ('syntactic', 1),\n",
      " ('incorporated', 1),\n",
      " ('employed', 1),\n",
      " ('obtaining', 1),\n",
      " ('template', 1),\n",
      " ('depending', 1),\n",
      " ('combine', 1),\n",
      " ('recurrent', 1),\n",
      " ('adjacent', 1),\n",
      " ('fashion', 1),\n",
      " ('earlier', 1),\n",
      " ('named', 1),\n",
      " ('share', 1),\n",
      " ('idea', 1),\n",
      " ('surrounding', 1),\n",
      " ('contrasting', 1),\n",
      " ('conceptual', 1),\n",
      " ('obtain', 1),\n",
      " ('firstly', 1),\n",
      " ('opposed', 1),\n",
      " ('secondly', 1),\n",
      " ('summing', 1),\n",
      " ('requiring', 1),\n",
      " ('map', 1),\n",
      " ('choice', 1),\n",
      " ('mapping', 1),\n",
      " ('caption', 1),\n",
      " ('breadth', 1),\n",
      " ('fairly', 1),\n",
      " ('area', 1),\n",
      " ('testing', 1),\n",
      " ('quality', 1),\n",
      " ('universality', 1),\n",
      " ('competing', 1),\n",
      " ('combined', 1),\n",
      " ('evaluated', 1),\n",
      " ('various', 1),\n",
      " ('follows', 1),\n",
      " ('movie', 1),\n",
      " ('polarity', 1),\n",
      " ('classify', 1),\n",
      " ('manner', 1),\n",
      " ('concatenating', 1),\n",
      " ('wise', 1),\n",
      " ('remaining', 1),\n",
      " ('inferred', 1),\n",
      " ('directly', 1),\n",
      " ('fed', 1),\n",
      " ('nested', 1),\n",
      " ('mrsp', 1),\n",
      " ('tuned', 1),\n",
      " ('cosine', 1),\n",
      " ('gold', 1),\n",
      " ('human', 1),\n",
      " ('judgement', 1),\n",
      " ('spearmans', 1),\n",
      " ('relatedness', 1),\n",
      " ('contains', 1),\n",
      " ('six', 1),\n",
      " ('basis', 1),\n",
      " ('origin', 1),\n",
      " ('headline', 1),\n",
      " ('forum', 1),\n",
      " ('created', 1),\n",
      " ('discussed', 1),\n",
      " ('consisting', 1),\n",
      " ('count', 1),\n",
      " ('weighed', 1),\n",
      " ('observing', 1),\n",
      " ('half', 1),\n",
      " ('weaker', 1),\n",
      " ('labelled', 1),\n",
      " ('faring', 1),\n",
      " ('poorly', 1),\n",
      " ('indicating', 1),\n",
      " ('lack', 1),\n",
      " ('generalizability', 1),\n",
      " ('rest', 1),\n",
      " ('lot', 1),\n",
      " ('contextual', 1),\n",
      " ('missing', 1),\n",
      " ('poor', 1),\n",
      " ('predicts', 1),\n",
      " ('good', 1),\n",
      " ('answer', 1),\n",
      " ('comparing', 1),\n",
      " ('par', 1),\n",
      " ('lagging', 1),\n",
      " ('behind', 1),\n",
      " ('subtasks', 1),\n",
      " ('observation', 1),\n",
      " ('attributed', 1),\n",
      " ('big', 1),\n",
      " ('chunk', 1),\n",
      " ('english', 1),\n",
      " ('helping', 1),\n",
      " ('involving', 1),\n",
      " ('item', 1),\n",
      " ('interestingly', 1),\n",
      " ('official', 1),\n",
      " ('edition', 1),\n",
      " ('delivers', 1),\n",
      " ('summarize', 1),\n",
      " ('unavailable', 1),\n",
      " ('tfidf', 1),\n",
      " ('calculated', 1),\n",
      " ('entry', 1),\n",
      " ('taken', 1),\n",
      " ('described', 1),\n",
      " ('noticeable', 1),\n",
      " ('dump', 1),\n",
      " ('faster', 1),\n",
      " ('owing', 1),\n",
      " ('degree', 1),\n",
      " ('parallelizability', 1),\n",
      " ('outperforming', 1),\n",
      " ('experimental', 1),\n",
      " ('removal', 1),\n",
      " ('value', 1),\n",
      " ('experiment', 1),\n",
      " ('hint', 1),\n",
      " ('reduce', 1),\n",
      " ('blacklist', 1),\n",
      " ('reported', 1),\n",
      " ('competitive', 1),\n",
      " ('ranked', 1),\n",
      " ('left', 1),\n",
      " ('right', 1),\n",
      " ('expected', 1),\n",
      " ('purely', 1),\n",
      " ('effect', 1),\n",
      " ('generalize', 1),\n",
      " ('sometimes', 1),\n",
      " ('beating', 1),\n",
      " ('interesting', 1),\n",
      " ('static', 1),\n",
      " ('rare', 1),\n",
      " ('seems', 1),\n",
      " ('roughly', 1),\n",
      " ('follow', 1),\n",
      " ('luhns', 1),\n",
      " ('hypothesis', 1),\n",
      " ('luhn', 1),\n",
      " ('retrieval', 1),\n",
      " ('paradigm', 1),\n",
      " ('stating', 1),\n",
      " ('mid', 1),\n",
      " ('rank', 1),\n",
      " ('discriminate', 1),\n",
      " ('modifying', 1),\n",
      " ('change', 1),\n",
      " ('oriented', 1),\n",
      " ('attribute', 1),\n",
      " ('lower', 1),\n",
      " ('fit', 1),\n",
      " ('conclusion', 1),\n",
      " ('introduced', 1),\n",
      " ('novel', 1),\n",
      " ('computationally', 1),\n",
      " ('infer', 1),\n",
      " ('achieves', 1),\n",
      " ('competitor', 1),\n",
      " ('future', 1),\n",
      " ('augmenting', 1),\n",
      " ('exploit', 1),\n",
      " ('furthermore', 1),\n",
      " ('investigate', 1),\n",
      " ('transfer', 1),\n",
      " ('indebted', 1),\n",
      " ('helpful', 1),\n",
      " ('reference', 1),\n",
      " ('banea', 1),\n",
      " ('carmen', 1),\n",
      " ('gonzalez', 1),\n",
      " ('aitor', 1),\n",
      " ('guo', 1),\n",
      " ('weiwei', 1),\n",
      " ('mihalcea', 1),\n",
      " ('rada', 1),\n",
      " ('rigau', 1),\n",
      " ('german', 1),\n",
      " ('dublin', 1),\n",
      " ('ireland', 1),\n",
      " ('yuanzhi', 1),\n",
      " ('risteski', 1),\n",
      " ('andrej', 1),\n",
      " ('latent', 1),\n",
      " ('pmibased', 1),\n",
      " ('tough', 1),\n",
      " ('beat', 1),\n",
      " ('klein', 1),\n",
      " ('ewan', 1),\n",
      " ('loper', 1),\n",
      " ('edward', 1),\n",
      " ('python', 1),\n",
      " ('analyzing', 1),\n",
      " ('oreilly', 1),\n",
      " ('medium', 1),\n",
      " ('inc', 1),\n",
      " ('enriching', 1),\n",
      " ('subword', 1),\n",
      " ('lopez', 1),\n",
      " ('gazpio', 1),\n",
      " ('inigo', 1),\n",
      " ('specia', 1),\n",
      " ('lucia', 1),\n",
      " ('lingual', 1),\n",
      " ('focused', 1),\n",
      " ('bill', 1),\n",
      " ('quirk', 1),\n",
      " ('brockett', 1),\n",
      " ('construction', 1),\n",
      " ('massively', 1),\n",
      " ('juri', 1),\n",
      " ('van', 1),\n",
      " ('durme', 1),\n",
      " ('benjamin', 1),\n",
      " ('callison', 1),\n",
      " ('burch', 1),\n",
      " ('ppdb', 1),\n",
      " ('minqing', 1),\n",
      " ('bing', 1),\n",
      " ('summarizing', 1),\n",
      " ('customer', 1),\n",
      " ('tenth', 1),\n",
      " ('acm', 1),\n",
      " ('sigkdd', 1),\n",
      " ('knowledge', 1),\n",
      " ('discovery', 1),\n",
      " ('huang', 1),\n",
      " ('furong', 1),\n",
      " ('anandkumar', 1),\n",
      " ('animashree', 1),\n",
      " ('scratch', 1),\n",
      " ('convolutional', 1),\n",
      " ('tensor', 1),\n",
      " ('decomposition', 1),\n",
      " ('european', 1),\n",
      " ('chapter', 1),\n",
      " ('short', 1),\n",
      " ('tom', 1),\n",
      " ('borisov', 1),\n",
      " ('alexey', 1),\n",
      " ('rijke', 1),\n",
      " ('maarten', 1),\n",
      " ('ryan', 1),\n",
      " ('zhu', 1),\n",
      " ('yukun', 1),\n",
      " ('salakhutdinov', 1),\n",
      " ('ruslan', 1),\n",
      " ('zemel', 1),\n",
      " ('urtasun', 1),\n",
      " ('raquel', 1),\n",
      " ('torralba', 1),\n",
      " ('antonio', 1),\n",
      " ('fidler', 1),\n",
      " ('sanja', 1),\n",
      " ('thought', 1),\n",
      " ('quoc', 1),\n",
      " ('icml', 1),\n",
      " ('dagan', 1),\n",
      " ('ido', 1),\n",
      " ('lesson', 1),\n",
      " ('automatic', 1),\n",
      " ('creation', 1),\n",
      " ('ibm', 1),\n",
      " ('research', 1),\n",
      " ('development', 1),\n",
      " ('surdeanu', 1),\n",
      " ('mihai', 1),\n",
      " ('bauer', 1),\n",
      " ('finkel', 1),\n",
      " ('jenny', 1),\n",
      " ('rose', 1),\n",
      " ('bethard', 1),\n",
      " ('mcclosky', 1),\n",
      " ('david', 1),\n",
      " ('corenlp', 1),\n",
      " ('demonstration', 1),\n",
      " ('menini', 1),\n",
      " ('stefano', 1),\n",
      " ('bentivogli', 1),\n",
      " ('luisa', 1),\n",
      " ('bernardi', 1),\n",
      " ('raffaella', 1),\n",
      " ('zamparelli', 1),\n",
      " ('roberto', 1),\n",
      " ('cure', 1),\n",
      " ('estimation', 1),\n",
      " ('space', 1),\n",
      " ('sutskever', 1),\n",
      " ('ilya', 1),\n",
      " ('jeff', 1),\n",
      " ('compositionality', 1),\n",
      " ('unlabelled', 1),\n",
      " ('naacl', 1),\n",
      " ('hlt', 1),\n",
      " ('february', 1),\n",
      " ('sentimental', 1),\n",
      " ('education', 1),\n",
      " ('analysis', 1),\n",
      " ('summarization', 1),\n",
      " ('minimum', 1),\n",
      " ('cut', 1),\n",
      " ('bengio', 1),\n",
      " ('yoshua', 1),\n",
      " ('understand', 1),\n",
      " ('url', 1),\n",
      " ('http', 1),\n",
      " ('columbia', 1),\n",
      " ('edu', 1),\n",
      " ('ojs', 1),\n",
      " ('index', 1),\n",
      " ('php', 1),\n",
      " ('article', 1),\n",
      " ('seeing', 1),\n",
      " ('star', 1),\n",
      " ('relationship', 1),\n",
      " ('categorization', 1),\n",
      " ('respect', 1),\n",
      " ('rating', 1),\n",
      " ('scale', 1),\n",
      " ('karl', 1),\n",
      " ('inheritance', 1),\n",
      " ('parent', 1),\n",
      " ('royal', 1),\n",
      " ('society', 1),\n",
      " ('london', 1),\n",
      " ('socher', 1),\n",
      " ('global', 1),\n",
      " ('kruszewski', 1),\n",
      " ('lazaridou', 1),\n",
      " ('jointly', 1),\n",
      " ('lexical', 1),\n",
      " ('sentential', 1),\n",
      " ('rockafellar', 1),\n",
      " ('tyrrell', 1),\n",
      " ('monotone', 1),\n",
      " ('siam', 1),\n",
      " ('control', 1),\n",
      " ('optimization', 1),\n",
      " ('proof', 1),\n",
      " ('measurement', 1),\n",
      " ('thing', 1),\n",
      " ('american', 1),\n",
      " ('psychology', 1),\n",
      " ('ellen', 1),\n",
      " ('overview', 1),\n",
      " ('answering', 1),\n",
      " ('track', 1),\n",
      " ('nist', 1),\n",
      " ('special', 1),\n",
      " ('publication', 1),\n",
      " ('wilson', 1),\n",
      " ('theresa', 1),\n",
      " ('annotating', 1),\n",
      " ('expression', 1),\n",
      " ('emotion', 1),\n",
      " ('roth', 1),\n",
      " ('dan', 1),\n",
      " ('back', 1),\n",
      " ('paraphrastic', 1),\n",
      " ('charagram', 1),\n",
      " ('improved', 1),\n",
      " ('adding', 1),\n",
      " ('regularizer', 1),\n",
      " ('leading', 1),\n",
      " ('slightly', 1),\n",
      " ('memory', 1),\n",
      " ('reason', 1),\n",
      " ('higher', 1),\n",
      " ('apply', 1),\n",
      " ('individually', 1),\n",
      " ('order', 1),\n",
      " ('minimize', 1),\n",
      " ('differentiable', 1),\n",
      " ('basic', 1),\n",
      " ('updated', 1),\n",
      " ('respectively', 1),\n",
      " ('run', 1),\n",
      " ('give', 1),\n",
      " ('boost', 1),\n",
      " ('take', 1),\n",
      " ('thus', 1),\n",
      " ('overhead', 1),\n",
      " ('wiki', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Extract vocabulary and lemmatize\n",
    "import operator\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vocab = {}\n",
    "for s in results:\n",
    "    words = WordPunctTokenizer().tokenize(s)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if word in stopwords.words('english') or not re.match('^[a-z]+$',word) or len(word) < 3: continue\n",
    "        if word in vocab:\n",
    "            vocab[word] += 1\n",
    "        else:\n",
    "            vocab[word] = 1\n",
    "vocab = sorted(vocab.items(), key=operator.itemgetter(1), reverse=True)\n",
    "pprint(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
